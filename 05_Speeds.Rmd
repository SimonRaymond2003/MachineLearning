# Speeds

```{r}
library(xgboost)
library(ROCR)
library(foreach)
library(doParallel)
library(Matrix)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(dplyr)
```


## Functions to run the tests

I will make 3 functions. One with my parallel processing, one with xgboost pp and then one with no pp at all so n thread = 1.

I have included the code so that you can see it but it is not important just know that we are looping r times and that the loops use bootstrapping for splits.

```{r}

xgb1 <- function(r, xs, y, params, nrounds) {
  start_time <- Sys.time() # Start timer
  
  auc <- numeric(r) # Pre-allocate a numeric vector for AUC values
  for (i in 1:r) {
    # Bootstrap sampling
    ind <- sample(nrow(xs), nrow(xs) * 0.8)
    md_x <- xs[ind, ]
    md_y <- y[ind]
    test_x <- xs[-ind, ] 
    test_y <- y[-ind] 
    
# onvert to DMatrix
dtrain <- xgb.DMatrix(data = md_x, label = md_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)  

    # Train the model
params <- list(objective = "binary:logistic", eval_metric = "auc")
model <- xgb.train(params = params, 
                       data = dtrain, 
                       nrounds = nrounds,
                       nthread = 1)

    predictions <- predict(model, dtest)
    pred <- ROCR::prediction(predictions, test_y)
    perf <- ROCR::performance(pred, "auc")
    auc[i] <- perf@y.values[[1]]
  }
  
  end_time <- Sys.time() # End timer
time_taken <- as.numeric(end_time - start_time, units = "secs")
  
  list(auc = auc, time_taken = time_taken)
}

```


```{r}

xgb2 <- function(r, xs, y, params, nrounds) {
  start_time <- Sys.time() # Start timer
  
  auc <- numeric(r) # Initialize the AUC vector to store AUC values for each iteration
  
  for (i in 1:r) {
   # Bootstrap sampling
    ind <- sample(nrow(xs), nrow(xs) * 0.8)
    md_x <- xs[ind, ]
    md_y <- y[ind]
    test_x <- xs[-ind, ] # Potential indexing issue
    test_y <- y[-ind] # Potential indexing issue
    
# onvert to DMatrix
dtrain <- xgb.DMatrix(data = md_x, label = md_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)  

    # Train the model
params <- list(objective = "binary:logistic", eval_metric = "auc")
model <- xgb.train(params = params, 
                       data = dtrain, 
                       nrounds = nrounds,
                       nthread = detectCores() - 1)

    # Predicting and calculating AUC
    predictions <- predict(model, dtest)
    pred <- ROCR::prediction(predictions, test_y)
    perf <- ROCR::performance(pred, "auc")
    auc[i] <- perf@y.values[[1]]
  }
  
  end_time <- Sys.time() # End timer
time_taken <- as.numeric(end_time - start_time, units = "secs")
  
  list(mean_auc = mean(auc), time_taken = time_taken)
}


```

```{r}
xgbpar <- function(r, xs, y, params, nrounds) {
  start_time <- Sys.time() # Start timer
  
  cl <- makeCluster(detectCores() - 1) # Use one less than the total number of cores
  registerDoParallel(cl)

  # Parallel processing using foreach
  results <- foreach(i = 1:r, .combine = 'c', .packages = c('xgboost', 'ROCR')) %dopar% {
    # Bootstrap sampling
    ind <- sample(nrow(xs), nrow(xs) * 0.8)
    md_x <- xs[ind, ]
    md_y <- y[ind]
    test_x <- xs[-ind, ] # Potential indexing issue
    test_y <- y[-ind] # Potential indexing issue
    
# onvert to DMatrix
dtrain <- xgb.DMatrix(data = md_x, label = md_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)  

    # Train the model
params <- list(objective = "binary:logistic", eval_metric = "auc")
model <- xgb.train(params = params, 
                       data = dtrain, 
                       nrounds = nrounds,
                       nthread = 1)

    predictions <- predict(model, dtest)
    pred <- prediction(predictions, test_y)
    perf <- performance(pred, "auc")
    perf@y.values[[1]]
  }

  # Stop the cluster
  stopCluster(cl)
  
  end_time <- Sys.time() # End timer
time_taken <- as.numeric(end_time - start_time, units = "secs")

  list(mean_auc = mean(results), time_taken = time_taken)
}


```


## 10,000 rows

Create the data. as a note this data has no relations so it wont be predictable
20 factor collums
10 numeric
```{r}
set.seed(123)
df <- data.frame(y = sample(c(0, 1), 10000, replace = TRUE))
for(i in 1:10) {
  df <- df %>% mutate(!!paste0("num", i) := runif(10000))
}
for(i in 1:20) {
  df <- df %>% mutate(!!paste0("fac", i) := factor(sample(c("1", "2", "3"), 10000, replace = TRUE)))
}


```



### With a normal Matrix
```{r}
xs <- model.matrix(~ . -1 - y, data = df)
y <- df$y
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)
nrounds <- 100
r <- detectCores() - 1
```

```{r}
# Execute the functions
x1 <- xgb1(r, xs, y, params, nrounds)
x2 <- xgb2(r, xs, y, params, nrounds)
xp <- xgbpar(r, xs, y, params, nrounds)

# Extract execution times
t1sf <- x1$time_taken
t2sf <- x2$time_taken
tpsf <- xp$time_taken
```

### With a sparse matrix
For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. 

```{r}
y <- df$y
xs <- sparse.model.matrix(~ ., data = as.data.frame(xs))
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)
nrounds <- 100
r <- detectCores() - 1
```

```{r}
# Execute the functions
x1 <- xgb1(r, xs, y, params, nrounds)
x2 <- xgb2(r, xs, y, params, nrounds)
xp <- xgbpar(r, xs, y, params, nrounds)

# Extract execution times
t1ssf <- x1$time_taken
t2ssf <- x2$time_taken
tpssf <- xp$time_taken

```

## 100,000 rows

```{r}
set.seed(123)
df <- data.frame(y = sample(c(0, 1), 100000, replace = TRUE))
for(i in 1:10) {
  df <- df %>% mutate(!!paste0("num", i) := runif(100000))
}
for(i in 1:20) {
  df <- df %>% mutate(!!paste0("fac", i) := factor(sample(c("1", "2", "3"), 100000, replace = TRUE)))
}


```



### With a normal Matrix
```{r}
xs <- model.matrix(~ . -1 - y, data = df)
y <- df$y
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)
nrounds <- 100
r <- detectCores() - 1
```

```{r}
# Execute the functions
x1 <- xgb1(r, xs, y, params, nrounds)
x2 <- xgb2(r, xs, y, params, nrounds)
xp <- xgbpar(r, xs, y, params, nrounds)

# Extract execution times
t1100 <- x1$time_taken
t2100 <- x2$time_taken
tp100 <- xp$time_taken
```

### With a sparse matrix
For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. 

```{r}
y <- df$y
xs <- sparse.model.matrix(~ ., data = as.data.frame(xs))
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)
nrounds <- 100
r <- detectCores() - 1
```

```{r}
# Execute the functions
x1 <- xgb1(r, xs, y, params, nrounds)
x2 <- xgb2(r, xs, y, params, nrounds)
xp <- xgbpar(r, xs, y, params, nrounds)

# Extract execution times
t1s100 <- x1$time_taken
t2s100 <- x2$time_taken
tps100 <- xp$time_taken
```


## 500,000 rows

```{r}
set.seed(123)
df <- data.frame(y = sample(c(0, 1), 500000, replace = TRUE))
for(i in 1:10) {
  df <- df %>% mutate(!!paste0("num", i) := runif(500000))
}
for(i in 1:20) {
  df <- df %>% mutate(!!paste0("fac", i) := factor(sample(c("1", "2", "3"), 500000, replace = TRUE)))
}


```



### With a normal Matrix
```{r}
xs <- model.matrix(~ . -1 - y, data = df)
y <- df$y
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)
nrounds <- 100
r <- detectCores() - 1
```

```{r}
# Execute the functions
x1 <- xgb1(r, xs, y, params, nrounds)
x2 <- xgb2(r, xs, y, params, nrounds)
xp <- xgbpar(r, xs, y, params, nrounds)

# Extract execution times
t150 <- x1$time_taken
t250 <- x2$time_taken
tp50 <- xp$time_taken
```

### With a sparse matrix
For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. 

```{r}
y <- df$y
xs <- sparse.model.matrix(~ ., data = as.data.frame(xs))
```

```{r}
params <- list(
  booster = "gbtree",
  objective = "binary:logistic",
  eta = 0.1,
  gamma = 0,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = 1,
  lambda = 1,
  alpha = 0
)
nrounds <- 100
r <- detectCores() - 1
```

```{r}
# Execute the functions
x1 <- xgb1(r, xs, y, params, nrounds)
x2 <- xgb2(r, xs, y, params, nrounds)
xp <- xgbpar(r, xs, y, params, nrounds)

# Extract execution times
t1s50 <- x1$time_taken
t2s50 <- x2$time_taken
tps50 <- xp$time_taken
```

## Presentations
Print them out 

nrounds is 100 for all

10ks

matrix
```{r}
t1sf 
t2sf
tpsf
```

sparse
```{r}
t1ssf 
t2ssf
tpssf
```

100ks

matrix
```{r}
t1100 
t2100 
tp100
```

sparse
```{r}
t1s100 
t2s100 
tps100
```

500ks

matrix
```{r}
t150 
t250 
tp50
```

sparse
```{r}
t1s50 
t2s50 
tps50 

```

## Analysis

### Graphs
```{r}
pres <- data.frame(
  size = rep(c("10k", "100k", "500k"), each = 6),
  type = rep(c("matrix", "sparse"), each = 3, times = 3),
  configuration = rep(c("t1", "t2", "tp"), times = 6),
  time = c(t1sf, t2sf, tpsf, t1ssf, t2ssf, tpssf, 
           t1100, t2100, tp100, t1s100, t2s100, tps100, 
           t150, t250, tp50, t1s50, t2s50, tps50)
)
```

```{r}

# Plot 1: Sparse times vs Normal matrix times
ggplot(pres, aes(x = interaction(size, configuration), y = time, fill = type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Execution Time Comparison: Sparse vs Normal Matrix",
       x = "Dataset and Configuration",
       y = "Time") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot 2: Comparing t1 vs t2 vs tp by each dataset size
ggplot(pres, aes(x = interaction(size, type), y = time, fill = configuration)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Execution Time Comparison: t1 vs t2 vs tp",
       x = "Dataset Size and Type",
       y = "Time") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Plot 3: Time per round for each dataset
pres <- pres %>%
  mutate(time_per_round = time / 100) # Since nrounds is 100 for all

ggplot(pres, aes(x = interaction(size, type, configuration), y = time_per_round)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Time Per Round for Each Dataset",
       x = "Dataset Size, Type, and Configuration",
       y = "Time per Round") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# Assuming the same setup as before, add the number of rows for each size
pres <- pres %>%
  mutate(nrows = case_when(
    size == "10k" ~ 10000,
    size == "100k" ~ 100000,
    size == "500k" ~ 500000
  ))

# Calculate the time per row
pres <- pres %>%
  mutate(time_per_row = time / nrows)

# Plot: Time per nrow for each dataset
ggplot(pres, aes(x = interaction(size, type, configuration), y = time_per_row, fill = configuration)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Execution Time per Row for Each Dataset and Configuration",
       x = "Dataset Size, Type, and Configuration",
       y = "Time per Row (seconds)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Note: Customize the labels and title as needed

```

### Takaways

With factor collums sparse seems to be better. With numeric collums though it would be worse. XGB is better with internal processing when nrows is huge. When nrows is smaller though we should use our own parrallel processing.

Also times include how many loops we are doing which is related to numcores of the machine being used if you look at our codes

# Notes for myself

Test light gbm with these.
Test adaboost with these.
Show that only numerical data kills a sparse matrix.
What about doparrallel with numcores set inside the xgboost funtions.
What about purrr or other ways of parrallel processing.
Do correlations in the data(the data actually being predictable have a effect).
How much does scaling help with speed?
What is faster 1 numeric collum or 1 factor collum?
The graphs look ugly clean them up.