# Initialize lists to store TPR and FPR values for each bootstrap iteration
all_tpr <- list()
all_fpr <- list()
for(i in 1:r) {
ind <- sample(nrow(xs), size = nrow(xs), replace = TRUE)
train_xs <- xs[ind, ]
test_xs <- xs[-ind, ]
train_y <- y[ind]
test_y <- y[-ind]
dtrain <- xgb.DMatrix(data = train_xs, label = train_y)
dtest <- xgb.DMatrix(data = test_xs, label = test_y)
xgbmdl_final <- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0)
final_phat <- predict(xgbmdl_final, dtest)
final_predicted_classes <- ifelse(final_phat > avg_best_threshold, 1, 0)
final_test_y_binary <- ifelse(test_y > avg_best_threshold, 1, 0)
conf_matrix <- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(final_test_y_binary, levels = c(1, 0)))
conf_mat_totals <- conf_mat_totals + as.matrix(conf_matrix)
final_pred_rocr <- prediction(final_phat, test_y)
final_perf_rocr <- performance(final_pred_rocr, "tpr", "fpr")
# Store TPR and FPR values for this iteration
all_tpr[[i]] <- final_perf_rocr@y.values[[1]]
all_fpr[[i]] <- final_perf_rocr@x.values[[1]]
}
# Calculate the average confusion matrix
avg_conf_matrix <- conf_mat_totals / r
print(avg_conf_matrix)
library(xgboost)
library(ROCR)
best_thresholds <- numeric(r)  # To store best thresholds for F1 score
f1_stats <- numeric(r)         # To store the F1 scores at the best thresholds
for(i in 1:r) {
# Bootstrap sampling
ind <- sample(nrow(xs), size = nrow(xs), replace = TRUE)
train_xs <- xs[ind, ]
test_xs <- xs[-ind, ]
train_y <- y[ind]
test_y <- y[-ind]
# Create DMatrix objects
dtrain <- xgb.DMatrix(data = train_xs, label = train_y)
dtest <- xgb.DMatrix(data = test_xs, label = test_y)
# Train the model with parameters extracted from a hypothetical 'best_hp' object
xgbmdl <- xgb.train(params = list(
booster = "gbtree",
objective = "binary:logistic",
eta = best_hp$eta,
max_depth = best_hp$max_depth,
subsample = best_hp$subsample,
colsample_bytree = best_hp$colsample_bytree,
min_child_weight = best_hp$min_child_weight,
gamma = best_hp$gamma,
alpha = best_hp$alpha,
lambda = best_hp$lambda
), data = dtrain, nrounds = best_hp$nrounds, verbose = 0)
# Predictions
phat <- predict(xgbmdl, dtest)
# Calculate F1 score for each threshold
pred <- prediction(phat, test_y)
perf <- performance(pred, measure = "prec", x.measure = "rec")
precision <- slot(perf, "y.values")[[1]]
recall <- slot(perf, "x.values")[[1]]
thresholds <- slot(perf, "alpha.values")[[1]]
f1_score <- (2 * precision * recall) / (precision + recall)
# Find the best threshold (maximizing F1 score)
best_idx <- which.max(f1_score)
best_thresholds[i] <- thresholds[best_idx]
f1_stats[i] <- f1_score[best_idx]
}
# Calculate and print the average of the best thresholds
avg_best_f1_threshold <- mean(best_thresholds)
cat("Average Best F1 Threshold:", avg_best_f1_threshold, "\n")
# Plot the distribution of max F1 scores for each iteration
hist(f1_stats, col = "blue", main = "Distribution of Max F1 Scores Across Bootstrap Iterations")
abline(v = mean(f1_stats), col = "red", lwd = 2) # Mean F1 score line
# Initialize storage for aggregated confusion matrix totals
conf_mat_totals <- matrix(0, nrow = 2, ncol = 2)
colnames(conf_mat_totals) <- c("Actual_1", "Actual_0")
rownames(conf_mat_totals) <- c("Predicted_1", "Predicted_0")
# Initialize lists to store TPR and FPR values for each bootstrap iteration
all_tpr <- list()
all_fpr <- list()
for(i in 1:r) {
ind <- sample(nrow(xs), size = nrow(xs), replace = TRUE)
train_xs <- xs[ind, ]
test_xs <- xs[-ind, ]
train_y <- y[ind]
test_y <- y[-ind]
dtrain <- xgb.DMatrix(data = train_xs, label = train_y)
dtest <- xgb.DMatrix(data = test_xs, label = test_y)
xgbmdl_final <- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0)
final_phat <- predict(xgbmdl_final, dtest)
final_predicted_classes <- ifelse(final_phat > avg_best_threshold, 1, 0)
final_test_y_binary <- ifelse(test_y > avg_best_threshold, 1, 0)
conf_matrix <- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(final_test_y_binary, levels = c(1, 0)))
conf_mat_totals <- conf_mat_totals + as.matrix(conf_matrix)
final_pred_rocr <- prediction(final_phat, test_y)
final_perf_rocr <- performance(final_pred_rocr, "tpr", "fpr")
# Store TPR and FPR values for this iteration
all_tpr[[i]] <- final_perf_rocr@y.values[[1]]
all_fpr[[i]] <- final_perf_rocr@x.values[[1]]
}
# Calculate the average confusion matrix
avg_conf_matrix <- conf_mat_totals / r
print(avg_conf_matrix)
avg_best_f1_threshold <- mean(best_thresholds)
# Find the indices in the ROC curve closest to these thresholds
f1_index <- which.min(abs(common_fpr_thresholds - avg_best_f1_threshold))
youden_index <- which.min(abs(common_fpr_thresholds - avg_best_youden_threshold))
# Plot the averaged ROC curve
plot(common_fpr_thresholds, averaged_tpr, type = 'l', col = 'blue', xlab = 'False Positive Rate', ylab = 'True Positive Rate', main = 'Averaged ROC Curve across Bootstrap Samples')
abline(a = 0, b = 1, lty = 2, col = 'red')
# Add a point for the average best F1 threshold
points(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], col = "green", pch = 19, cex = 1.5)
text(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], " Avg Best F1", pos = 4)
points(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], col = "orange", pch = 19, cex = 1.5)
text(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], " Avg Best Youden", pos = 4)
# Legend to help identify the points
legend("bottomright", legend = c("Avg Best F1", "Avg Best Youden"), col = c("green", "orange"), pch = 19, bty = "n")
# Calculate average best F1 threshold from previous results
avg_best_f1_threshold <- mean(best_thresholds)
# Find the indices in the ROC curve closest to these thresholds
f1_index <- which.min(abs(common_fpr_thresholds - avg_best_f1_threshold))
youden_index <- which.min(abs(common_fpr_thresholds - avg_best_youden_threshold))
fixed_threshold_index <- which.min(abs(common_fpr_thresholds - 0.5))  # For the 0.5 fixed threshold
# Plot the averaged ROC curve
plot(common_fpr_thresholds, averaged_tpr, type = 'l', col = 'blue', xlab = 'False Positive Rate', ylab = 'True Positive Rate', main = 'Averaged ROC Curve across Bootstrap Samples')
abline(a = 0, b = 1, lty = 2, col = 'red')
# Add points for the average best F1 and Youden thresholds
points(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], col = "green", pch = 19, cex = 1.5)
text(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], " Avg Best F1", pos = 4)
points(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], col = "orange", pch = 19, cex = 1.5)
text(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], " Avg Best Youden", pos = 4)
# Add a point for the 0.5 fixed threshold
points(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], col = "purple", pch = 19, cex = 1.5)
text(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], " Fixed 0.5 Threshold", pos = 4, col = "purple")
# Update the legend to include the 0.5 threshold
legend("bottomright", legend = c("Avg Best F1", "Avg Best Youden", "Fixed 0.5 Threshold"), col = c("green", "orange", "purple"), pch = 19, bty = "n")
# Calculate average best F1 threshold from previous results
avg_best_f1_threshold <- mean(best_thresholds)
# Find the indices in the ROC curve closest to these thresholds
f1_index <- which.min(abs(common_fpr_thresholds - avg_best_f1_threshold))
youden_index <- which.min(abs(common_fpr_thresholds - avg_best_youden_threshold))
fixed_threshold_index <- which.min(abs(common_fpr_thresholds - 0.5))  # For the 0.5 fixed threshold
# Plot the averaged ROC curve
plot(common_fpr_thresholds, averaged_tpr, type = 'l', col = 'blue', xlab = 'False Positive Rate', ylab = 'True Positive Rate', main = 'Averaged ROC Curve across Bootstrap Samples')
abline(a = 0, b = 1, lty = 2, col = 'red')
# Add points for the average best F1 and Youden thresholds
points(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], col = "green", pch = 19, cex = 1.5)
text(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], " Avg Best F1", pos = 4)
points(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], col = "orange", pch = 19, cex = 1.5)
text(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], " Avg Best Youden", pos = 4)
# Add a point for the 0.5 fixed threshold
points(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], col = "purple", pch = 19, cex = 1.5)
text(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], " Fixed 0.5 Threshold", pos = 4, col = "purple")
# Update the legend to include the 0.5 threshold
legend("bottomright", legend = c("Avg Best F1", "Avg Best Youden", "Fixed 0.5 Threshold"), col = c("green", "orange", "purple"), pch = 19, bty = "n")
avg_best_f1_threshold <- mean(best_thresholds)
# Find the indices in the ROC curve closest to these thresholds
f1_index <- which.min(abs(common_fpr_thresholds - avg_best_f1_threshold))
youden_index <- which.min(abs(common_fpr_thresholds - avg_best_youden_threshold))
# Plot the averaged ROC curve
plot(common_fpr_thresholds, averaged_tpr, type = 'l', col = 'blue', xlab = 'False Positive Rate', ylab = 'True Positive Rate', main = 'Averaged ROC Curve across Bootstrap Samples')
abline(a = 0, b = 1, lty = 2, col = 'red')
# Add a point for the average best F1 threshold
points(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], col = "green", pch = 19, cex = 1.5)
text(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], " Avg Best F1", pos = 4)
points(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], col = "orange", pch = 19, cex = 1.5)
text(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], " Avg Best Youden", pos = 4)
# Legend to help identify the points
legend("bottomright", legend = c("Avg Best F1", "Avg Best Youden"), col = c("green", "orange"), pch = 19, bty = "n")
# Install the latest version of H2O from CRAN
install.packages("h2o")
install.packages("h2o")
install.packages("h2o")
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(DataExplorer)
library(caret)
library(GGally)
library(tidyr)
library(readr)
data <- read_csv("machine failure.csv")
glimpse(data)
library(janitor)
data <- data %>%
clean_names()
data <- data %>%
mutate(across(c(type, twf, hdf, pwf, osf, rnf, machine_failure), as.factor))
data <- data %>%
select(-1, -2)
data <- data %>%
rename(y = machine_failure) %>%
relocate(y, .before = 1)
introduce(data)
plot_missing(data)
# Histograms for numerical variables
plot_histogram(data)
# For factor variables, use bar plots
plot_bar(data)
# Summary statistics for numerical data
data %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median, IQR = IQR, min = min, max = max)))
# Identify zero variance features
zero_var_indices <- nearZeroVar(data, saveMetrics = TRUE)
# View the metrics to determine which variables have zero or near-zero variance
print(zero_var_indices)
# Optionally, print the names of columns with zero variance
zero_var_columns <- names(data)[zero_var_indices$nzv]
print(zero_var_columns)
#Remove the types of failure
data <- data %>%
select(-c(twf, hdf, pwf, osf, rnf))
library(h2o)
h2o.init()
data_h2o <- as.h2o(data)
data_h2o <- as.h2o(data)
# Check if XGBoost is available
h2o.xgboost.available()
h2o.shutdown(prompt = FALSE)
if (FALSE) {
library(h2o)
h2o.init()
# Import the titanic dataset
f <- "https://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv"
titanic <- h2o.importFile(f)
# Set predictors and response; set response as a factor
titanic['survived'] <- as.factor(titanic['survived'])
predictors <- setdiff(colnames(titanic), colnames(titanic)[2:3])
response <- "survived"
# Split the dataset into train and valid
splits <- h2o.splitFrame(data =  titanic, ratios = .8, seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]
# Train the XGB model
titanic_xgb <- h2o.xgboost(x = predictors, y = response,
training_frame = train, validation_frame = valid,
booster = "dart", normalize_type = "tree",
seed = 1234)
}
h2o.shutdown(prompt = FALSE)
library(h2o)
h2o.init()
data_h2o <- as.h2o(data)
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.gbm.available()
# Check if XGBoost is available
h2o.GBM.available()
# Check if XGBoost is available
h2o.xgboos.available()
# Check if XGBoost is available
h2o.xgboos.available()
# Check if XGBoost is available
h2o.xgboost.available()
h2o.shutdown(prompt = FALSE)
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)
h2o.init()
library(h2o)
h2o.init()
data_h2o <- as.h2o(data)
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.xgboost.available()
h2o.shutdown(prompt = FALSE)
h2o.init(max_mem_size = "12g")
library(h2o)
h2o.init(max_mem_size = "12g")
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.xgboost.available()
# Check if XGBoost is available
h2o.xgboost.available()
# Check if GLM is available in H2O
glm_available <- h2o.getModelCapabilities()["GLM"]
library(h2o)
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
# Check if XGBoost is available
h2o.xgboost.available()
# Check if GLM is available in H2O
glm_available <- h2o.getModelCapabilities()["GLM"]
library(h2o)
h2o.init(max_mem_size = "12g")
library(h2o)
h2o.init(max_mem_size = "12g")
library(h2o)
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
# Check if XGBoost is available
h2o.xgboost.available()
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(DataExplorer)
library(caret)
library(GGally)
library(tidyr)
library(readr)
data <- read_csv("machine failure.csv")
glimpse(data)
library(janitor)
data <- data %>%
clean_names()
data <- data %>%
mutate(across(c(type, twf, hdf, pwf, osf, rnf, machine_failure), as.factor))
data <- data %>%
select(-1, -2)
data <- data %>%
rename(y = machine_failure) %>%
relocate(y, .before = 1)
introduce(data)
plot_missing(data)
# Histograms for numerical variables
plot_histogram(data)
# For factor variables, use bar plots
plot_bar(data)
# Summary statistics for numerical data
data %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median, IQR = IQR, min = min, max = max)))
# Identify zero variance features
zero_var_indices <- nearZeroVar(data, saveMetrics = TRUE)
# View the metrics to determine which variables have zero or near-zero variance
print(zero_var_indices)
# Optionally, print the names of columns with zero variance
zero_var_columns <- names(data)[zero_var_indices$nzv]
print(zero_var_columns)
#Remove the types of failure
data <- data %>%
select(-c(twf, hdf, pwf, osf, rnf))
library(h2o)
h2o.init(max_mem_size = "8g")
library(h2o)
h2o.init(max_mem_size = "8g")
View(data_h2o)
View(data)
h2o.init(max_mem_size = "8g")
aml <- h2o.automl(
x = names(data_h2o)[-which(names(data_h2o) == "y")],
y = "y",
training_frame = data_h2o,
max_runtime_secs = c(60*60*4),
nfolds = 5,
seed = 1234
)
y <- "target_column"  # Replace with your target column name
X <- setdiff(names(data), y)  # All other columns as predictors
# Optionally, split the data
splits <- h2o.splitFrame(data, ratios = 0.75, seed = 123)
y <- "target_column"  # Replace with your target column name
X <- setdiff(names(data_h2o), y)  # All other columns as predictors
# Optionally, split the data
splits <- h2o.splitFrame(data_h2o, ratios = 0.75, seed = 123)
train <- splits[[1]]
valid <- splits[[2]]
y <- "target_column"  # Replace with your target column name
X <- setdiff(names(data), y)  # All other columns as predictors
# Optionally, split the data
splits <- h2o.splitFrame(data_h2o, ratios = 0.75, seed = 123)
train <- splits[[1]]
valid <- splits[[2]]
h2o.shutdown(prompt = FALSE)
library(h2o)
h2o.init(max_mem_size = "8g")
h2o.init(max_mem_size = "12g")
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
y <- "target_column"  # Replace with your target column name
X <- setdiff(names(data), y)  # All other columns as predictors
# Optionally, split the data
splits <- h2o.splitFrame(data_h2o, ratios = 0.75, seed = 123)
train <- splits[[1]]
valid <- splits[[2]]
View(data_h2o)
View(splits)
View(train)
View(valid)
View(zero_var_indices)
h2o.shutdown(prompt = FALSE)
library(h2o)
h2o.init(max_mem_size = "12g")
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
# Step 1: Define your target variable
y <- "target"
# Step 2: Define predictors
x <- setdiff(names(data_h20), y)
# Step 1: Define your target variable
y <- "target"
# Step 2: Define predictors
x <- setdiff(names(data_h2o), y)
# Step 3: Run AutoML
aml <- h2o.automl(
x = x,
y = y,
training_frame = data_h2o,
max_models = 20,
max_runtime_secs = 3600
)
View(data)
str(data_h2o)
# Step 1: Define your target variable
y <- "y"
# Step 2: Define predictors
x <- setdiff(names(data_h2o), y)
# Step 3: Run AutoML
aml <- h2o.automl(
x = x,
y = y,
training_frame = data_h2o,
max_models = 20,
max_runtime_secs = 3600
)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(DataExplorer)
library(caret)
library(GGally)
library(tidyr)
library(readr)
data <- read_csv("machine failure.csv")
glimpse(data)
library(janitor)
data <- data %>%
clean_names()
data <- data %>%
mutate(across(c(type, twf, hdf, pwf, osf, rnf, machine_failure), as.factor))
data <- data %>%
select(-1, -2)
data <- data %>%
rename(y = machine_failure) %>%
relocate(y, .before = 1)
introduce(data)
plot_missing(data)
# Histograms for numerical variables
plot_histogram(data)
# For factor variables, use bar plots
plot_bar(data)
# Summary statistics for numerical data
data %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median, IQR = IQR, min = min, max = max)))
# Identify zero variance features
zero_var_indices <- nearZeroVar(data, saveMetrics = TRUE)
# View the metrics to determine which variables have zero or near-zero variance
print(zero_var_indices)
# Optionally, print the names of columns with zero variance
zero_var_columns <- names(data)[zero_var_indices$nzv]
print(zero_var_columns)
#Remove the types of failure
data <- data %>%
select(-c(twf, hdf, pwf, osf, rnf))
library(h2o)
h2o.init(max_mem_size = "12g")
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
# Step 1: Define your target variable
y <- "y"
# Step 2: Define predictors
x <- setdiff(names(data_h2o), y)
# Step 3: Run AutoML
aml <- h2o.automl(
x = x,
y = y,
training_frame = data_h2o,
max_models = 20,
max_runtime_secs = 3600*5
)
# Step 4: View the AutoML Leaderboard
lb <- h2o.automl.get_leaderboard(aml, extra_columns = "ALL")
# Step 4: View the AutoML Leaderboard
lb <- h2o.get_leaderboard(aml, extra_columns = "ALL")
print(lb)
View(lb)
lb
view(lb)
# Step 5: Get the best model
best_model <- h2o.get_model(aml@leader$model_id)
# Step 5: Get the best model
best_model <- h2o.getModel(aml@leader$model_id)
best_model <- aml@leader
h2o.performance(best_model, data_h20)
h2o.performance(best_model, data_h2o)
best_model@model_id
best_model@parameters
metalearner <- h2o.getMetalearner(algo = best_model)
base_models <- best_model@model$base_models
base_models <- best_model@model$base_models
base_models
metalearner <- best_model@model$metalearner
metalearner
h2o.getModel(metalearner)
h2o.getModel(metalearner)
h2o.getModel(/3/Models/metalearner_AUTO_StackedEnsemble_AllModels_1_AutoML_1_20240421_00735)
