print(zero_var_indices)
# Optionally, print the names of columns with zero variance
zero_var_columns <- names(data)[zero_var_indices$nzv]
print(zero_var_columns)
#Remove the types of failure
data <- data %>%
select(-c(twf, hdf, pwf, osf, rnf))
library(h2o)
h2o.init(max_mem_size = "12g")
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
data_h2o$y <- as.factor(data_h2o$y)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 2),
min_rows = seq(10, 10, by = 10),
learn_rate = seq(0.01, 0.2, by = 0.01),
learn_rate_annealing = seq(0.95, 1, by = 0.01),
sample_rate = seq(0.5, 1, length.out = 6),
col_sample_rate = seq(0.5, 1, length.out = 6),
ntrees = c(100, 250, 500, 750, 1000)
)
search_criteria <- list(strategy = "Cartesian")
View(hyper_params)
hyper_params
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 2),
min_rows = seq(10, 10, by = 10),
learn_rate = seq(0.01, 0.2, by = 0.02),
learn_rate_annealing = seq(0.95, 1, by = 0.01),
sample_rate = seq(0.5, 1, length.out = 4),
col_sample_rate = seq(0.5, 1, length.out = 4),
ntrees = c(100, 250, 500, 750, 1000)
)
search_criteria <- list(strategy = "Cartesian")
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 2),
min_rows = seq(10, 100, by = 10),
learn_rate = seq(0.01, 0.2, by = 0.02),
learn_rate_annealing = seq(0.95, 1, by = 0.01),
sample_rate = seq(0.5, 1, length.out = 3),
col_sample_rate = seq(0.5, 1, length.out = 3),
ntrees = c(100, 250, 500, 750),
bagging_fraction = seq(0.5, 1, length.out = 3),
bagging_freq = c(0, 10),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
View(hyper_params)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 2),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.02),
learn_rate_annealing = seq(0.95, 1, by = 0.01),
sample_rate = seq(0.5, 1, length.out = 3),
col_sample_rate = seq(0.5, 1, length.out = 3),
ntrees = c(100, 250, 500, 750),
bagging_fraction = seq(0.5, 1, length.out = 3),
bagging_freq = c(0, 10),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
View(hyper_params)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
sample_rate = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
bagging_fraction = seq(0.5, 1, length.out = 3),
bagging_freq = c(0, 10),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
sample_rate = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
bagging_fraction = seq(0.5, 1, length.out = 3),
bagging_freq = c(0, 10),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 10
)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
sample_rate = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
sample_rate = seq(0.5, 1, length.out = 3),
col_sample_rate = c(0, 10),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 10
)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
sample_rate = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 10
)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 10
)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
lambda = c(0, 0.1),
alpha = c(0, 0.1)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 10
)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5, 1e-6, 1e-7)
)
search_criteria <- list(strategy = "Cartesian")
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 10, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5, 1e-6)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 10
)
h2o.shutdown(prompt = FALSE)
library(h2o)
h2o.init(max_mem_size = "12g")
h2o.init(max_mem_size = "12g")
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 6, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(0.75, 1, length.out = 2),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5, 1e-6)
)
search_criteria <- list(strategy = "Cartesian")
View(hyper_params)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 6, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(1, 1, length.out = 1),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5, 1e-6)
)
search_criteria <- list(strategy = "Cartesian")
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 6, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.95, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(1, 1, length.out = 1),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5)
)
search_criteria <- list(strategy = "Cartesian")
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 6, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.99, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(1, 1, length.out = 1),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 5
)
h2o.shutdown(prompt = FALSE)
library(readr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(DataExplorer)
library(caret)
library(GGally)
library(tidyr)
library(readr)
data <- read_csv("machine failure.csv")
glimpse(data)
library(janitor)
data <- data %>%
clean_names()
data <- data %>%
mutate(across(c(type, twf, hdf, pwf, osf, rnf, machine_failure), as.factor))
data <- data %>%
select(-1, -2)
data <- data %>%
rename(y = machine_failure) %>%
relocate(y, .before = 1)
introduce(data)
plot_missing(data)
# Histograms for numerical variables
plot_histogram(data)
# For factor variables, use bar plots
plot_bar(data)
# Summary statistics for numerical data
data %>%
summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median, IQR = IQR, min = min, max = max)))
# Identify zero variance features
zero_var_indices <- nearZeroVar(data, saveMetrics = TRUE)
# View the metrics to determine which variables have zero or near-zero variance
print(zero_var_indices)
# Optionally, print the names of columns with zero variance
zero_var_columns <- names(data)[zero_var_indices$nzv]
print(zero_var_columns)
#Remove the types of failure
data <- data %>%
select(-c(twf, hdf, pwf, osf, rnf))
library(h2o)
h2o.init(max_mem_size = "12g")
h2o.init(max_mem_size = "12g")
data_h2o <- as.h2o(data)
data_h2o$y <- as.factor(data_h2o$y)
# Define the hyperparameter grid with added ntrees and learn_rate_annealing options
hyper_params <- list(
max_depth = seq(3, 6, by = 3),
min_rows = c(10),
learn_rate = seq(0.01, 0.2, by = 0.04),
learn_rate_annealing = seq(0.99, 1, length.out = 2),
col_sample_rate_per_tree = seq(0.75, 1, length.out = 2),
col_sample_rate = seq(1, 1, length.out = 1),
ntrees = c(100, 250, 500, 750),
min_split_improvement = c(1e-5)
)
search_criteria <- list(strategy = "Cartesian")
# Grid search setup with added parameters
grid_result <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid_search_extended",
x = setdiff(names(data_h2o), "y"),
y = "y",
training_frame = data_h2o,
hyper_params = hyper_params,
search_criteria = search_criteria,
nfolds = 5
)
# Retrieve the grid results, sorted by AUC
full_grid <- h2o.getGrid(grid_id = "gbm_grid_search_extended", sort_by = "auc", decreasing = TRUE)
# Extract the grid results into a data frame
full_summary_df <- as.data.frame(full_grid@summary_table)
# Print the structure of the data frame
print(str(full_summary_df))
splits <- h2o.splitFrame(data = data_h2o, ratios = 0.8)
train <- splits[[1]]
test <- splits[[2]]
best_model <- h2o.getModel(full_grid@model_ids[[1]])
predictions <- h2o.predict(best_model, test)
actual <- factor(as.vector(test$y), levels = c("1", "0"))
predicted <- factor(as.vector(predictions$predict), levels = c("1", "0")) # maxed f1 score
confusion_matrix <- table(Actual = actual, Predicted = predicted)
print(confusion_matrix)
# Access the best model from the grid search
best_model_id <- grid_result@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Retrieve the panel with all parameters used by the best model
best_params <- best_model@allparameters
# Display winning hyperparameters
best_params <- best_model@allparameters
cat("Winning Hyperparameters:\n")
cat(sprintf("Max Depth: %d\n", best_params$max_depth))
cat(sprintf("Min Rows: %d\n", best_params$min_rows))
cat(sprintf("Learn Rate: %.2f\n", best_params$learn_rate))
cat(sprintf("Learn Rate Annealing: %.2f\n", best_params$learn_rate_annealing))
cat(sprintf("Sample Rate: %.2f\n", best_params$col_sample_rate_per_tree))
cat(sprintf("Column Sample Rate: %.2f\n", best_params$col_sample_rate))
cat(sprintf("Number of Trees (ntrees): %d\n", best_params$ntrees))
cat(sprintf("Minimum Split Improvement: %.1e\n", best_params$min_split_improvement))
# Retrieve the parameters used by the best model
# Loop to perform repeated train-test split and model evaluation
r <- 10o
# Retrieve the parameters used by the best model
# Loop to perform repeated train-test split and model evaluation
r <- 100
auc_scores <- numeric(r)
for (i in 1:r) {
splits <- h2o.splitFrame(data = data_h2o, ratios = 0.8, seed = i)
train <- splits[[1]]
test <- splits[[2]]
# Train the model with the best parameters from the grid search
model <- h2o.gbm(
x = setdiff(names(train), "y"),
y = "y",
training_frame = train,
validation_frame = test,
learn_rate = best_params$learn_rate,
max_depth = best_params$max_depth,
col_sample_rate_per_tree = best_params$col_sample_rate_per_tree,
col_sample_rate = best_params$col_sample_rate,
ntrees = best_params$ntrees,
learn_rate_annealing = best_params$learn_rate_annealing,
min_split_improvement = best_params$min_split_improvement
)
perf <- h2o.performance(model, newdata = test)
auc_scores[i] <- h2o.auc(perf)
}
mean_auc <- mean(auc_scores)
print(paste("Average AUC over", r, "trials:", mean_auc))
# Step 1: Define your target variable
y <- "y"
# Step 2: Define predictors
x <- setdiff(names(data_h2o), y)
# Step 3: Run AutoML
aml <- h2o.automl(
x = x,
y = y,
training_frame = data_h2o,
max_models = 20,
max_runtime_secs = 3600*5
)
# Step 4: View the AutoML Leaderboard
lb <- h2o.get_leaderboard(aml, extra_columns = "ALL")
print(lb)
# Step 5: Get the best model
best_model <- aml@leader
h2o.performance(best_model, data_h2o)
# Retrieve the best model from AutoML
best_model <- aml@leader
# Initialize a vector to store AUC scores or any other performance metric
performance_scores <- numeric(100)
# Loop to perform the train-test split, predict, and calculate performance 100 times
for (i in 1:100) {
# Split the data into 80% training and 20% testing
splits <- h2o.splitFrame(data = data_h2o, ratios = 0.8)
train <- splits[[1]]
test <- splits[[2]]
# Since the model is already trained, we directly use it for prediction
pred <- h2o.predict(best_model, test)
# Evaluate performance
perf <- h2o.performance(best_model, newdata = test)
# For binary classification, assuming AUC is the metric of interest
auc_score <- h2o.auc(perf)
performance_scores[i] <- auc_score
}
# Calculate average performance across all splits
average_performance <- mean(performance_scores)
print(paste("Average AUC over 100 trials: ", average_performance))
# Calculate average performance across all splits
average_performance <- mean(performance_scores)
print(paste("Average AUC over 100 trials: ", average_performance))
pred
str(pred)
View(pred)
pred
view <-  as.data.frame(pred)
View(view)
View(data)
v2 <- as.data.frame(test)
View(v2)
auc_values <- c()
for (i in 1:100) {
# Split the data into train and test sets
splits <- h2o.splitFrame(data = data_h2o, ratios = 0.8)
train <- splits[[1]]
test <- splits[[2]]
# Train a Random Forest model
rf_model <- h2o.randomForest(
x = names(train)[-which(names(train) == "y")],
y = "y",
training_frame = train,
ntrees = 1200
)
# Perform predictions
predictions <- h2o.predict(rf_model, test)
# Get performance object
perf <- h2o.performance(rf_model, newdata = test)
# Calculate AUC
auc_values[i] <- h2o.auc(perf)
}
# Calculate the mean AUC value
mean(auc_values)
# Get variable importance from the last trained Random Forest model
var_importance <- h2o.varimp(rf_model)
print(var_importance)
