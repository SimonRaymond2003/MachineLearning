[["index.html", "Machine Learning Applications Chapter 1 About Myself 1.1 Purpose 1.2 Notes for public 1.3 Work in progress", " Machine Learning Applications Simon P. Raymond 2024-02-19 Chapter 1 About Myself My name is Simon Raymond. I have finished my Third year at Saint Mary’s University in Halifax NS. I currently have not picked a major but i am very interested in Econometrics and Machine learning. I intend to keep learning Machine Learning techniques outside of a classroom. This is where i will be uploading some of my generic or basic Algorithms and Practices. Contact: simon.raymond@smu.ca 1.1 Purpose I want to display a few things Show some basic skeletons of how to build machine learning algorithms Make some more practical applications Explain key concepts Keep track of my learning goals 1.2 Notes for public N/A 1.3 Work in progress Parts/sections Equations Skeletons Binary Application. Regression Application Imbalanced data Speed Testing Our own models package free Multi class Applicationn With real mnist Parrallel proccesing ways For myself git add docs git commit -m”update” git push -u origin main git add . git commit -m “Fix graph paths” git push origin main "],["algorithms.html", "Chapter 2 Algorithms 2.1 Data 2.2 Parametric 2.3 Trees 2.4 Boosting 2.5 Other Non-parametric", " Chapter 2 Algorithms These all will be done in classification but can easily be changed and you can look at the regressional examples 2.1 Data suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(ROCR)) suppressPackageStartupMessages(library(rpart)) suppressPackageStartupMessages(library(randomForest)) suppressPackageStartupMessages(library(xgboost)) suppressPackageStartupMessages(library(doParallel)) suppressPackageStartupMessages(library(gbm)) suppressPackageStartupMessages(library(ada)) suppressPackageStartupMessages(library(caret)) suppressPackageStartupMessages(library(nnet)) library(dslabs) data(&quot;mnist_27&quot;) data &lt;- rbind(mnist_27$train, mnist_27$test) dataf &lt;- as.data.frame(lapply(data, function(x) if(is.numeric(x)) scale(x) else x)) data &lt;- dataf data$y &lt;- ifelse(data$y == &quot;7&quot;, 1, 0) glimpse(data) ## Rows: 1,000 ## Columns: 3 ## $ y &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, … ## $ x_1 &lt;dbl&gt; -1.6192183, -0.2321362, -1.8274064, -0.5171458, 2.3938524, -1.5154… ## $ x_2 &lt;dbl&gt; -1.19265064, -2.30968844, -0.10549778, -0.74534398, 0.94485419, -0… glimpse(dataf) ## Rows: 1,000 ## Columns: 3 ## $ y &lt;fct&gt; 2, 7, 2, 2, 7, 2, 7, 7, 7, 2, 2, 7, 2, 2, 7, 2, 2, 2, 2, 2, 7, 2, … ## $ x_1 &lt;dbl&gt; -1.6192183, -0.2321362, -1.8274064, -0.5171458, 2.3938524, -1.5154… ## $ x_2 &lt;dbl&gt; -1.19265064, -2.30968844, -0.10549778, -0.74534398, 0.94485419, -0… 2.2 Parametric 2.2.1 LM auc_lm &lt;- c() n &lt;- 100 for (i in 1:n){ idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] mdl &lt;- lm(y ~ ., data = trn) phat &lt;- predict(mdl, tst) pred &lt;- prediction(phat, tst$y) auc_lm[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_lm # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3 Trees 2.3.1 CART auc_cart &lt;- c() n &lt;- 100 for (i in 1:n) { idx &lt;- unique(sample(nrow(dataf), size = nrow(dataf), replace = TRUE)) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] # Fit a CART model mdl &lt;- rpart(y ~ ., data = trn, method = &quot;class&quot;) # Predict probabilities. Adjust if your &#39;y&#39; variable is factor with levels other than 0 and 1 phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, tst$y) auc_cart[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_cart # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with CART&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3.2 Bagging auc_bag &lt;- c() n &lt;- 100 B &lt;- 100 num_vars &lt;- ncol(dataf) - 1 for (i in 1:n) { idx &lt;- sample(nrow(dataf), nrow(dataf), replace = TRUE) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] mdl &lt;- randomForest(y ~ ., data = trn, ntree = B, mtry = num_vars) phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, as.numeric(as.character(tst$y))) auc_bag[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_bag # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with Bagging&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3.3 RF auc_rf &lt;- c() n &lt;- 100 B &lt;- 100 for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(dataf), size = nrow(dataf), replace = TRUE)) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] # Fit a Random Forest model mdl &lt;- randomForest(y ~ ., data = trn, ntree = B) # Predict probabilities for the positive class phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, as.numeric(as.character(tst$y))) auc_rf[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_rf # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with RF&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4 Boosting 2.4.1 Adaboost This works now just make it like the others We also can just take gbm and switch it to adaboost ada_model &lt;- ada(y ~ ., data = dataf, iter = 100, nu = 0.1, control = rpart.control(maxdepth = 3)) summary(ada_model) ## Call: ## ada(y ~ ., data = dataf, iter = 100, nu = 0.1, control = rpart.control(maxdepth = 3)) ## ## Loss: exponential Method: discrete Iteration: 100 ## ## Training Results ## ## Accuracy: 0.855 Kappa: 0.709 2.4.2 GBM boost grid &lt;- expand.grid( n.trees = seq(100, 200, by = 100), # Number of trees interaction.depth = seq(1, 2, by = 1), # Max depth of trees shrinkage = seq(0.1, 0.2, by = 0.1) # Learning rate ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] n &lt;- 10 v &lt;- 3 results &lt;- matrix(nrow = n, ncol = 4) for (i in 1:n) { # Bootstrap sampling for training and test sets idx &lt;- sample(nrow(data), nrow(data), replace = TRUE) train_data &lt;- data[idx, ] test_data &lt;- data[-idx, ] auc_vg &lt;- c() for (j in 1:nrow(rgrid)) { auc_v &lt;- c() for (k in 1:v) { v_idx &lt;- sample(nrow(train_data), nrow(train_data), replace = TRUE) val_data &lt;- train_data[-v_idx, ] # Fit GBM model mdl &lt;- gbm(y ~ ., data = train_data[v_idx, ], distribution = &quot;bernoulli&quot;, n.trees = rgrid[j, &quot;n.trees&quot;], interaction.depth = rgrid[j, &quot;interaction.depth&quot;], shrinkage = rgrid[j, &quot;shrinkage&quot;], verbose = FALSE) # Predict on validation set and calculate AUC p &lt;- predict(mdl, newdata = val_data, n.trees = rgrid[j, &quot;n.trees&quot;], type = &quot;response&quot;) pred &lt;- prediction(p, val_data$y) auc_v[k] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc_vg[j] &lt;- mean(auc_v) } # Identify the best model best_idx &lt;- which.max(auc_vg) best_prm &lt;- rgrid[best_idx, ] # Train final model on the full training data and predict on test set mdl_final &lt;- gbm(y ~ ., data = train_data, distribution = &quot;bernoulli&quot;, n.trees = best_prm[1, &quot;n.trees&quot;], interaction.depth = best_prm[&quot;interaction.depth&quot;], shrinkage = best_prm[&quot;shrinkage&quot;], verbose = FALSE) p_t &lt;- predict(mdl_final, newdata = test_data, n.trees = best_prm[1, &quot;n.trees&quot;], type = &quot;response&quot;) pred_t &lt;- prediction(p_t, test_data$y) auc_test &lt;- performance(pred_t, &quot;auc&quot;)@y.values[[1]] results[i, 1] &lt;- auc_test results[i, 2] &lt;- best_prm[1, &quot;n.trees&quot;] results[i, 3] &lt;- best_prm[1, &quot;interaction.depth&quot;] results[i, 4] &lt;- best_prm[1, &quot;shrinkage&quot;] } df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, &quot;n.trees&quot;, &quot;interaction.depth&quot;, &quot;shrinkage&quot;) # Plotting plot(df_results$AUC_Test, col = &quot;red&quot;, main = &quot;AUC Test Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(df_results$AUC_Test), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(df_results$AUC_Test) - 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(df_results$AUC_Test) + 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.3 XGBoost grid &lt;- expand.grid( eta = seq(0.1, 0.2, by = 0.05), max_depth = seq(1, 2, by = 1), min_child_weight = seq(1, 1, by = 0), subsample = seq(1, 1, by = 0), colsample_bytree = seq(1, 1, by = 0), lambda = seq(0, 1, by = 1), alpha = seq(0, 1, by = 1), gamma = seq(0, 1, by = 1), nrounds = seq(100, 200, by = 100) ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] xs &lt;- model.matrix(~ . - 1 - y, data = data) y &lt;- data$y nc &lt;- 1 #detectCores - 1 n &lt;- 10 v &lt;- 5 # Adjust the matrix size according to the number of hyperparameters + 1 for AUC results &lt;- matrix(nrow = n, ncol = length(rgrid[1,]) + 1) for (i in 1:n) { idx &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) dx &lt;- xs[idx, ] dy &lt;- y[idx] tx &lt;- xs[-idx, ] ty &lt;- y[-idx] auc_vg &lt;- c() for (j in 1:nrow(rgrid)) { auc_v &lt;- c() for (k in 1:v) { v_idx &lt;- sample(nrow(dx), nrow(dx), replace = TRUE) vx &lt;- dx[v_idx, ] vy &lt;- dy[v_idx] val_x &lt;- dx[-v_idx, ] val_y &lt;- dy[-v_idx] prm &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = rgrid[j, &quot;max_depth&quot;], eta = rgrid[j, &quot;eta&quot;], subsample = rgrid[j, &quot;subsample&quot;], colsample_bytree = rgrid[j, &quot;colsample_bytree&quot;], gamma = rgrid[j, &quot;gamma&quot;], min_child_weight = rgrid[j, &quot;min_child_weight&quot;], alpha = rgrid[j, &quot;alpha&quot;], lambda = rgrid[j, &quot;lambda&quot;], nthread = nc ) dm_train &lt;- xgb.DMatrix(data = vx, label = vy) mdl &lt;- xgb.train(params = prm, data = dm_train, nrounds = rgrid[j, &quot;nrounds&quot;], verbose = FALSE) p &lt;- predict(mdl, xgb.DMatrix(data = val_x)) pred &lt;- prediction(p, val_y) auc_v &lt;- c(auc_v, performance(pred, &quot;auc&quot;)@y.values[[1]]) } auc_vg &lt;- c(auc_vg, mean(auc_v)) } best_idx &lt;- which.max(auc_vg) best_prm &lt;- rgrid[best_idx, ] best_prm_list &lt;- as.list(best_prm[-which(names(best_prm) == &quot;nrounds&quot;)]) best_prm_list$booster &lt;- &quot;gbtree&quot; best_prm_list$objective &lt;- &quot;binary:logistic&quot; best_prm_list$nthread &lt;- nc dm_final &lt;- xgb.DMatrix(data = dx, label = dy) dt_final &lt;- xgb.DMatrix(data = tx, label = ty) mdl_final &lt;- xgb.train(params = best_prm_list, data = dm_final, nrounds = best_prm[ ,&quot;nrounds&quot;], verbose = FALSE) p_t &lt;- predict(mdl_final, dt_final) pred_t &lt;- prediction(p_t, ty) auc_test &lt;- performance(pred_t, &quot;auc&quot;)@y.values[[1]] # Store AUC and hyperparameters in the results matrix results[i, 1] &lt;- auc_test results[i, 2:ncol(results)] &lt;- as.numeric(best_prm) } # Convert results to a dataframe for easy handling df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, names(rgrid[1,])) # Example of plotting, adjust as necessary plot(df_results$AUC_Test, col = &quot;red&quot;, main = &quot;AUC Test Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(df_results$AUC_Test), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(df_results$AUC_Test) - 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(df_results$AUC_Test) + 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.4 Light GBM boost 2.4.5 Catboost 2.5 Other Non-parametric 2.5.1 KNN k &lt;- seq(5, 50, 5) tauc &lt;- c() tuned_k &lt;- c() for (t in 1:25) { ind &lt;- sample(nrow(data), nrow(data)*0.8) mdata &lt;- data[ind, ] test &lt;- data[-ind, ] mauc &lt;- c() for(i in 1:length(k)) { auc &lt;- c() for(j in 1:2) { ind2 &lt;- sample(nrow(mdata), nrow(mdata), replace = TRUE) train &lt;- mdata[ind2, ] val &lt;- mdata[-ind2, ] model &lt;- knn3(y ~ ., data = train, k = k[i]) phat &lt;- predict(model, val, type = &quot;prob&quot;)[,2] pred_rocr &lt;- prediction(phat, val$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[j] &lt;- auc_ROCR@y.values[[1]] } mauc[i] &lt;- mean(auc) } tuned_k &lt;- k[which.max(mauc)] model &lt;- knn3(y ~ ., data = mdata, k = tuned_k) phat &lt;- predict(model, test, type = &quot;prob&quot;)[,2] pred_rocr &lt;- prediction(phat, test$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) tauc[t] &lt;- auc_ROCR@y.values[[1]] } auc &lt;- tauc # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with KNN3&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.5.2 Nueralnet size &lt;- c(seq(2, 3, 1)) decay &lt;- c(seq(0.01, 0.25, 0.05)) maxit &lt;- c(seq(100, 900, 400)) grid &lt;- expand.grid(size, decay, maxit) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] n &lt;- 100 v &lt;- 5 # store out best values via validation scores opt &lt;- matrix(0, nrow = n, ncol = 5) colnames(opt) &lt;- c(&quot;size&quot;, &quot;decay&quot;, &quot;maxit&quot;, &quot;AUC_val&quot;, &quot;AUC_TEST&quot;) for (j in 1:n){ # put aside data for final test. creat md and test ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) md &lt;- data[ind, ] test &lt;- data[-ind, ] auc_runs &lt;- c() for (i in 1:nrow(rgrid)){ #cat(&quot;loops: &quot;, j, i, &quot;\\r&quot;) auc_tuning &lt;- c() for (p in 1:v){ # bootstrap from md to make a train and val set idx &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE)) train &lt;- md[idx,] val &lt;- md[-idx, ] # model on the train data model &lt;- nnet(y ~ ., data = train, trace = FALSE, act.fct = &quot;logistic&quot;, size = rgrid[i, 1], decay = rgrid[i, 2], maxit = rgrid[i, 3] ) # predict on the val data phat &lt;- predict(model, val) # find the auc pred_rocr &lt;- prediction(phat, val$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_tuning[p] &lt;- auc_ROCR@y.values[[1]] } auc_runs[i] &lt;- mean(auc_tuning) #take the mean of v runs for that one specific hyper parameter } # index the best hyper parameters BI &lt;- which.max(auc_runs) best_AUC &lt;- auc_runs[BI] best_params &lt;- rgrid[BI, ] # store the best hyper parames based on the mean aucs opt[j, 1] &lt;- best_params[1, 1] opt[j, 2] &lt;- best_params[1, 2] opt[j, 3] &lt;- best_params[1, 3] opt[j, 4] &lt;- best_AUC # model with the md data model &lt;- nnet(y ~ ., data = md, trace = FALSE, act.fct = &quot;logistic&quot;, size = opt[j, 1], decay = opt[j, 2], maxit = opt[j, 3] ) # predict the set aside test set phat_t &lt;- predict(model, test) # get the test auc pred_rocr &lt;- prediction(phat_t, test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_test &lt;- auc_ROCR@y.values[[1]] # store the test auc opt[j, 5] &lt;- auc_test } auc &lt;- opt[,5] # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with nnet&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.5.3 SVM "],["binary-application.html", "Chapter 3 Binary Application 3.1 Data 3.2 Model Selection 3.3 Comparisions 3.4 Statistic selection 3.5 Confusion table and ROC", " Chapter 3 Binary Application I will compare results in a different data set that predicts if income is over 50k (1) or not (0) suppressMessages(library(readr)) suppressMessages(library(caret)) suppressMessages(library(ROCR)) suppressMessages(library(xgboost)) suppressMessages(library(foreach)) suppressMessages(library(doParallel)) suppressMessages(library(Matrix)) suppressMessages(library(dplyr)) suppressMessages(library(tidyverse)) suppressMessages(library(forcats)) suppressMessages(library(DataExplorer)) suppressMessages(library(randomForest)) First, we import all necessary libraries for data manipulation, visualization, and machine learning. 3.1 Data 3.1.1 Data Prep data &lt;- read_csv(&quot;adult_train.csv&quot;, col_names = FALSE) ## Rows: 32561 Columns: 15 ## ── Column specification ───────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (9): X2, X4, X6, X7, X8, X9, X10, X14, X15 ## dbl (6): X1, X3, X5, X11, X12, X13 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. colnames(data) &lt;- c(&quot;age&quot;, &quot;workclass&quot;, &quot;fnlwgt&quot;, &quot;education&quot;, &quot;education_num&quot;, &quot;marital_status&quot;, &quot;occupation&quot;, &quot;relationship&quot;, &quot;race&quot;, &quot;sex&quot;, &quot;capital_gain&quot;, &quot;capital_loss&quot;, &quot;hours_per_week&quot;, &quot;native_country&quot;, &quot;income&quot;) glimpse(data) ## Rows: 32,561 ## Columns: 15 ## $ age &lt;dbl&gt; 39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32,… ## $ workclass &lt;chr&gt; &quot;State-gov&quot;, &quot;Self-emp-not-inc&quot;, &quot;Private&quot;, &quot;Private&quot;, … ## $ fnlwgt &lt;dbl&gt; 77516, 83311, 215646, 234721, 338409, 284582, 160187, 2… ## $ education &lt;chr&gt; &quot;Bachelors&quot;, &quot;Bachelors&quot;, &quot;HS-grad&quot;, &quot;11th&quot;, &quot;Bachelors… ## $ education_num &lt;dbl&gt; 13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11,… ## $ marital_status &lt;chr&gt; &quot;Never-married&quot;, &quot;Married-civ-spouse&quot;, &quot;Divorced&quot;, &quot;Mar… ## $ occupation &lt;chr&gt; &quot;Adm-clerical&quot;, &quot;Exec-managerial&quot;, &quot;Handlers-cleaners&quot;,… ## $ relationship &lt;chr&gt; &quot;Not-in-family&quot;, &quot;Husband&quot;, &quot;Not-in-family&quot;, &quot;Husband&quot;,… ## $ race &lt;chr&gt; &quot;White&quot;, &quot;White&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;White&quot;, &quot;… ## $ sex &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Fe… ## $ capital_gain &lt;dbl&gt; 2174, 0, 0, 0, 0, 0, 0, 0, 14084, 5178, 0, 0, 0, 0, 0, … ## $ capital_loss &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hours_per_week &lt;dbl&gt; 40, 13, 40, 40, 40, 40, 16, 45, 50, 40, 80, 40, 30, 50,… ## $ native_country &lt;chr&gt; &quot;United-States&quot;, &quot;United-States&quot;, &quot;United-States&quot;, &quot;Uni… ## $ income &lt;chr&gt; &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;… table(data$income) ## ## &lt;=50K &gt;50K ## 24720 7841 Identify columns containing “?” and count their occurrence. Then, transform character columns to factors and adjust the ‘income’ column for binary classification. question_mark_counts_dplyr &lt;- data %&gt;% summarise(across(everything(), ~sum(. == &quot;?&quot;, na.rm = TRUE))) %&gt;% select(where(~. &gt; 0)) question_mark_counts_dplyr ## # A tibble: 1 × 3 ## workclass occupation native_country ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1836 1843 583 data_transformed &lt;- data %&gt;% mutate(across(where(is.character), as.factor)) %&gt;% mutate(income_binary = if_else(income == &quot;&gt;50K&quot;, 1, 0)) %&gt;% select(-income) %&gt;% select(income_binary, everything()) df &lt;- rename(data_transformed, y = income_binary) df$native_country &lt;- fct_lump(df$native_country, n = 3, other_level = &quot;other&quot;) sapply(df, function(col) { if(is.factor(col)) { sorted_tab &lt;- sort(table(col)) return(sorted_tab) } }) ## $y ## NULL ## ## $age ## NULL ## ## $workclass ## col ## Never-worked Without-pay Federal-gov Self-emp-inc ## 7 14 960 1116 ## State-gov ? Local-gov Self-emp-not-inc ## 1298 1836 2093 2541 ## Private ## 22696 ## ## $fnlwgt ## NULL ## ## $education ## col ## Preschool 1st-4th 5th-6th Doctorate 12th 9th ## 51 168 333 413 433 514 ## Prof-school 7th-8th 10th Assoc-acdm 11th Assoc-voc ## 576 646 933 1067 1175 1382 ## Masters Bachelors Some-college HS-grad ## 1723 5355 7291 10501 ## ## $education_num ## NULL ## ## $marital_status ## col ## Married-AF-spouse Married-spouse-absent Widowed ## 23 418 993 ## Separated Divorced Never-married ## 1025 4443 10683 ## Married-civ-spouse ## 14976 ## ## $occupation ## col ## Armed-Forces Priv-house-serv Protective-serv Tech-support ## 9 149 649 928 ## Farming-fishing Handlers-cleaners Transport-moving ? ## 994 1370 1597 1843 ## Machine-op-inspct Other-service Sales Adm-clerical ## 2002 3295 3650 3770 ## Exec-managerial Craft-repair Prof-specialty ## 4066 4099 4140 ## ## $relationship ## col ## Other-relative Wife Unmarried Own-child Not-in-family ## 981 1568 3446 5068 8305 ## Husband ## 13193 ## ## $race ## col ## Other Amer-Indian-Eskimo Asian-Pac-Islander Black ## 271 311 1039 3124 ## White ## 27816 ## ## $sex ## col ## Female Male ## 10771 21790 ## ## $capital_gain ## NULL ## ## $capital_loss ## NULL ## ## $hours_per_week ## NULL ## ## $native_country ## col ## ? Mexico other United-States ## 583 643 2165 29170 Combine some levels df$education &lt;- as.factor(ifelse(df$education %in% c(&quot;Preschool&quot;, &quot;1st-4th&quot;), &quot;Early Childhood&quot;, as.character(df$education))) df$workclass &lt;- as.factor(ifelse(df$workclass %in% c(&quot;Never-worked&quot;, &quot;Without-pay&quot;), &quot;Unemployed&quot;, as.character(df$workclass))) df$marital_status &lt;- as.factor(ifelse(df$marital_status == &quot;Married-AF-spouse&quot;, &quot;Married-spouse-absent&quot;, as.character(df$marital_status))) df$occupation &lt;- as.factor(ifelse(df$occupation == &quot;Armed-Forces&quot;, &quot;Protective-serv&quot;, as.character(df$occupation))) glimpse(df) ## Rows: 32,561 ## Columns: 15 ## $ y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0… ## $ age &lt;dbl&gt; 39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32,… ## $ workclass &lt;fct&gt; State-gov, Self-emp-not-inc, Private, Private, Private,… ## $ fnlwgt &lt;dbl&gt; 77516, 83311, 215646, 234721, 338409, 284582, 160187, 2… ## $ education &lt;fct&gt; Bachelors, Bachelors, HS-grad, 11th, Bachelors, Masters… ## $ education_num &lt;dbl&gt; 13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11,… ## $ marital_status &lt;fct&gt; Never-married, Married-civ-spouse, Divorced, Married-ci… ## $ occupation &lt;fct&gt; Adm-clerical, Exec-managerial, Handlers-cleaners, Handl… ## $ relationship &lt;fct&gt; Not-in-family, Husband, Not-in-family, Husband, Wife, W… ## $ race &lt;fct&gt; White, White, White, Black, Black, White, Black, White,… ## $ sex &lt;fct&gt; Male, Male, Male, Male, Female, Female, Female, Male, F… ## $ capital_gain &lt;dbl&gt; 2174, 0, 0, 0, 0, 0, 0, 0, 14084, 5178, 0, 0, 0, 0, 0, … ## $ capital_loss &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hours_per_week &lt;dbl&gt; 40, 13, 40, 40, 40, 40, 16, 45, 50, 40, 80, 40, 30, 50,… ## $ native_country &lt;fct&gt; United-States, United-States, United-States, United-Sta… df_numeric &lt;- df %&gt;% select(where(is.numeric)) df_factor &lt;- df %&gt;% select(where(is.factor)) plot_correlation(df_numeric) plot_correlation(cbind(df_factor, df$y)) numeric_columns &lt;- sapply(df, is.numeric) &amp; names(df) != &quot;y&quot; dfs &lt;- df dfs[numeric_columns] &lt;- scale(df[numeric_columns]) head(dfs) ## # A tibble: 6 × 15 ## y age workclass fnlwgt education education_num marital_status ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 0.0307 State-gov -1.06 Bachelors 1.13 Never-married ## 2 0 0.837 Self-emp-not-inc -1.01 Bachelors 1.13 Married-civ-spo… ## 3 0 -0.0426 Private 0.245 HS-grad -0.420 Divorced ## 4 0 1.06 Private 0.426 11th -1.20 Married-civ-spo… ## 5 0 -0.776 Private 1.41 Bachelors 1.13 Married-civ-spo… ## 6 0 -0.116 Private 0.898 Masters 1.52 Married-civ-spo… ## # ℹ 8 more variables: occupation &lt;fct&gt;, relationship &lt;fct&gt;, race &lt;fct&gt;, ## # sex &lt;fct&gt;, capital_gain &lt;dbl&gt;, capital_loss &lt;dbl&gt;, hours_per_week &lt;dbl&gt;, ## # native_country &lt;fct&gt; X &lt;- model.matrix(~ . -1 - y, data = dfs) y &lt;- dfs$y xs &lt;- Matrix(X, sparse = TRUE) 3.2 Model Selection 3.2.1 Functions run_xgb &lt;- function(xs, y, grd, v) { # Setup parallel computing nc &lt;- detectCores() - 1 cl &lt;- makeCluster(nc) registerDoParallel(cl) results &lt;- foreach(g = 1:nrow(grd), .combine=&#39;rbind&#39;, .packages=c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { auc &lt;- numeric(v) # AUC scores for validations for (k in 1:v) { # Bootstrap sampling for validation idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) tr_x &lt;- xs[idx, ] tr_y &lt;- y[idx] vl_x &lt;- xs[-idx, ] vl_y &lt;- y[-idx] # Model training for validation prms &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = grd[g, &quot;max_depth&quot;], eta = grd[g, &quot;eta&quot;], subsample = grd[g, &quot;subsample&quot;], colsample_bytree = grd[g, &quot;colsample_bytree&quot;], gamma = grd[g, &quot;gamma&quot;], min_child_weight = grd[g, &quot;min_child_weight&quot;], alpha = grd[g, &quot;alpha&quot;], lambda = grd[g, &quot;lambda&quot;] ) xgb_tr &lt;- xgb.DMatrix(data = tr_x, label = tr_y) xm &lt;- xgb.train(params = prms, data = xgb_tr, nrounds = grd[g, &quot;nrounds&quot;], verbose = FALSE, nthread = 1) # AUC calculation for validation phat &lt;- predict(xm, xgb.DMatrix(data = vl_x)) pred &lt;- prediction(phat, vl_y) auc[k] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } # AUC mean and params auc_mean &lt;- mean(auc) c(grd[g, ], auc_mean) } # Stop the cluster stopCluster(cl) # Convert results to a tibble and set column names res &lt;- as_tibble(results) names(res) &lt;- c(names(grd), &quot;AUC_Mean&quot;) res &lt;- res %&gt;% mutate(across(everything(), ~unlist(.))) return(res) } test_top_hp &lt;- function(hp, xs, y, t) { # Setup parallel computing nc &lt;- detectCores() - 1 cl &lt;- makeCluster(nc) registerDoParallel(cl) # Testing loop ts_res &lt;- foreach(h = 1:nrow(hp), .combine = &#39;rbind&#39;, .packages = c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { aucs &lt;- numeric(t) for (i in 1:t) { # Bootstrap sampling for testing idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) mdx &lt;- xs[idx, ] mdy &lt;- y[idx] tx &lt;- xs[-idx, ] ty &lt;- y[-idx] # Parameters for the model prms &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = hp[h, &quot;max_depth&quot;], eta = hp[h, &quot;eta&quot;], subsample = hp[h, &quot;subsample&quot;], colsample_bytree = hp[h, &quot;colsample_bytree&quot;], gamma = hp[h, &quot;gamma&quot;], min_child_weight = hp[h, &quot;min_child_weight&quot;], alpha = hp[h, &quot;alpha&quot;], lambda = hp[h, &quot;lambda&quot;], nrounds = hp[h, &quot;nrounds&quot;] ) # Train and test the model dtr &lt;- xgb.DMatrix(data = mdx, label = mdy) dte &lt;- xgb.DMatrix(data = tx, label = ty) mdl &lt;- xgb.train(params = prms, data = dtr, nrounds = prms$nrounds, verbose = 0, nthread = 1) # AUC calculation ph &lt;- predict(mdl, dte) pr &lt;- prediction(ph, ty) aucs[i] &lt;- performance(pr, &quot;auc&quot;)@y.values[[1]] } # Average AUC and combine with hyperparameters c(hp[h, ], mean_auc = mean(aucs)) } # Stop the cluster stopCluster(cl) # Convert to tibble and unlist columns ts_tbl &lt;- as_tibble(ts_res) %&gt;% mutate(across(everything(), ~unlist(.))) return(ts_tbl) } test_auc &lt;- function(xs, y, best_hp, runs) { aucs &lt;- numeric(runs) for (i in 1:runs) { # Bootstrap sampling for testing idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) md_x &lt;- xs[idx, ] md_y &lt;- y[idx] test_x &lt;- xs[-idx, ] test_y &lt;- y[-idx] # Set parameters for the model params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = best_hp$max_depth, eta = best_hp$eta, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, gamma = best_hp$gamma, min_child_weight = best_hp$min_child_weight, alpha = best_hp$alpha, lambda = best_hp$lambda ) # Train and test the model md &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) model &lt;- xgb.train(params = params, data = md, nrounds = best_hp$nrounds, verbose = 0, nthread = 1) # Calculate AUC phat &lt;- predict(model, dtest) pred &lt;- prediction(phat, test_y) aucs[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } return(aucs) } opt_nrd4eta &lt;- function(xs, y, params, o , grd) { # Assuming xgb.train and related functions are already available (via library(xgboost)) library(xgboost) library(ROCR) # Detect the number of cores numCores &lt;- detectCores() - 1 # Initialize an AUC matrix aucm &lt;- matrix(0, nrow = o, ncol = length(grd)) start_time &lt;- Sys.time() for (j in 1:o){ # Creating a bootstrap sample ind &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) dm &lt;- xgb.DMatrix(data = xs[ind, ], label = y[ind]) dv &lt;- xgb.DMatrix(data = xs[-ind, ], label = y[-ind]) auc &lt;- c() for (i in 1:length(grd)){ # Training the model on the bootstrap sample bsm &lt;- xgb.train(params = params, data = dm, nrounds = grd[i], verbose = FALSE, nthread = numCores ) # Predict on the validation set and calculate AUC phat &lt;- predict(bsm, dv, type = &quot;prob&quot;) # Calculating the AUC pred_rocr &lt;- prediction(phat, y[-ind]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] } aucm[j, ] &lt;- auc } evalauc &lt;- colMeans(aucm) # Plotting plot(grd, evalauc, type = &quot;b&quot;, col = &quot;blue&quot;, xlab = &quot;Number of Rounds&quot;, ylab = &quot;AUC&quot;, main = &quot;AUC over o rounds vs Number of Rounds in XGBoost&quot;) best_nrounds &lt;- grd[which.max(evalauc)] max_auc &lt;- max(evalauc) end_time &lt;- Sys.time() elapsed_time &lt;- end_time - start_time return(list(best_nrounds = best_nrounds, max_auc = max_auc, elapsed_time = elapsed_time)) } 3.2.2 XGB # Define XGBoost parameters params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.05 ) grd &lt;- seq(1, 500, by = 50) o &lt;- 5 rs &lt;- opt_nrd4eta(xs = xs, y = y, params = params, o, grd) This is a very small grid and cuts out alot of tuning power grid &lt;- expand.grid( eta = seq(0.04, 0.06, by = 0.02), max_depth = seq(6, 8, by = 2), min_child_weight = seq(1, 1, by = 0), subsample = seq(0.8, 1, by = 0.2), colsample_bytree = seq(1, 1, by = 0), lambda = seq(2, 5, by = 3), alpha = seq(0, 2, by = 2), gamma = seq(0, 2, by = 2), nrounds = seq(300, 500, by = 100) ) conf_lev &lt;- .95 num_max &lt;- 5 n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] v &lt;- 5 #This number should be much higher vr &lt;- run_xgb(xs, y, rgrid, v) print(head(vr), width = Inf) ## # A tibble: 6 × 10 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.06 6 1 0.8 1 2 0 0 ## 2 0.04 8 1 0.8 1 2 0 0 ## 3 0.06 6 1 1 1 2 2 0 ## 4 0.06 6 1 1 1 5 2 0 ## 5 0.06 6 1 1 1 5 2 2 ## 6 0.04 8 1 0.8 1 2 2 2 ## nrounds AUC_Mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 400 0.926 ## 2 300 0.926 ## 3 400 0.928 ## 4 500 0.925 ## 5 300 0.927 ## 6 400 0.925 We will take the top 10% and do a more thorough search # Sort vr by AUC_Mean and select the top 25% top_hp &lt;- vr %&gt;% arrange(desc(AUC_Mean)) %&gt;% slice_head(prop = 0.05) t &lt;- 25 ts_res &lt;- test_top_hp(top_hp, xs, y, t) print(head(ts_res), width = Inf) ## # A tibble: 5 × 11 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.04 8 1 1 1 5 2 2 ## 2 0.04 6 1 1 1 5 0 0 ## 3 0.04 6 1 1 1 2 0 0 ## 4 0.06 6 1 1 1 5 0 2 ## 5 0.04 6 1 0.8 1 2 0 2 ## nrounds AUC_Mean mean_auc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 500 0.929 0.926 ## 2 500 0.929 0.927 ## 3 400 0.929 0.928 ## 4 500 0.929 0.927 ## 5 500 0.929 0.927 Final AUC pres best_hp &lt;- ts_res %&gt;% dplyr::arrange(desc(mean_auc)) %&gt;% dplyr::slice(1) print(best_hp, width = Inf) ## # A tibble: 1 × 11 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.04 6 1 1 1 2 0 0 ## nrounds AUC_Mean mean_auc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 400 0.929 0.928 r &lt;- 100 auc &lt;- test_auc(xs, y, best_hp, r) mauc &lt;- mean(auc) mauc ## [1] 0.9283498 sd(auc) ## [1] 0.001807057 plot(auc, col=&quot;red&quot;) abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) 3.3 Comparisions Random Forest rfd &lt;- dfs rfd$y &lt;- as.factor(rfd$y) B &lt;- 120 n &lt;- 100 obs &lt;- nrow(data) numCores &lt;- detectCores()-1 cl &lt;- makeCluster(numCores) registerDoParallel(cl) lst &lt;- foreach(i=1:n, .packages = c(&quot;randomForest&quot;, &quot;ROCR&quot;)) %dopar% { tryCatch({ idx &lt;- unique(sample(obs, obs, replace = TRUE)) train &lt;- rfd[idx,] test &lt;- rfd[-idx, ] model &lt;- randomForest(y ~ ., ntree = B, data = train) phat &lt;- predict(model, test, type = &quot;prob&quot;) pred_rocr &lt;- prediction(phat[,2], test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] }, error = function(e) { NA # Return NA on error }) } stopCluster(cl) # combine the results auc &lt;- unlist(lst) auc &lt;- na.omit(auc) # plot auc and mean plot(auc, col=&quot;red&quot;) abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) LPM data_lm &lt;- rfd data_lm$y &lt;- as.numeric(data_lm$y) data_lm$y &lt;- data_lm$y - 1 # bring it back to 1 and 0s n &lt;- 100 obs &lt;- nrow(data_lm) numCores &lt;- detectCores() - 1 cl &lt;- makeCluster(numCores) registerDoParallel(cl) auc_list &lt;- foreach(i = 1:n, .packages = &quot;ROCR&quot;) %dopar% { tryCatch({ idx &lt;- unique(sample(obs, obs, replace = TRUE)) trn &lt;- data_lm[idx, ] # Training data tst &lt;- data_lm[-idx, ] # Test data mdl &lt;- lm(y ~ ., data = trn) phat &lt;- predict(mdl, tst) pred &lt;- prediction(phat, tst$y) performance(pred, &quot;auc&quot;)@y.values[[1]] }, error = function(e) { NA # Return NA on error }) } stopCluster(cl) # Process the AUC results auc_values &lt;- unlist(auc_list) auc_values &lt;- na.omit(auc_values) mean_auc &lt;- mean(auc_values) sd_auc &lt;- sd(auc_values) # Plot AUC values, mean, and confidence intervals plot(auc_values, col = &quot;red&quot;, main = &quot;AUC Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean_auc, col = &quot;blue&quot;, lwd = 2, lty = 2) # Mean line abline(h = mean_auc - 1.96 * sd_auc, col = &quot;green&quot;, lwd = 2, lty = 3) # Lower CI abline(h = mean_auc + 1.96 * sd_auc, col = &quot;green&quot;, lwd = 2, lty = 3) # Upper CI 3.4 Statistic selection We are going to use jstat but you can pick another confusion table metric if you want r &lt;- 100 best_thresholds &lt;- c() j_stats &lt;- c() for(i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Extract parameters for the model from best_hp params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = best_hp$eta, max_depth = best_hp$max_depth, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, min_child_weight = best_hp$min_child_weight, gamma = best_hp$gamma, alpha = best_hp$alpha, lambda = best_hp$lambda ) # Train the model xgbmdl &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions phat &lt;- predict(xgbmdl, dtest) # ROCR predictions object pred &lt;- prediction(phat, test_y) # Calculate performance measures perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;spec&quot;) sensitivity &lt;- slot(perf, &quot;y.values&quot;)[[1]] specificity &lt;- slot(perf, &quot;x.values&quot;)[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] j_stat &lt;- sensitivity + specificity - 1 # Find the best threshold (maximizing J statistic) best_idx &lt;- which.max(j_stat) best_thresholds[i] &lt;- thresholds[best_idx] j_stats[i] &lt;- j_stat[best_idx] } # Calculate and print the average of the best thresholds avg_best_threshold &lt;- mean(best_thresholds) cat(&quot;Average Best Threshold:&quot;, avg_best_threshold, &quot;\\n&quot;) ## Average Best Threshold: 0.2297591 # Plot the distribution of max J statistics for each iteration hist(j_stats, col = &quot;blue&quot;, xlab = &quot;Max J Statistic&quot;, ylab = &quot;Frequency&quot;, main = &quot;Distribution of Max J Statistics Across Bootstrap Iterations&quot;) abline(v = mean(j_stats), col = &quot;red&quot;, lwd = 2) # Mean J Stat line 3.5 Confusion table and ROC Avg confusion table. we should do a CI later # Initialize storage for aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) # Initialize lists to store TPR and FPR values for each bootstrap iteration all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:100) { ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) final_phat &lt;- predict(xgbmdl_final, dtest) final_predicted_classes &lt;- ifelse(final_phat &gt; avg_best_threshold, 1, 0) final_test_y_binary &lt;- ifelse(test_y &gt; avg_best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(final_test_y_binary, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) final_pred_rocr &lt;- prediction(final_phat, test_y) final_perf_rocr &lt;- performance(final_pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) # Store TPR and FPR values for this iteration all_tpr[[i]] &lt;- final_perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- final_perf_rocr@x.values[[1]] } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_mat_totals / 100 print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 2512.61 1680.54 ## Predicted_0 372.22 7414.28 This doesn’t have a CI but its a average # Determine common FPR thresholds for interpolation (simplified approach) common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line Massive notes I want to acknowledge that j statistic is not be the best idea in this case. I used it so we can see our confusion table and then adjust in the future to what statistic we may desire. We should try f statistic due to the imbalance "],["regressional-application.html", "Chapter 4 Regressional Application 4.1 Data 4.2 Model Selection 4.3 Statistic selection", " Chapter 4 Regressional Application 4.1 Data 4.1.1 Data Prep 4.1.2 Data Explorations 4.2 Model Selection 4.3 Statistic selection "],["imbalanced-application.html", "Chapter 5 Imbalanced Application 5.1 Data 5.2 Approaches 5.3 Statistic selection", " Chapter 5 Imbalanced Application 5.1 Data library(readr) library(dplyr) library(randomForest) library(ROCR) library(smotefamily) data &lt;- read_csv(&quot;HTRU_2.csv&quot;, col_names = FALSE) ## Rows: 17898 Columns: 9 ## ── Column specification ───────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (9): X1, X2, X3, X4, X5, X6, X7, X8, X9 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Renaming X9 to y and moving it to the first position df &lt;- data.frame(y = data$X9, data[, names(data) != &quot;X9&quot;]) # Converting y to a factor with levels 1 and 0 df$y &lt;- factor(df$y, levels = c(&quot;1&quot;, &quot;0&quot;)) glimpse(df) ## Rows: 17,898 ## Columns: 9 ## $ y &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0… ## $ X1 &lt;dbl&gt; 140.56250, 102.50781, 103.01562, 136.75000, 88.72656, 93.57031, 119… ## $ X2 &lt;dbl&gt; 55.68378, 58.88243, 39.34165, 57.17845, 40.67223, 46.69811, 48.7650… ## $ X3 &lt;dbl&gt; -0.23457141, 0.46531815, 0.32332837, -0.06841464, 0.60086608, 0.531… ## $ X4 &lt;dbl&gt; -0.69964840, -0.51508791, 1.05116443, -0.63623837, 1.12349169, 0.41… ## $ X5 &lt;dbl&gt; 3.1998328, 1.6772575, 3.1212375, 3.6429766, 1.1789298, 1.6362876, 0… ## $ X6 &lt;dbl&gt; 19.110426, 14.860146, 21.744669, 20.959280, 11.468720, 14.545074, 9… ## $ X7 &lt;dbl&gt; 7.975532, 10.576487, 7.735822, 6.896499, 14.269573, 10.621748, 19.2… ## $ X8 &lt;dbl&gt; 74.24222, 127.39358, 63.17191, 53.59366, 252.56731, 131.39400, 479.… table(df$y) ## ## 1 0 ## 1639 16259 df_y1 &lt;- subset(df, y == 1) df_yNot1 &lt;- subset(df, y != 1) df_y1_sampled &lt;- df_y1[sample(nrow(df_y1), size = ceiling(0.1 * nrow(df_y1))), ] df &lt;- rbind(df_yNot1, df_y1_sampled) table(df$y) ## ## 1 0 ## 164 16259 5.2 Approaches 5.2.1 No changes library(randomForest) library(ROCR) B &lt;- 100 n &lt;- 100 # Initialize storage for AUC values and ROC curve data auc &lt;- numeric(n) all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { # Bootstrap sampling idx &lt;- sample(nrow(df), size = nrow(df), replace = TRUE) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Training the RandomForest model model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predicting probabilities phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculating AUC pred_rocr &lt;- prediction(predictions = phat[,1], labels = test$y) # Assuming class 1 probabilities are in the first column auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Constructing ROC curves perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.2 Undersample library(randomForest) library(ROCR) library(dplyr) B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) # Store AUC values all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Downsampling the majority class d1 &lt;- subset(train, y == 1) d0 &lt;- subset(train, y == 0) d0s &lt;- d0[sample(nrow(d0), nrow(d1), replace = TRUE), ] train &lt;- rbind(d1, d0s) %&gt;% sample_n(nrow(d1) * 2) # RandomForest model training model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predicting probabilities phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculating AUC pred_rocr &lt;- prediction(phat[,1], test$y) # Ensure correct column for class probabilities auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Calculating TPR and FPR for ROC perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.3 Oversampling B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) # To store AUC values all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Manual oversampling of the minority class d1 &lt;- subset(train, y == 1) d0 &lt;- subset(train, y == 0) d1s &lt;- d1[sample(nrow(d1), nrow(d0), replace = TRUE), ] train &lt;- rbind(d1s, d0) # Combined oversampled positives with all negatives train &lt;- train[sample(nrow(train)), ] # Shuffle the rows # Model training model &lt;- randomForest(y ~ ., data = train, ntree = B) phat &lt;- predict(model, test, type = &quot;prob&quot;) # AUC calculation pred_rocr &lt;- prediction(phat[,1], test$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # ROC Curve Calculation perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.4 Smoteing Smote has to be in the loop on the train data B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx,] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predict probabilities on the test set phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculate AUC pred_rocr &lt;- prediction(phat[,2], test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Calculate TPR and FPR for ROC perf_rocr &lt;- performance(pred_rocr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.3 Statistic selection Obviously our data isnt balenced so we cannot use ACC or J stat for our threashold selection n &lt;- 100 # Number of iterations b &lt;- 100 # Number of trees in the RandomForest model # Placeholder for the aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2, dimnames = list(c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;), c(&quot;Actual_1&quot;, &quot;Actual_0&quot;))) bt &lt;- c() for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx, ] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model rf_model &lt;- randomForest(y ~ ., data = train, ntree = b) # Predict probabilities on the test set phat &lt;- predict(rf_model, newdata = test, type = &quot;prob&quot;)[,2] # Calculate ROC curve pred_rocr &lt;- prediction(predictions = phat, labels = test$y) # Calculate performance measures for Youden&#39;s J statistic perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;spec&quot;) sensitivity &lt;- perf@y.values[[1]] specificity &lt;- perf@x.values[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] j_values &lt;- sensitivity + specificity - 1 # Find the best threshold best_threshold_index &lt;- which.max(j_values) best_threshold &lt;- thresholds[best_threshold_index] bt[i] &lt;- best_threshold # Make predictions based on the best threshold final_predicted_classes &lt;- ifelse(phat &gt; best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test$y, levels = c(1, 0))) # Update the confusion matrix conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average confusion matrix after all iterations avg_conf_matrix &lt;- conf_mat_totals / n print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 51.54 92.74 ## Predicted_0 7.63 5890.55 mean(bt) ## [1] 0.1387 That has issues We will use F1 for our threshold n &lt;- 100 # Number of iterations b &lt;- 100 # Number of trees in the RandomForest model best_f1_scores &lt;- c() bt &lt;- c() conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2, dimnames = list(c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;), c(&quot;Actual_1&quot;, &quot;Actual_0&quot;))) for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx, ] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model rf_model &lt;- randomForest(y ~ ., data = train, ntree = b) # Predict probabilities on the test set phat &lt;- predict(rf_model, newdata = test, type = &quot;prob&quot;)[,2] # Calculate ROC curve pred_rocr &lt;- prediction(predictions = phat, labels = test$y) perf &lt;- performance(pred_rocr, measure = &quot;prec&quot;, x.measure = &quot;rec&quot;) precision &lt;- perf@y.values[[1]] recall &lt;- perf@x.values[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] # Calculate F1 scores for each threshold f1_scores &lt;- 2 * (precision * recall) / (precision + recall) # Find the best threshold based on F1 score best_f1_index &lt;- which.max(f1_scores) best_f1 &lt;- f1_scores[best_f1_index] best_threshold &lt;- thresholds[best_f1_index] bt[i] &lt;- best_threshold # Store the best F1 score for this iteration best_f1_scores[i] &lt;- best_f1 # Make predictions based on the best threshold final_predicted_classes &lt;- ifelse(phat &gt; best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test$y, levels = c(1, 0))) # Update the confusion matrix totals with the current confusion matrix conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average of the best F1 scores after all iterations avg_best_f1_score &lt;- mean(best_f1_scores) print(paste(&quot;Average Best F1 Score:&quot;, avg_best_f1_score)) ## [1] &quot;Average Best F1 Score: 0.783269701011343&quot; # Calculate the average confusion matrix after all iterations avg_conf_matrix &lt;- conf_mat_totals / n print(&quot;Average Confusion Matrix:&quot;) ## [1] &quot;Average Confusion Matrix:&quot; print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 44.57 10.23 ## Predicted_0 15.97 5972.49 mean(bt) ## [1] 0.6467 We could do more analysis but this sets a good understanding and basline of what we were trying to accomplish As a note for roc curves and confusion tables these are averages not CIs which we should do aswell. "],["speeds.html", "Chapter 6 Speeds 6.1 Functions to run the tests 6.2 10,000 rows 6.3 100,000 rows 6.4 500,000 rows 6.5 Presentations 6.6 Analysis", " Chapter 6 Speeds library(xgboost) library(ROCR) library(foreach) library(doParallel) library(Matrix) library(readr) library(dplyr) library(ggplot2) library(tidyr) library(dplyr) 6.1 Functions to run the tests I will make 3 functions. One with my parallel processing, one with xgboost pp and then one with no pp at all so n thread = 1. I have included the code so that you can see it but it is not important just know that we are looping r times and that the loops use bootstrapping for splits. xgb1 &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer auc &lt;- numeric(r) # Pre-allocate a numeric vector for AUC values for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] test_y &lt;- y[-ind] # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = 1) predictions &lt;- predict(model, dtest) pred &lt;- ROCR::prediction(predictions, test_y) perf &lt;- ROCR::performance(pred, &quot;auc&quot;) auc[i] &lt;- perf@y.values[[1]] } end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(auc = auc, time_taken = time_taken) } xgb2 &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer auc &lt;- numeric(r) # Initialize the AUC vector to store AUC values for each iteration for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] # Potential indexing issue test_y &lt;- y[-ind] # Potential indexing issue # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = detectCores() - 1) # Predicting and calculating AUC predictions &lt;- predict(model, dtest) pred &lt;- ROCR::prediction(predictions, test_y) perf &lt;- ROCR::performance(pred, &quot;auc&quot;) auc[i] &lt;- perf@y.values[[1]] } end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(mean_auc = mean(auc), time_taken = time_taken) } xgbpar &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer cl &lt;- makeCluster(detectCores() - 1) # Use one less than the total number of cores registerDoParallel(cl) # Parallel processing using foreach results &lt;- foreach(i = 1:r, .combine = &#39;c&#39;, .packages = c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] # Potential indexing issue test_y &lt;- y[-ind] # Potential indexing issue # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = 1) predictions &lt;- predict(model, dtest) pred &lt;- prediction(predictions, test_y) perf &lt;- performance(pred, &quot;auc&quot;) perf@y.values[[1]] } # Stop the cluster stopCluster(cl) end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(mean_auc = mean(results), time_taken = time_taken) } 6.2 10,000 rows Create the data. as a note this data has no relations so it wont be predictable 20 factor collums 10 numeric set.seed(123) df &lt;- data.frame(y = sample(c(0, 1), 10000, replace = TRUE)) for(i in 1:10) { df &lt;- df %&gt;% mutate(!!paste0(&quot;num&quot;, i) := runif(10000)) } for(i in 1:20) { df &lt;- df %&gt;% mutate(!!paste0(&quot;fac&quot;, i) := factor(sample(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), 10000, replace = TRUE))) } 6.2.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = df) y &lt;- df$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1sf &lt;- x1$time_taken t2sf &lt;- x2$time_taken tpsf &lt;- xp$time_taken 6.2.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- df$y xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1ssf &lt;- x1$time_taken t2ssf &lt;- x2$time_taken tpssf &lt;- xp$time_taken 6.3 100,000 rows set.seed(123) df &lt;- data.frame(y = sample(c(0, 1), 100000, replace = TRUE)) for(i in 1:10) { df &lt;- df %&gt;% mutate(!!paste0(&quot;num&quot;, i) := runif(100000)) } for(i in 1:20) { df &lt;- df %&gt;% mutate(!!paste0(&quot;fac&quot;, i) := factor(sample(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), 100000, replace = TRUE))) } 6.3.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = df) y &lt;- df$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1100 &lt;- x1$time_taken t2100 &lt;- x2$time_taken tp100 &lt;- xp$time_taken 6.3.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- df$y xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1s100 &lt;- x1$time_taken t2s100 &lt;- x2$time_taken tps100 &lt;- xp$time_taken 6.4 500,000 rows set.seed(123) df &lt;- data.frame(y = sample(c(0, 1), 500000, replace = TRUE)) for(i in 1:10) { df &lt;- df %&gt;% mutate(!!paste0(&quot;num&quot;, i) := runif(500000)) } for(i in 1:20) { df &lt;- df %&gt;% mutate(!!paste0(&quot;fac&quot;, i) := factor(sample(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), 500000, replace = TRUE))) } 6.4.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = df) y &lt;- df$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t150 &lt;- x1$time_taken t250 &lt;- x2$time_taken tp50 &lt;- xp$time_taken 6.4.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- df$y xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1s50 &lt;- x1$time_taken t2s50 &lt;- x2$time_taken tps50 &lt;- xp$time_taken 6.5 Presentations Print them out nrounds is 100 for all 10ks matrix t1sf ## [1] 30.56726 t2sf ## [1] 12.30036 tpsf ## [1] 7.061776 sparse t1ssf ## [1] 22.12066 t2ssf ## [1] 11.88173 tpssf ## [1] 5.574149 100ks matrix t1100 ## [1] 291.3755 t2100 ## [1] 50.7654 tp100 ## [1] 44.22783 sparse t1s100 ## [1] 207.894 t2s100 ## [1] 45.30879 tps100 ## [1] 32.38782 500ks matrix t150 ## [1] 1564.263 t250 ## [1] 228.6939 tp50 ## [1] 466.8494 sparse t1s50 ## [1] 1146.216 t2s50 ## [1] 202.6244 tps50 ## [1] 358.5363 6.6 Analysis 6.6.1 Graphs pres &lt;- data.frame( size = rep(c(&quot;10k&quot;, &quot;100k&quot;, &quot;500k&quot;), each = 6), type = rep(c(&quot;matrix&quot;, &quot;sparse&quot;), each = 3, times = 3), configuration = rep(c(&quot;t1&quot;, &quot;t2&quot;, &quot;tp&quot;), times = 6), time = c(t1sf, t2sf, tpsf, t1ssf, t2ssf, tpssf, t1100, t2100, tp100, t1s100, t2s100, tps100, t150, t250, tp50, t1s50, t2s50, tps50) ) # Plot 1: Sparse times vs Normal matrix times ggplot(pres, aes(x = interaction(size, configuration), y = time, fill = type)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time Comparison: Sparse vs Normal Matrix&quot;, x = &quot;Dataset and Configuration&quot;, y = &quot;Time&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Plot 2: Comparing t1 vs t2 vs tp by each dataset size ggplot(pres, aes(x = interaction(size, type), y = time, fill = configuration)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time Comparison: t1 vs t2 vs tp&quot;, x = &quot;Dataset Size and Type&quot;, y = &quot;Time&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Plot 3: Time per round for each dataset pres &lt;- pres %&gt;% mutate(time_per_round = time / 100) # Since nrounds is 100 for all ggplot(pres, aes(x = interaction(size, type, configuration), y = time_per_round)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + labs(title = &quot;Time Per Round for Each Dataset&quot;, x = &quot;Dataset Size, Type, and Configuration&quot;, y = &quot;Time per Round&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Assuming the same setup as before, add the number of rows for each size pres &lt;- pres %&gt;% mutate(nrows = case_when( size == &quot;10k&quot; ~ 10000, size == &quot;100k&quot; ~ 100000, size == &quot;500k&quot; ~ 500000 )) # Calculate the time per row pres &lt;- pres %&gt;% mutate(time_per_row = time / nrows) # Plot: Time per nrow for each dataset ggplot(pres, aes(x = interaction(size, type, configuration), y = time_per_row, fill = configuration)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time per Row for Each Dataset and Configuration&quot;, x = &quot;Dataset Size, Type, and Configuration&quot;, y = &quot;Time per Row (seconds)&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) 6.6.2 Takaways With factor collums sparse seems to be better. With numeric collums though it would be worse. XGB is better with internal processing when nrows is huge. When nrows is smaller though we should use our own parrallel processing. Also times include how many loops we are doing which is related to numcores of the machine being used if you look at our codes 6.6.3 Notes for myself Test light gbm with these. Test adaboost with these. Show that only numerical data kills a sparse matrix. What about doparrallel with numcores set inside the xgboost funtions. What about purrr or other ways of parrallel processing. Do correlations in the data(the data actually being predictable have a effect). How much does scaling help with speed? What is faster 1 numeric collum or 1 factor collum? The graphs look ugly clean them up. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
