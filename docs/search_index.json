[["index.html", "Machine Learning Applications Chapter 1 About Myself 1.1 Purpose 1.2 Notes for public 1.3 Work in progress", " Machine Learning Applications Simon P. Raymond 2024-12-16 Chapter 1 About Myself My name is Simon Raymond. I am finishing my Fourth year at Saint Mary’s University in Halifax NS. I intend to keep learning Machine Learning techniques outside of a classroom. This is where i will be uploading some of my generic or basic Algorithms and Practices. Contact: simon.raymond@smu.ca 1.1 Purpose I want to display a few things Show some basic skeletons of how to build machine learning algorithms Make some more practical applications Explain key concepts Keep track of my learning goals 1.2 Notes for public The main purpose of this bookdown is not for the public. However all codes are made availible. Due to some issues while knitting this bookdown much of the code was converted to base R code. N/A 1.3 Work in progress Parts/sections Equations Skeletons, Binary Application. Regression Application, Imbalanced data, Speed Testing, Multi Variable Classification Our own models package free, Parrallel proccesing approaches For myself "],["algorithms.html", "Chapter 2 Algorithms 2.1 Data 2.2 Parametric 2.3 Trees 2.4 Boosting 2.5 Other Non-parametric", " Chapter 2 Algorithms These all will be done in classification but can easily be changed and you can look at the regressional examples 2.1 Data suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(ROCR)) suppressPackageStartupMessages(library(rpart)) suppressPackageStartupMessages(library(randomForest)) suppressPackageStartupMessages(library(xgboost)) suppressPackageStartupMessages(library(doParallel)) suppressPackageStartupMessages(library(gbm)) suppressPackageStartupMessages(library(ada)) suppressPackageStartupMessages(library(caret)) suppressPackageStartupMessages(library(nnet)) suppressPackageStartupMessages(library(lightgbm)) library(dslabs) data(&quot;mnist_27&quot;) data &lt;- rbind(mnist_27$train, mnist_27$test) dataf &lt;- as.data.frame(lapply(data, function(x) if(is.numeric(x)) scale(x) else x)) data &lt;- dataf data$y &lt;- ifelse(data$y == &quot;7&quot;, 1, 0) glimpse(data) ## Rows: 1,000 ## Columns: 3 ## $ y &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, … ## $ x_1 &lt;dbl&gt; -1.6192183, -0.2321362, -1.8274064, -0.5171458, 2.3938524, -1.5154… ## $ x_2 &lt;dbl&gt; -1.19265064, -2.30968844, -0.10549778, -0.74534398, 0.94485419, -0… glimpse(dataf) ## Rows: 1,000 ## Columns: 3 ## $ y &lt;fct&gt; 2, 7, 2, 2, 7, 2, 7, 7, 7, 2, 2, 7, 2, 2, 7, 2, 2, 2, 2, 2, 7, 2, … ## $ x_1 &lt;dbl&gt; -1.6192183, -0.2321362, -1.8274064, -0.5171458, 2.3938524, -1.5154… ## $ x_2 &lt;dbl&gt; -1.19265064, -2.30968844, -0.10549778, -0.74534398, 0.94485419, -0… 2.2 Parametric 2.2.1 LM auc_lm &lt;- c() n &lt;- 100 for (i in 1:n){ idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] mdl &lt;- lm(y ~ ., data = trn) phat &lt;- predict(mdl, tst) pred &lt;- prediction(phat, tst$y) auc_lm[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_lm # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3 Trees 2.3.1 CART auc_cart &lt;- c() n &lt;- 100 for (i in 1:n) { idx &lt;- unique(sample(nrow(dataf), size = nrow(dataf), replace = TRUE)) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] # Fit a CART model mdl &lt;- rpart(y ~ ., data = trn, method = &quot;class&quot;) # Predict probabilities. Adjust if your &#39;y&#39; variable is factor with levels other than 0 and 1 phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, tst$y) auc_cart[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_cart # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with CART&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3.2 Bagging auc_bag &lt;- c() n &lt;- 100 B &lt;- 100 num_vars &lt;- ncol(dataf) - 1 for (i in 1:n) { idx &lt;- sample(nrow(dataf), nrow(dataf), replace = TRUE) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] mdl &lt;- randomForest(y ~ ., data = trn, ntree = B, mtry = num_vars) phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, as.numeric(as.character(tst$y))) auc_bag[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_bag # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with Bagging&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3.3 RF auc_rf &lt;- c() n &lt;- 100 B &lt;- 100 for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(dataf), size = nrow(dataf), replace = TRUE)) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] # Fit a Random Forest model mdl &lt;- randomForest(y ~ ., data = trn, ntree = B) # Predict probabilities for the positive class phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, as.numeric(as.character(tst$y))) auc_rf[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_rf # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with RF&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4 Boosting 2.4.1 Adaboost This works now just make it like the others We also can just take gbm and switch it to adaboost ada_model &lt;- ada(y ~ ., data = dataf, iter = 100, nu = 0.1, control = rpart.control(maxdepth = 3)) summary(ada_model) ## Call: ## ada(y ~ ., data = dataf, iter = 100, nu = 0.1, control = rpart.control(maxdepth = 3)) ## ## Loss: exponential Method: discrete Iteration: 100 ## ## Training Results ## ## Accuracy: 0.857 Kappa: 0.713 2.4.2 GBM boost grid &lt;- expand.grid( n.trees = seq(100, 200, by = 100), # Number of trees interaction.depth = seq(1, 2, by = 1), # Max depth of trees shrinkage = seq(0.1, 0.2, by = 0.1) # Learning rate ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] n &lt;- 50 v &lt;- 3 results &lt;- matrix(nrow = n, ncol = 4) for (i in 1:n) { # Bootstrap sampling for training and test sets idx &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) md &lt;- data[idx, ] test_data &lt;- data[-idx, ] auc_vg &lt;- c() for (j in 1:nrow(rgrid)) { auc_v &lt;- c() for (k in 1:v) { v_idx &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE)) train_data &lt;- md[v_idx, ] val_data &lt;- md[-v_idx, ] # Fit GBM model mdl &lt;- gbm(y ~ ., data = train_data, distribution = &quot;bernoulli&quot;, n.trees = rgrid[j, &quot;n.trees&quot;], interaction.depth = rgrid[j, &quot;interaction.depth&quot;], shrinkage = rgrid[j, &quot;shrinkage&quot;], verbose = FALSE) # Predict on validation set and calculate AUC p &lt;- predict(mdl, newdata = val_data, n.trees = rgrid[j, &quot;n.trees&quot;], type = &quot;response&quot;) pred &lt;- prediction(p, val_data$y) auc_v[k] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc_vg[j] &lt;- mean(auc_v) } # Identify the best model best_idx &lt;- which.max(auc_vg) best_prm &lt;- rgrid[best_idx, ] # Train final model on the full training data and predict on test set mdl_final &lt;- gbm(y ~ ., data = md, distribution = &quot;bernoulli&quot;, n.trees = best_prm[1, &quot;n.trees&quot;], interaction.depth = best_prm[&quot;interaction.depth&quot;], shrinkage = best_prm[&quot;shrinkage&quot;], verbose = FALSE) p_t &lt;- predict(mdl_final, newdata = test_data, n.trees = best_prm[1, &quot;n.trees&quot;], type = &quot;response&quot;) pred_t &lt;- prediction(p_t, test_data$y) auc_test &lt;- performance(pred_t, &quot;auc&quot;)@y.values[[1]] results[i, 1] &lt;- auc_test results[i, 2] &lt;- best_prm[1, &quot;n.trees&quot;] results[i, 3] &lt;- best_prm[1, &quot;interaction.depth&quot;] results[i, 4] &lt;- best_prm[1, &quot;shrinkage&quot;] } df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, &quot;n.trees&quot;, &quot;interaction.depth&quot;, &quot;shrinkage&quot;) # Plotting plot(df_results$AUC_Test, col = &quot;red&quot;, main = &quot;AUC Test Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(df_results$AUC_Test), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(df_results$AUC_Test) - 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(df_results$AUC_Test) + 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.3 XGBoost grid &lt;- expand.grid( eta = seq(0.1, 0.2, by = 0.05), max_depth = seq(1, 2, by = 1), min_child_weight = seq(1, 1, by = 0), subsample = seq(1, 1, by = 0), colsample_bytree = seq(1, 1, by = 0), lambda = seq(0, 1, by = 1), alpha = seq(0, 1, by = 1), gamma = seq(0, 1, by = 1), nrounds = seq(100, 200, by = 100) ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] xs &lt;- model.matrix(~ . - 1 - y, data = data) y &lt;- data$y nc &lt;- 1 #detectCores - 1 n &lt;- 50 v &lt;- 3 # should be much higher # Adjust the matrix size according to the number of hyperparameters + 1 for AUC results &lt;- matrix(nrow = n, ncol = length(rgrid[1,]) + 1) for (i in 1:n) { idx &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) dx &lt;- xs[idx, ] dy &lt;- y[idx] tx &lt;- xs[-idx, ] ty &lt;- y[-idx] auc_vg &lt;- c() for (j in 1:nrow(rgrid)) { auc_v &lt;- c() for (k in 1:v) { v_idx &lt;- sample(nrow(dx), nrow(dx), replace = TRUE) vx &lt;- dx[v_idx, ] vy &lt;- dy[v_idx] val_x &lt;- dx[-v_idx, ] val_y &lt;- dy[-v_idx] prm &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = rgrid[j, &quot;max_depth&quot;], eta = rgrid[j, &quot;eta&quot;], subsample = rgrid[j, &quot;subsample&quot;], colsample_bytree = rgrid[j, &quot;colsample_bytree&quot;], gamma = rgrid[j, &quot;gamma&quot;], min_child_weight = rgrid[j, &quot;min_child_weight&quot;], alpha = rgrid[j, &quot;alpha&quot;], lambda = rgrid[j, &quot;lambda&quot;], nthread = nc ) dm_train &lt;- xgb.DMatrix(data = vx, label = vy) mdl &lt;- xgb.train(params = prm, data = dm_train, nrounds = rgrid[j, &quot;nrounds&quot;], verbose = FALSE) p &lt;- predict(mdl, xgb.DMatrix(data = val_x)) pred &lt;- prediction(p, val_y) auc_v &lt;- c(auc_v, performance(pred, &quot;auc&quot;)@y.values[[1]]) } auc_vg &lt;- c(auc_vg, mean(auc_v)) } best_idx &lt;- which.max(auc_vg) best_prm &lt;- rgrid[best_idx, ] best_prm_list &lt;- as.list(best_prm[-which(names(best_prm) == &quot;nrounds&quot;)]) best_prm_list$booster &lt;- &quot;gbtree&quot; best_prm_list$objective &lt;- &quot;binary:logistic&quot; best_prm_list$nthread &lt;- nc dm_final &lt;- xgb.DMatrix(data = dx, label = dy) dt_final &lt;- xgb.DMatrix(data = tx, label = ty) mdl_final &lt;- xgb.train(params = best_prm_list, data = dm_final, nrounds = best_prm[ ,&quot;nrounds&quot;], verbose = FALSE) p_t &lt;- predict(mdl_final, dt_final) pred_t &lt;- prediction(p_t, ty) auc_test &lt;- performance(pred_t, &quot;auc&quot;)@y.values[[1]] # Store AUC and hyperparameters in the results matrix results[i, 1] &lt;- auc_test results[i, 2:ncol(results)] &lt;- as.numeric(best_prm) } # Convert results to a dataframe for easy handling df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, names(rgrid[1,])) # Example of plotting, adjust as necessary plot(df_results$AUC_Test, col = &quot;red&quot;, main = &quot;AUC Test Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(df_results$AUC_Test), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(df_results$AUC_Test) - 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(df_results$AUC_Test) + 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.4 Light GBM boost grid &lt;- expand.grid( learning_rate = seq(0.05, 0.15, by = 0.1), num_leaves = seq(2, 2, by = 0), min_data_in_leaf = seq(1, 1, by = 0), feature_fraction = seq(1, 1, by = 0), bagging_fraction = seq(1, 1, by = 0), bagging_freq = seq(0, 0, by = 0), lambda_l1 = seq(0, 1, by = 1), lambda_l2 = seq(0, 1, by = 1), max_depth = seq(1, 2, by = 1), # -1 for no limit nrounds = seq(100, 200, by = 100) ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] # Create model matrix for xgboost xs &lt;- model.matrix(~ . -1 -y, data = data) y &lt;- data$y n &lt;- 50 v &lt;- 3 results &lt;- matrix(nrow = n, ncol = length(rgrid[1,]) + 1) for (i in 1:n) { # Bootstrap sampling ind &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] test_y &lt;- y[-ind] auc &lt;- numeric(nrow(rgrid)) for (j in 1:nrow(rgrid)) { auc_100 &lt;- numeric(v) for (k in 1:v) { # Nested validation loop ind2 &lt;- unique(sample(nrow(md_x), nrow(md_x), replace = TRUE)) train_x &lt;- md_x[ind2, ] train_y &lt;- md_y[ind2] val_x &lt;- md_x[-ind2, ] val_y &lt;- md_y[-ind2] # Model training lgb_train &lt;- lgb.Dataset(train_x, label = train_y) lgb_val &lt;- lgb.Dataset(val_x, label = val_y, free_raw_data = FALSE) params &lt;- list( objective = &quot;binary&quot;, metric = &quot;auc&quot;, learning_rate = rgrid[j, &quot;learning_rate&quot;], num_leaves = rgrid[j, &quot;num_leaves&quot;], feature_fraction = rgrid[j, &quot;feature_fraction&quot;], bagging_fraction = rgrid[j, &quot;bagging_fraction&quot;], bagging_freq = rgrid[j, &quot;bagging_freq&quot;], lambda_l1 = rgrid[j, &quot;lambda_l1&quot;], lambda_l2 = rgrid[j, &quot;lambda_l2&quot;], min_data_in_leaf = rgrid[j, &quot;min_data_in_leaf&quot;], max_depth = rgrid[j, &quot;max_depth&quot;] ) lgmb_model &lt;- lgb.train(params, lgb_train, valids = list(val = lgb_val), nrounds = rgrid[j, &quot;nrounds&quot;], verbose = 0) # AUC calculation phat &lt;- predict(lgmb_model, val_x) pred_rocr &lt;- prediction(phat, val_y) auc_100[k] &lt;- performance(pred_rocr, &quot;auc&quot;)@y.values[[1]] } auc[j] &lt;- mean(auc_100) } # Get the best hyperparameters BI &lt;- which.max(auc) params_best &lt;- rgrid[BI, ] # Retrain and test params_final &lt;- list( objective = &quot;binary&quot;, metric = &quot;auc&quot;, learning_rate = params_best[&quot;learning_rate&quot;], num_leaves = params_best[&quot;num_leaves&quot;], feature_fraction = params_best[&quot;feature_fraction&quot;], bagging_fraction = params_best[&quot;bagging_fraction&quot;], bagging_freq = params_best[&quot;bagging_freq&quot;], lambda_l1 = params_best[&quot;lambda_l1&quot;], lambda_l2 = params_best[&quot;lambda_l2&quot;], min_data_in_leaf = params_best[&quot;min_data_in_leaf&quot;], max_depth = params_best[&quot;max_depth&quot;] ) lgb_dm &lt;- lgb.Dataset(md_x, label = md_y) lgmb_model_final &lt;- lgb.train(params_final, lgb_dm, nrounds = params_best[1, &quot;nrounds&quot;], verbose = 0) phat_t &lt;- predict(lgmb_model_final, test_x) pred_rocr_t &lt;- prediction(phat_t, test_y) auc_t &lt;- performance(pred_rocr_t, &quot;auc&quot;)@y.values[[1]] # Store AUC and hyperparameters in the results matrix results[i, 1] &lt;- auc_t results[i, 2] &lt;- params_best[1, 1] results[i, 3] &lt;- params_best[1, 2] results[i, 4] &lt;- params_best[1, 3] results[i, 5] &lt;- params_best[1, 4] results[i, 6] &lt;- params_best[1, 5] results[i, 7] &lt;- params_best[1, 6] results[i, 8] &lt;- params_best[1, 7] results[i, 9] &lt;- params_best[1, 8] results[i, 10] &lt;- params_best[1, 9] results[i, 11] &lt;- params_best[1, 10] } # Convert results to a dataframe for easy handling df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, names(rgrid[1,])) auc &lt;- df_results$AUC_Test # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with lgbm&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.5 Catboost Somthing to look at in the future 2.5 Other Non-parametric 2.5.1 KNN k &lt;- seq(5, 50, 5) tauc &lt;- c() tuned_k &lt;- c() for (t in 1:50) { ind &lt;- sample(nrow(data), nrow(data)*0.8) mdata &lt;- data[ind, ] test &lt;- data[-ind, ] mauc &lt;- c() for(i in 1:length(k)) { auc &lt;- c() for(j in 1:2) { ind2 &lt;- sample(nrow(mdata), nrow(mdata), replace = TRUE) train &lt;- mdata[ind2, ] val &lt;- mdata[-ind2, ] model &lt;- knn3(y ~ ., data = train, k = k[i]) phat &lt;- predict(model, val, type = &quot;prob&quot;)[,2] pred_rocr &lt;- prediction(phat, val$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[j] &lt;- auc_ROCR@y.values[[1]] } mauc[i] &lt;- mean(auc) } tuned_k &lt;- k[which.max(mauc)] model &lt;- knn3(y ~ ., data = mdata, k = tuned_k) phat &lt;- predict(model, test, type = &quot;prob&quot;)[,2] pred_rocr &lt;- prediction(phat, test$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) tauc[t] &lt;- auc_ROCR@y.values[[1]] } auc &lt;- tauc # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with KNN3&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.5.2 Nueralnet size &lt;- c(seq(2, 3, 1)) decay &lt;- c(seq(0.01, 0.25, 0.05)) maxit &lt;- c(seq(100, 900, 400)) grid &lt;- expand.grid(size, decay, maxit) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] n &lt;- 100 v &lt;- 5 # store out best values via validation scores opt &lt;- matrix(0, nrow = n, ncol = 5) colnames(opt) &lt;- c(&quot;size&quot;, &quot;decay&quot;, &quot;maxit&quot;, &quot;AUC_val&quot;, &quot;AUC_TEST&quot;) for (j in 1:n){ # put aside data for final test. creat md and test ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) md &lt;- data[ind, ] test &lt;- data[-ind, ] auc_runs &lt;- c() for (i in 1:nrow(rgrid)){ #cat(&quot;loops: &quot;, j, i, &quot;\\r&quot;) auc_tuning &lt;- c() for (p in 1:v){ # bootstrap from md to make a train and val set idx &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE)) train &lt;- md[idx,] val &lt;- md[-idx, ] # model on the train data model &lt;- nnet(y ~ ., data = train, trace = FALSE, act.fct = &quot;logistic&quot;, size = rgrid[i, 1], decay = rgrid[i, 2], maxit = rgrid[i, 3] ) # predict on the val data phat &lt;- predict(model, val) # find the auc pred_rocr &lt;- prediction(phat, val$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_tuning[p] &lt;- auc_ROCR@y.values[[1]] } auc_runs[i] &lt;- mean(auc_tuning) #take the mean of v runs for that one specific hyper parameter } # index the best hyper parameters BI &lt;- which.max(auc_runs) best_AUC &lt;- auc_runs[BI] best_params &lt;- rgrid[BI, ] # store the best hyper parames based on the mean aucs opt[j, 1] &lt;- best_params[1, 1] opt[j, 2] &lt;- best_params[1, 2] opt[j, 3] &lt;- best_params[1, 3] opt[j, 4] &lt;- best_AUC # model with the md data model &lt;- nnet(y ~ ., data = md, trace = FALSE, act.fct = &quot;logistic&quot;, size = opt[j, 1], decay = opt[j, 2], maxit = opt[j, 3] ) # predict the set aside test set phat_t &lt;- predict(model, test) # get the test auc pred_rocr &lt;- prediction(phat_t, test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_test &lt;- auc_ROCR@y.values[[1]] # store the test auc opt[j, 5] &lt;- auc_test } auc &lt;- opt[,5] # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with nnet&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.5.3 SVM "],["binary-application.html", "Chapter 3 Binary Application 3.1 Data 3.2 Model Selection 3.3 Comparisions 3.4 Statistic selection", " Chapter 3 Binary Application I will compare results in a different data set that predicts if income is over 50k (1) or not (0) suppressMessages(library(readr)) suppressMessages(library(caret)) suppressMessages(library(ROCR)) suppressMessages(library(xgboost)) suppressMessages(library(foreach)) suppressMessages(library(doParallel)) suppressMessages(library(Matrix)) suppressMessages(library(dplyr)) suppressMessages(library(tidyverse)) suppressMessages(library(forcats)) suppressMessages(library(DataExplorer)) suppressMessages(library(randomForest)) First, we import all necessary libraries for data manipulation, visualization, and machine learning. 3.1 Data 3.1.1 Data Prep data &lt;- read_csv(&quot;adult_train.csv&quot;, col_names = FALSE) ## Rows: 32561 Columns: 15 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (9): X2, X4, X6, X7, X8, X9, X10, X14, X15 ## dbl (6): X1, X3, X5, X11, X12, X13 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. colnames(data) &lt;- c(&quot;age&quot;, &quot;workclass&quot;, &quot;fnlwgt&quot;, &quot;education&quot;, &quot;education_num&quot;, &quot;marital_status&quot;, &quot;occupation&quot;, &quot;relationship&quot;, &quot;race&quot;, &quot;sex&quot;, &quot;capital_gain&quot;, &quot;capital_loss&quot;, &quot;hours_per_week&quot;, &quot;native_country&quot;, &quot;income&quot;) glimpse(data) ## Rows: 32,561 ## Columns: 15 ## $ age &lt;dbl&gt; 39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32,… ## $ workclass &lt;chr&gt; &quot;State-gov&quot;, &quot;Self-emp-not-inc&quot;, &quot;Private&quot;, &quot;Private&quot;, … ## $ fnlwgt &lt;dbl&gt; 77516, 83311, 215646, 234721, 338409, 284582, 160187, 2… ## $ education &lt;chr&gt; &quot;Bachelors&quot;, &quot;Bachelors&quot;, &quot;HS-grad&quot;, &quot;11th&quot;, &quot;Bachelors… ## $ education_num &lt;dbl&gt; 13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11,… ## $ marital_status &lt;chr&gt; &quot;Never-married&quot;, &quot;Married-civ-spouse&quot;, &quot;Divorced&quot;, &quot;Mar… ## $ occupation &lt;chr&gt; &quot;Adm-clerical&quot;, &quot;Exec-managerial&quot;, &quot;Handlers-cleaners&quot;,… ## $ relationship &lt;chr&gt; &quot;Not-in-family&quot;, &quot;Husband&quot;, &quot;Not-in-family&quot;, &quot;Husband&quot;,… ## $ race &lt;chr&gt; &quot;White&quot;, &quot;White&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;White&quot;, &quot;… ## $ sex &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Fe… ## $ capital_gain &lt;dbl&gt; 2174, 0, 0, 0, 0, 0, 0, 0, 14084, 5178, 0, 0, 0, 0, 0, … ## $ capital_loss &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hours_per_week &lt;dbl&gt; 40, 13, 40, 40, 40, 40, 16, 45, 50, 40, 80, 40, 30, 50,… ## $ native_country &lt;chr&gt; &quot;United-States&quot;, &quot;United-States&quot;, &quot;United-States&quot;, &quot;Uni… ## $ income &lt;chr&gt; &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;… table(data$income) ## ## &lt;=50K &gt;50K ## 24720 7841 Identify columns containing “?” and count their occurrence. Then, transform character columns to factors and adjust the ‘income’ column for binary classification. question_mark_counts_dplyr &lt;- data %&gt;% summarise(across(everything(), ~sum(. == &quot;?&quot;, na.rm = TRUE))) %&gt;% select(where(~. &gt; 0)) question_mark_counts_dplyr ## # A tibble: 1 × 3 ## workclass occupation native_country ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1836 1843 583 data_transformed &lt;- data %&gt;% mutate(across(where(is.character), as.factor)) %&gt;% mutate(income_binary = if_else(income == &quot;&gt;50K&quot;, 1, 0)) %&gt;% select(-income) %&gt;% select(income_binary, everything()) df &lt;- rename(data_transformed, y = income_binary) df$native_country &lt;- fct_lump(df$native_country, n = 3, other_level = &quot;other&quot;) sapply(df, function(col) { if(is.factor(col)) { sorted_tab &lt;- sort(table(col)) return(sorted_tab) } }) ## $y ## NULL ## ## $age ## NULL ## ## $workclass ## col ## Never-worked Without-pay Federal-gov Self-emp-inc ## 7 14 960 1116 ## State-gov ? Local-gov Self-emp-not-inc ## 1298 1836 2093 2541 ## Private ## 22696 ## ## $fnlwgt ## NULL ## ## $education ## col ## Preschool 1st-4th 5th-6th Doctorate 12th 9th ## 51 168 333 413 433 514 ## Prof-school 7th-8th 10th Assoc-acdm 11th Assoc-voc ## 576 646 933 1067 1175 1382 ## Masters Bachelors Some-college HS-grad ## 1723 5355 7291 10501 ## ## $education_num ## NULL ## ## $marital_status ## col ## Married-AF-spouse Married-spouse-absent Widowed ## 23 418 993 ## Separated Divorced Never-married ## 1025 4443 10683 ## Married-civ-spouse ## 14976 ## ## $occupation ## col ## Armed-Forces Priv-house-serv Protective-serv Tech-support ## 9 149 649 928 ## Farming-fishing Handlers-cleaners Transport-moving ? ## 994 1370 1597 1843 ## Machine-op-inspct Other-service Sales Adm-clerical ## 2002 3295 3650 3770 ## Exec-managerial Craft-repair Prof-specialty ## 4066 4099 4140 ## ## $relationship ## col ## Other-relative Wife Unmarried Own-child Not-in-family ## 981 1568 3446 5068 8305 ## Husband ## 13193 ## ## $race ## col ## Other Amer-Indian-Eskimo Asian-Pac-Islander Black ## 271 311 1039 3124 ## White ## 27816 ## ## $sex ## col ## Female Male ## 10771 21790 ## ## $capital_gain ## NULL ## ## $capital_loss ## NULL ## ## $hours_per_week ## NULL ## ## $native_country ## col ## ? Mexico other United-States ## 583 643 2165 29170 Combine some levels df$education &lt;- as.factor(ifelse(df$education %in% c(&quot;Preschool&quot;, &quot;1st-4th&quot;), &quot;Early Childhood&quot;, as.character(df$education))) df$workclass &lt;- as.factor(ifelse(df$workclass %in% c(&quot;Never-worked&quot;, &quot;Without-pay&quot;), &quot;Unemployed&quot;, as.character(df$workclass))) df$marital_status &lt;- as.factor(ifelse(df$marital_status == &quot;Married-AF-spouse&quot;, &quot;Married-spouse-absent&quot;, as.character(df$marital_status))) df$occupation &lt;- as.factor(ifelse(df$occupation == &quot;Armed-Forces&quot;, &quot;Protective-serv&quot;, as.character(df$occupation))) glimpse(df) ## Rows: 32,561 ## Columns: 15 ## $ y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0… ## $ age &lt;dbl&gt; 39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32,… ## $ workclass &lt;fct&gt; State-gov, Self-emp-not-inc, Private, Private, Private,… ## $ fnlwgt &lt;dbl&gt; 77516, 83311, 215646, 234721, 338409, 284582, 160187, 2… ## $ education &lt;fct&gt; Bachelors, Bachelors, HS-grad, 11th, Bachelors, Masters… ## $ education_num &lt;dbl&gt; 13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11,… ## $ marital_status &lt;fct&gt; Never-married, Married-civ-spouse, Divorced, Married-ci… ## $ occupation &lt;fct&gt; Adm-clerical, Exec-managerial, Handlers-cleaners, Handl… ## $ relationship &lt;fct&gt; Not-in-family, Husband, Not-in-family, Husband, Wife, W… ## $ race &lt;fct&gt; White, White, White, Black, Black, White, Black, White,… ## $ sex &lt;fct&gt; Male, Male, Male, Male, Female, Female, Female, Male, F… ## $ capital_gain &lt;dbl&gt; 2174, 0, 0, 0, 0, 0, 0, 0, 14084, 5178, 0, 0, 0, 0, 0, … ## $ capital_loss &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hours_per_week &lt;dbl&gt; 40, 13, 40, 40, 40, 40, 16, 45, 50, 40, 80, 40, 30, 50,… ## $ native_country &lt;fct&gt; United-States, United-States, United-States, United-Sta… df_numeric &lt;- df %&gt;% select(where(is.numeric)) df_factor &lt;- df %&gt;% select(where(is.factor)) plot_correlation(df_numeric) plot_correlation(cbind(df_factor, df$y)) numeric_columns &lt;- sapply(df, is.numeric) &amp; names(df) != &quot;y&quot; dfs &lt;- df dfs[numeric_columns] &lt;- scale(df[numeric_columns]) head(dfs) ## # A tibble: 6 × 15 ## y age workclass fnlwgt education education_num marital_status ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 0.0307 State-gov -1.06 Bachelors 1.13 Never-married ## 2 0 0.837 Self-emp-not-inc -1.01 Bachelors 1.13 Married-civ-spo… ## 3 0 -0.0426 Private 0.245 HS-grad -0.420 Divorced ## 4 0 1.06 Private 0.426 11th -1.20 Married-civ-spo… ## 5 0 -0.776 Private 1.41 Bachelors 1.13 Married-civ-spo… ## 6 0 -0.116 Private 0.898 Masters 1.52 Married-civ-spo… ## # ℹ 8 more variables: occupation &lt;fct&gt;, relationship &lt;fct&gt;, race &lt;fct&gt;, ## # sex &lt;fct&gt;, capital_gain &lt;dbl&gt;, capital_loss &lt;dbl&gt;, hours_per_week &lt;dbl&gt;, ## # native_country &lt;fct&gt; table(dfs$y) ## ## 0 1 ## 24720 7841 X &lt;- model.matrix(~ . -1 - y, data = dfs) y &lt;- dfs$y xs &lt;- Matrix(X, sparse = TRUE) 3.2 Model Selection 3.2.1 Functions run_xgb &lt;- function(xs, y, grd, v) { # Setup parallel computing nc &lt;- detectCores() - 1 cl &lt;- makeCluster(nc) registerDoParallel(cl) results &lt;- foreach(g = 1:nrow(grd), .combine=&#39;rbind&#39;, .packages=c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { auc &lt;- numeric(v) # AUC scores for validations for (k in 1:v) { # Bootstrap sampling for validation idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) tr_x &lt;- xs[idx, ] tr_y &lt;- y[idx] vl_x &lt;- xs[-idx, ] vl_y &lt;- y[-idx] # Model training for validation prms &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = grd[g, &quot;max_depth&quot;], eta = grd[g, &quot;eta&quot;], subsample = grd[g, &quot;subsample&quot;], colsample_bytree = grd[g, &quot;colsample_bytree&quot;], gamma = grd[g, &quot;gamma&quot;], min_child_weight = grd[g, &quot;min_child_weight&quot;], alpha = grd[g, &quot;alpha&quot;], lambda = grd[g, &quot;lambda&quot;] ) xgb_tr &lt;- xgb.DMatrix(data = tr_x, label = tr_y) xm &lt;- xgb.train(params = prms, data = xgb_tr, nrounds = grd[g, &quot;nrounds&quot;], verbose = FALSE, nthread = 1) # AUC calculation for validation phat &lt;- predict(xm, xgb.DMatrix(data = vl_x)) pred &lt;- prediction(phat, vl_y) auc[k] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } # AUC mean and params auc_mean &lt;- mean(auc) c(grd[g, ], auc_mean) } # Stop the cluster stopCluster(cl) # Convert results to a tibble and set column names res &lt;- as_tibble(results) names(res) &lt;- c(names(grd), &quot;AUC_Mean&quot;) res &lt;- res %&gt;% mutate(across(everything(), ~unlist(.))) return(res) } test_top_hp &lt;- function(hp, xs, y, t) { # Setup parallel computing nc &lt;- detectCores() - 1 cl &lt;- makeCluster(nc) registerDoParallel(cl) # Testing loop ts_res &lt;- foreach(h = 1:nrow(hp), .combine = &#39;rbind&#39;, .packages = c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { aucs &lt;- numeric(t) for (i in 1:t) { # Bootstrap sampling for testing idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) mdx &lt;- xs[idx, ] mdy &lt;- y[idx] tx &lt;- xs[-idx, ] ty &lt;- y[-idx] # Parameters for the model prms &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = hp[h, &quot;max_depth&quot;], eta = hp[h, &quot;eta&quot;], subsample = hp[h, &quot;subsample&quot;], colsample_bytree = hp[h, &quot;colsample_bytree&quot;], gamma = hp[h, &quot;gamma&quot;], min_child_weight = hp[h, &quot;min_child_weight&quot;], alpha = hp[h, &quot;alpha&quot;], lambda = hp[h, &quot;lambda&quot;], nrounds = hp[h, &quot;nrounds&quot;] ) # Train and test the model dtr &lt;- xgb.DMatrix(data = mdx, label = mdy) dte &lt;- xgb.DMatrix(data = tx, label = ty) mdl &lt;- xgb.train(params = prms, data = dtr, nrounds = prms$nrounds, verbose = 0, nthread = 1) # AUC calculation ph &lt;- predict(mdl, dte) pr &lt;- prediction(ph, ty) aucs[i] &lt;- performance(pr, &quot;auc&quot;)@y.values[[1]] } # Average AUC and combine with hyperparameters c(hp[h, ], mean_auc = mean(aucs)) } # Stop the cluster stopCluster(cl) # Convert to tibble and unlist columns ts_tbl &lt;- as_tibble(ts_res) %&gt;% mutate(across(everything(), ~unlist(.))) return(ts_tbl) } test_auc &lt;- function(xs, y, best_hp, runs) { aucs &lt;- numeric(runs) for (i in 1:runs) { # Bootstrap sampling for testing idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) md_x &lt;- xs[idx, ] md_y &lt;- y[idx] test_x &lt;- xs[-idx, ] test_y &lt;- y[-idx] # Set parameters for the model params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = best_hp$max_depth, eta = best_hp$eta, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, gamma = best_hp$gamma, min_child_weight = best_hp$min_child_weight, alpha = best_hp$alpha, lambda = best_hp$lambda ) # Train and test the model md &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) model &lt;- xgb.train(params = params, data = md, nrounds = best_hp$nrounds, verbose = 0, nthread = 1) # Calculate AUC phat &lt;- predict(model, dtest) pred &lt;- prediction(phat, test_y) aucs[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } return(aucs) } opt_nrd4eta &lt;- function(xs, y, params, o , grd) { # Assuming xgb.train and related functions are already available (via library(xgboost)) library(xgboost) library(ROCR) # Detect the number of cores numCores &lt;- detectCores() - 1 # Initialize an AUC matrix aucm &lt;- matrix(0, nrow = o, ncol = length(grd)) start_time &lt;- Sys.time() for (j in 1:o){ # Creating a bootstrap sample ind &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) dm &lt;- xgb.DMatrix(data = xs[ind, ], label = y[ind]) dv &lt;- xgb.DMatrix(data = xs[-ind, ], label = y[-ind]) auc &lt;- c() for (i in 1:length(grd)){ # Training the model on the bootstrap sample bsm &lt;- xgb.train(params = params, data = dm, nrounds = grd[i], verbose = FALSE, nthread = numCores ) # Predict on the validation set and calculate AUC phat &lt;- predict(bsm, dv, type = &quot;prob&quot;) # Calculating the AUC pred_rocr &lt;- prediction(phat, y[-ind]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] } aucm[j, ] &lt;- auc } evalauc &lt;- colMeans(aucm) # Plotting plot(grd, evalauc, type = &quot;b&quot;, col = &quot;blue&quot;, xlab = &quot;Number of Rounds&quot;, ylab = &quot;AUC&quot;, main = &quot;AUC over o rounds vs Number of Rounds in XGBoost&quot;) best_nrounds &lt;- grd[which.max(evalauc)] max_auc &lt;- max(evalauc) end_time &lt;- Sys.time() elapsed_time &lt;- end_time - start_time return(list(best_nrounds = best_nrounds, max_auc = max_auc, elapsed_time = elapsed_time)) } 3.2.2 XGB # Define XGBoost parameters params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.05 ) grd &lt;- seq(1, 500, by = 50) o &lt;- 5 rs &lt;- opt_nrd4eta(xs = xs, y = y, params = params, o, grd) This is a very small grid and cuts out alot of tuning power grid &lt;- expand.grid( eta = seq(0.03, 0.05, by = 0.02), max_depth = seq(4, 6, by = 2), min_child_weight = seq(1, 1, by = 0), subsample = seq(1, 1, by = 0), colsample_bytree = seq(1, 1, by = 0), lambda = seq(1, 3, by = 1), alpha = seq(0, 1, by = 1), gamma = seq(0, 1, by = 1), nrounds = seq(350, 450, by = 50) ) conf_lev &lt;- .95 num_max &lt;- 5 n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] v &lt;- 5 #This number should be much higher vr &lt;- run_xgb(xs, y, rgrid, v) print(head(vr), width = Inf) ## # A tibble: 6 × 10 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 4 1 1 1 3 0 0 ## 2 0.05 6 1 1 1 1 0 1 ## 3 0.05 6 1 1 1 2 0 0 ## 4 0.03 4 1 1 1 2 1 0 ## 5 0.03 4 1 1 1 3 1 1 ## 6 0.03 4 1 1 1 1 0 1 ## nrounds AUC_Mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 350 0.926 ## 2 350 0.928 ## 3 400 0.928 ## 4 400 0.926 ## 5 450 0.926 ## 6 350 0.925 We will take the top 10% and do a more thorough search # Sort vr by AUC_Mean and select the top 25% top_hp &lt;- vr %&gt;% arrange(desc(AUC_Mean)) %&gt;% slice_head(prop = 0.05) t &lt;- 25 ts_res &lt;- test_top_hp(top_hp, xs, y, t) print(head(ts_res), width = Inf) ## # A tibble: 4 × 11 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 4 1 1 1 1 0 0 ## 2 0.03 6 1 1 1 3 0 1 ## 3 0.05 6 1 1 1 2 0 0 ## 4 0.05 6 1 1 1 2 0 0 ## nrounds AUC_Mean mean_auc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 450 0.930 0.928 ## 2 450 0.929 0.928 ## 3 450 0.929 0.928 ## 4 350 0.929 0.928 Final AUC pres best_hp &lt;- ts_res %&gt;% dplyr::arrange(desc(mean_auc)) %&gt;% dplyr::slice(1) print(best_hp, width = Inf) ## # A tibble: 1 × 11 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 6 1 1 1 2 0 0 ## nrounds AUC_Mean mean_auc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 350 0.929 0.928 r &lt;- 100 auc &lt;- test_auc(xs, y, best_hp, r) mauc &lt;- mean(auc) mauc ## [1] 0.928195 sd(auc) ## [1] 0.002015594 plot(auc, col=&quot;red&quot;) abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) 3.3 Comparisions Random Forest rfd &lt;- dfs rfd$y &lt;- as.factor(rfd$y) B &lt;- 120 n &lt;- 100 obs &lt;- nrow(data) numCores &lt;- detectCores()-1 cl &lt;- makeCluster(numCores) registerDoParallel(cl) lst &lt;- foreach(i=1:n, .packages = c(&quot;randomForest&quot;, &quot;ROCR&quot;)) %dopar% { tryCatch({ idx &lt;- unique(sample(obs, obs, replace = TRUE)) train &lt;- rfd[idx,] test &lt;- rfd[-idx, ] model &lt;- randomForest(y ~ ., ntree = B, data = train) phat &lt;- predict(model, test, type = &quot;prob&quot;) pred_rocr &lt;- prediction(phat[,2], test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] }, error = function(e) { NA # Return NA on error }) } stopCluster(cl) # combine the results auc &lt;- unlist(lst) auc &lt;- na.omit(auc) # plot auc and mean plot(auc, col=&quot;red&quot;) abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) LPM data_lm &lt;- rfd data_lm$y &lt;- as.numeric(data_lm$y) data_lm$y &lt;- data_lm$y - 1 # bring it back to 1 and 0s n &lt;- 100 obs &lt;- nrow(data_lm) numCores &lt;- detectCores() - 1 cl &lt;- makeCluster(numCores) registerDoParallel(cl) auc_list &lt;- foreach(i = 1:n, .packages = &quot;ROCR&quot;) %dopar% { tryCatch({ idx &lt;- unique(sample(obs, obs, replace = TRUE)) trn &lt;- data_lm[idx, ] # Training data tst &lt;- data_lm[-idx, ] # Test data mdl &lt;- lm(y ~ ., data = trn) phat &lt;- predict(mdl, tst) pred &lt;- prediction(phat, tst$y) performance(pred, &quot;auc&quot;)@y.values[[1]] }, error = function(e) { NA # Return NA on error }) } stopCluster(cl) # Process the AUC results auc_values &lt;- unlist(auc_list) auc_values &lt;- na.omit(auc_values) mean_auc &lt;- mean(auc_values) sd_auc &lt;- sd(auc_values) # Plot AUC values, mean, and confidence intervals plot(auc_values, col = &quot;red&quot;, main = &quot;AUC Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean_auc, col = &quot;blue&quot;, lwd = 2, lty = 2) # Mean line abline(h = mean_auc - 1.96 * sd_auc, col = &quot;green&quot;, lwd = 2, lty = 3) # Lower CI abline(h = mean_auc + 1.96 * sd_auc, col = &quot;green&quot;, lwd = 2, lty = 3) # Upper CI 3.4 Statistic selection Times we run everything here r &lt;- 10 3.4.1 0.5 # Initialize storage for the confusion matrix conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Train the model with predefined parameters xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions using a fixed threshold of 0.5 final_phat &lt;- predict(xgbmdl_final, dtest) final_predicted_classes &lt;- ifelse(final_phat &gt; 0.5, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test_y, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_matrix / r print(avg_conf_matrix) ## Actual ## Predicted 1 0 ## 1 188.1 57.3 ## 0 98.8 846.4 3.4.2 Youden’s J Statistic best_thresholds &lt;- c() j_stats &lt;- c() for(i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Extract parameters for the model from best_hp params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = best_hp$eta, max_depth = best_hp$max_depth, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, min_child_weight = best_hp$min_child_weight, gamma = best_hp$gamma, alpha = best_hp$alpha, lambda = best_hp$lambda ) # Train the model xgbmdl &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions phat &lt;- predict(xgbmdl, dtest) # ROCR predictions object pred &lt;- prediction(phat, test_y) # Calculate performance measures perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;spec&quot;) sensitivity &lt;- slot(perf, &quot;y.values&quot;)[[1]] specificity &lt;- slot(perf, &quot;x.values&quot;)[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] j_stat &lt;- sensitivity + specificity - 1 # Find the best threshold (maximizing J statistic) best_idx &lt;- which.max(j_stat) best_thresholds[i] &lt;- thresholds[best_idx] j_stats[i] &lt;- j_stat[best_idx] } # Calculate and print the average of the best thresholds avg_best_youden_threshold &lt;- mean(best_thresholds) cat(&quot;Average Best Threshold:&quot;, avg_best_youden_threshold, &quot;\\n&quot;) ## Average Best Threshold: 0.2272045 # Plot the distribution of max J statistics for each iteration hist(j_stats, col = &quot;blue&quot;, xlab = &quot;Max J Statistic&quot;, ylab = &quot;Frequency&quot;, main = &quot;Distribution of Max J Statistics Across Bootstrap Iterations&quot;) abline(v = mean(j_stats), col = &quot;red&quot;, lwd = 2) # Mean J Stat line # Initialize storage for aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) # Initialize lists to store TPR and FPR values for each bootstrap iteration all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:r) { ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) final_phat &lt;- predict(xgbmdl_final, dtest) final_predicted_classes &lt;- ifelse(final_phat &gt; avg_best_youden_threshold, 1, 0) # Using F1 threshold conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test_y, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) final_pred_rocr &lt;- prediction(final_phat, test_y) final_perf_rocr &lt;- performance(final_pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) # Store TPR and FPR values for this iteration all_tpr[[i]] &lt;- final_perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- final_perf_rocr@x.values[[1]] } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_mat_totals / r print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 2519.9 1717.2 ## Predicted_0 345.1 7397.1 3.4.3 Fscore library(xgboost) library(ROCR) best_thresholds &lt;- numeric(r) # To store best thresholds for F1 score f1_stats &lt;- numeric(r) # To store the F1 scores at the best thresholds for(i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Train the model with parameters extracted from a hypothetical &#39;best_hp&#39; object xgbmdl &lt;- xgb.train(params = list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = best_hp$eta, max_depth = best_hp$max_depth, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, min_child_weight = best_hp$min_child_weight, gamma = best_hp$gamma, alpha = best_hp$alpha, lambda = best_hp$lambda ), data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions phat &lt;- predict(xgbmdl, dtest) # Calculate F1 score for each threshold pred &lt;- prediction(phat, test_y) perf &lt;- performance(pred, measure = &quot;prec&quot;, x.measure = &quot;rec&quot;) precision &lt;- slot(perf, &quot;y.values&quot;)[[1]] recall &lt;- slot(perf, &quot;x.values&quot;)[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] f1_score &lt;- (2 * precision * recall) / (precision + recall) # Find the best threshold (maximizing F1 score) best_idx &lt;- which.max(f1_score) best_thresholds[i] &lt;- thresholds[best_idx] f1_stats[i] &lt;- f1_score[best_idx] } # Calculate and print the average of the best thresholds avg_best_f1_threshold &lt;- mean(best_thresholds) cat(&quot;Average Best F1 Threshold:&quot;, avg_best_f1_threshold, &quot;\\n&quot;) ## Average Best F1 Threshold: 0.3865134 # Plot the distribution of max F1 scores for each iteration hist(f1_stats, col = &quot;blue&quot;, main = &quot;Distribution of Max F1 Scores Across Bootstrap Iterations&quot;) abline(v = mean(f1_stats), col = &quot;red&quot;, lwd = 2) # Mean F1 score line # Initialize storage for aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) # Initialize lists to store TPR and FPR values for each bootstrap iteration all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:r) { ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) final_phat &lt;- predict(xgbmdl_final, dtest) # Using the F1 threshold we calculated earlier final_predicted_classes &lt;- ifelse(final_phat &gt; avg_best_f1_threshold, 1, 0) # No need to transform test_y as it&#39;s already binary conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test_y, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) final_pred_rocr &lt;- prediction(final_phat, test_y) final_perf_rocr &lt;- performance(final_pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) # Store TPR and FPR values for this iteration all_tpr[[i]] &lt;- final_perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- final_perf_rocr@x.values[[1]] } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_mat_totals / r print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 2176.0 919.4 ## Predicted_0 718.1 8204.1 3.4.4 ROC curve # First, create common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vector to hold averaged TPR values averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Calculate average best F1 threshold from previous results avg_best_f1_threshold &lt;- mean(best_thresholds) # Find the indices in the ROC curve closest to these thresholds f1_index &lt;- which.min(abs(common_fpr_thresholds - avg_best_f1_threshold)) youden_index &lt;- which.min(abs(common_fpr_thresholds - avg_best_youden_threshold)) fixed_threshold_index &lt;- which.min(abs(common_fpr_thresholds - 0.5)) # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Add points for the thresholds points(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], col = &quot;green&quot;, pch = 19, cex = 1.5) text(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], &quot; Avg Best F1&quot;, pos = 4) points(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], col = &quot;orange&quot;, pch = 19, cex = 1.5) text(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], &quot; Avg Best Youden&quot;, pos = 4) points(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], col = &quot;purple&quot;, pch = 19, cex = 1.5) text(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], &quot; Fixed 0.5 Threshold&quot;, pos = 4, col = &quot;purple&quot;) # Add legend legend(&quot;bottomright&quot;, legend = c(&quot;Avg Best F1&quot;, &quot;Avg Best Youden&quot;, &quot;Fixed 0.5 Threshold&quot;), col = c(&quot;green&quot;, &quot;orange&quot;, &quot;purple&quot;), pch = 19, bty = &quot;n&quot;) "],["regressional-application.html", "Chapter 4 Regressional Application 4.1 Data 4.2 Model Selection", " Chapter 4 Regressional Application 4.1 Data suppressPackageStartupMessages(library(MASS)) suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(DataExplorer)) suppressPackageStartupMessages(library(randomForest)) data(&quot;Boston&quot;) df &lt;- Boston crim: Per capita crime rate by town. zn: Proportion of residential land zoned for lots over 25,000 sq.ft. indus: Proportion of non-retail business acres per town. chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). nox: Nitric oxides concentration (parts per 10 million). rm: Average number of rooms per dwelling. age: Proportion of owner-occupied units built prior to 1940. dis: Weighted distances to five Boston employment centres. rad: Index of accessibility to radial highways. tax: Full-value property-tax rate per $10,000. ptratio: Pupil-teacher ratio by town. black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. lstat: Lower status of the population (percent). medv: Median value of owner-occupied homes in $1000s. glimpse(df) ## Rows: 506 ## Columns: 14 ## $ crim &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ zn &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ indus &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ chas &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ nox &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ rm &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ age &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ dis &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ rad &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ tax &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ black &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ lstat &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ medv &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… 4.1.1 Data Prep data &lt;- df data$chas &lt;- as.factor(data$chas) data$rad &lt;- as.factor(data$rad) names(data)[names(data) == &quot;medv&quot;] &lt;- &quot;y&quot; glimpse(data) ## Rows: 506 ## Columns: 14 ## $ crim &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ zn &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ indus &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ chas &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ nox &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ rm &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ age &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ dis &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ rad &lt;fct&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ tax &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ black &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ lstat &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ y &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… data &lt;- data.frame(Map(function(x, name) { if(is.numeric(x) &amp;&amp; name != &quot;y&quot;) scale(x) else x }, data, names(data))) 4.1.2 Data Explorations plot_correlation(df) 4.2 Model Selection 4.2.1 RF B &lt;- 1200 n &lt;- 100 rmspe &lt;- c() for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] # Fit a Random Forest model mdl &lt;- randomForest(y ~ ., data = trn, ntree = B) yhat &lt;- predict(mdl, tst) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((tst$y - yhat)^2)) } mean_rmspe &lt;- mean(rmspe, na.rm = TRUE) mean_rmspe ## [1] 3.448929 4.2.2 LM n &lt;- 100 rmspe &lt;- c() for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] mdl &lt;-lm(y ~ ., data = trn) yhat &lt;- predict(mdl, tst) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((tst$y - yhat)^2)) } mean_rmspe &lt;- mean(rmspe, na.rm = TRUE) mean_rmspe ## [1] 4.935709 We will add others later but it is the same basic idea as classification algorithms only we use rmspe instead of auc. "],["imbalanced-application.html", "Chapter 5 Imbalanced Application 5.1 Data 5.2 Approaches 5.3 Statistic selection", " Chapter 5 Imbalanced Application 5.1 Data library(readr) library(dplyr) library(randomForest) library(ROCR) library(smotefamily) data &lt;- read_csv(&quot;HTRU_2.csv&quot;, col_names = FALSE) ## Rows: 17898 Columns: 9 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## dbl (9): X1, X2, X3, X4, X5, X6, X7, X8, X9 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Renaming X9 to y and moving it to the first position df &lt;- data.frame(y = data$X9, data[, names(data) != &quot;X9&quot;]) # Converting y to a factor with levels 1 and 0 df$y &lt;- factor(df$y, levels = c(&quot;1&quot;, &quot;0&quot;)) glimpse(df) ## Rows: 17,898 ## Columns: 9 ## $ y &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0… ## $ X1 &lt;dbl&gt; 140.56250, 102.50781, 103.01562, 136.75000, 88.72656, 93.57031, 119… ## $ X2 &lt;dbl&gt; 55.68378, 58.88243, 39.34165, 57.17845, 40.67223, 46.69811, 48.7650… ## $ X3 &lt;dbl&gt; -0.23457141, 0.46531815, 0.32332837, -0.06841464, 0.60086608, 0.531… ## $ X4 &lt;dbl&gt; -0.69964840, -0.51508791, 1.05116443, -0.63623837, 1.12349169, 0.41… ## $ X5 &lt;dbl&gt; 3.1998328, 1.6772575, 3.1212375, 3.6429766, 1.1789298, 1.6362876, 0… ## $ X6 &lt;dbl&gt; 19.110426, 14.860146, 21.744669, 20.959280, 11.468720, 14.545074, 9… ## $ X7 &lt;dbl&gt; 7.975532, 10.576487, 7.735822, 6.896499, 14.269573, 10.621748, 19.2… ## $ X8 &lt;dbl&gt; 74.24222, 127.39358, 63.17191, 53.59366, 252.56731, 131.39400, 479.… table(df$y) ## ## 1 0 ## 1639 16259 df_y1 &lt;- subset(df, y == 1) df_yNot1 &lt;- subset(df, y != 1) df_y1_sampled &lt;- df_y1[sample(nrow(df_y1), size = ceiling(0.1 * nrow(df_y1))), ] df &lt;- rbind(df_yNot1, df_y1_sampled) table(df$y) ## ## 1 0 ## 164 16259 5.2 Approaches 5.2.1 No changes library(randomForest) library(ROCR) B &lt;- 100 n &lt;- 100 # Initialize storage for AUC values and ROC curve data auc &lt;- numeric(n) all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { # Bootstrap sampling idx &lt;- sample(nrow(df), size = nrow(df), replace = TRUE) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Training the RandomForest model model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predicting probabilities phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculating AUC pred_rocr &lt;- prediction(predictions = phat[,1], labels = test$y) # Assuming class 1 probabilities are in the first column auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Constructing ROC curves perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.2 Undersample library(randomForest) library(ROCR) library(dplyr) B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) # Store AUC values all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Downsampling the majority class d1 &lt;- subset(train, y == 1) d0 &lt;- subset(train, y == 0) d0s &lt;- d0[sample(nrow(d0), nrow(d1), replace = TRUE), ] train &lt;- rbind(d1, d0s) %&gt;% sample_n(nrow(d1) * 2) # RandomForest model training model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predicting probabilities phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculating AUC pred_rocr &lt;- prediction(phat[,1], test$y) # Ensure correct column for class probabilities auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Calculating TPR and FPR for ROC perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.3 Oversampling B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) # To store AUC values all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Manual oversampling of the minority class d1 &lt;- subset(train, y == 1) d0 &lt;- subset(train, y == 0) d1s &lt;- d1[sample(nrow(d1), nrow(d0), replace = TRUE), ] train &lt;- rbind(d1s, d0) # Combined oversampled positives with all negatives train &lt;- train[sample(nrow(train)), ] # Shuffle the rows # Model training model &lt;- randomForest(y ~ ., data = train, ntree = B) phat &lt;- predict(model, test, type = &quot;prob&quot;) # AUC calculation pred_rocr &lt;- prediction(phat[,1], test$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # ROC Curve Calculation perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.4 Smoteing Smote has to be in the loop on the train data B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx,] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predict probabilities on the test set phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculate AUC pred_rocr &lt;- prediction(phat[,2], test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Calculate TPR and FPR for ROC perf_rocr &lt;- performance(pred_rocr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.3 Statistic selection Obviously our data isnt balenced so we cannot use ACC or J stat for our threashold selection n &lt;- 100 # Number of iterations b &lt;- 100 # Number of trees in the RandomForest model # Placeholder for the aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2, dimnames = list(c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;), c(&quot;Actual_1&quot;, &quot;Actual_0&quot;))) bt &lt;- c() for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx, ] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model rf_model &lt;- randomForest(y ~ ., data = train, ntree = b) # Predict probabilities on the test set phat &lt;- predict(rf_model, newdata = test, type = &quot;prob&quot;)[,2] # Calculate ROC curve pred_rocr &lt;- prediction(predictions = phat, labels = test$y) # Calculate performance measures for Youden&#39;s J statistic perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;spec&quot;) sensitivity &lt;- perf@y.values[[1]] specificity &lt;- perf@x.values[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] j_values &lt;- sensitivity + specificity - 1 # Find the best threshold best_threshold_index &lt;- which.max(j_values) best_threshold &lt;- thresholds[best_threshold_index] bt[i] &lt;- best_threshold # Make predictions based on the best threshold final_predicted_classes &lt;- ifelse(phat &gt; best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test$y, levels = c(1, 0))) # Update the confusion matrix conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average confusion matrix after all iterations avg_conf_matrix &lt;- conf_mat_totals / n print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 54.21 145.72 ## Predicted_0 7.33 5833.23 mean(bt) ## [1] 0.0794 That has issues We will use F1 for our threshold n &lt;- 100 # Number of iterations b &lt;- 100 # Number of trees in the RandomForest model best_f1_scores &lt;- c() bt &lt;- c() conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2, dimnames = list(c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;), c(&quot;Actual_1&quot;, &quot;Actual_0&quot;))) for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx, ] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model rf_model &lt;- randomForest(y ~ ., data = train, ntree = b) # Predict probabilities on the test set phat &lt;- predict(rf_model, newdata = test, type = &quot;prob&quot;)[,2] # Calculate ROC curve pred_rocr &lt;- prediction(predictions = phat, labels = test$y) perf &lt;- performance(pred_rocr, measure = &quot;prec&quot;, x.measure = &quot;rec&quot;) precision &lt;- perf@y.values[[1]] recall &lt;- perf@x.values[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] # Calculate F1 scores for each threshold f1_scores &lt;- 2 * (precision * recall) / (precision + recall) # Find the best threshold based on F1 score best_f1_index &lt;- which.max(f1_scores) best_f1 &lt;- f1_scores[best_f1_index] best_threshold &lt;- thresholds[best_f1_index] bt[i] &lt;- best_threshold # Store the best F1 score for this iteration best_f1_scores[i] &lt;- best_f1 # Make predictions based on the best threshold final_predicted_classes &lt;- ifelse(phat &gt; best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test$y, levels = c(1, 0))) # Update the confusion matrix totals with the current confusion matrix conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average of the best F1 scores after all iterations avg_best_f1_score &lt;- mean(best_f1_scores) print(paste(&quot;Average Best F1 Score:&quot;, avg_best_f1_score)) ## [1] &quot;Average Best F1 Score: 0.779442513644753&quot; # Calculate the average confusion matrix after all iterations avg_conf_matrix &lt;- conf_mat_totals / n print(&quot;Average Confusion Matrix:&quot;) ## [1] &quot;Average Confusion Matrix:&quot; print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 43.67 10.15 ## Predicted_0 16.24 5973.19 mean(bt) ## [1] 0.6623 We could do more analysis but this sets a good understanding and basline of what we were trying to accomplish As a note for roc curves and confusion tables these are averages not CIs which we should do aswell. "],["speeds.html", "Chapter 6 Speeds 6.1 Functions to run the tests 6.2 Data 6.3 10,000 rows 6.4 500,000 rows 6.5 Presentations 6.6 Analysis", " Chapter 6 Speeds library(xgboost) library(ROCR) library(foreach) library(doParallel) library(Matrix) library(readr) library(dplyr) library(ggplot2) library(tidyr) 6.1 Functions to run the tests I will make 3 functions. One with my parallel processing, one with xgboost pp and then one with no pp at all so n thread = 1. I have included the code so that you can see it but it is not important just know that we are looping r times and that the loops use bootstrapping for splits. xgb1 &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer auc &lt;- numeric(r) # Pre-allocate a numeric vector for AUC values for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] test_y &lt;- y[-ind] # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = 1) predictions &lt;- predict(model, dtest) pred &lt;- ROCR::prediction(predictions, test_y) perf &lt;- ROCR::performance(pred, &quot;auc&quot;) auc[i] &lt;- perf@y.values[[1]] } end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(auc = auc, time_taken = time_taken) } xgb2 &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer auc &lt;- numeric(r) # Initialize the AUC vector to store AUC values for each iteration for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] # Potential indexing issue test_y &lt;- y[-ind] # Potential indexing issue # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = detectCores() - 1) # Predicting and calculating AUC predictions &lt;- predict(model, dtest) pred &lt;- ROCR::prediction(predictions, test_y) perf &lt;- ROCR::performance(pred, &quot;auc&quot;) auc[i] &lt;- perf@y.values[[1]] } end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(mean_auc = mean(auc), time_taken = time_taken) } xgbpar &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer cl &lt;- makeCluster(detectCores() - 1) # Use one less than the total number of cores registerDoParallel(cl) # Parallel processing using foreach results &lt;- foreach(i = 1:r, .combine = &#39;c&#39;, .packages = c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] # Potential indexing issue test_y &lt;- y[-ind] # Potential indexing issue # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = 1) predictions &lt;- predict(model, dtest) pred &lt;- prediction(predictions, test_y) perf &lt;- performance(pred, &quot;auc&quot;) perf@y.values[[1]] } # Stop the cluster stopCluster(cl) end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(mean_auc = mean(results), time_taken = time_taken) } 6.2 Data generate_dataset &lt;- function(n_rows) { set.seed(123) # Ensure reproducibility df &lt;- data.frame(y = sample(c(0, 1), n_rows, replace = TRUE)) # Add 10 numeric columns for(i in 1:10) { df &lt;- df %&gt;% mutate(!!paste0(&quot;num&quot;, i) := runif(n_rows)) } # Add 20 factor columns for(i in 1:20) { df &lt;- df %&gt;% mutate(!!paste0(&quot;fac&quot;, i) := factor(sample(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), n_rows, replace = TRUE))) } return(df) } # Generate datasets of different sizes df&lt;- generate_dataset(1000) dfb &lt;- generate_dataset(500000) 6.3 10,000 rows Create the data. as a note this data has no relations so it wont be predictable 20 factor collums 10 numeric 6.3.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = df) y &lt;- df$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1sf &lt;- x1$time_taken t2sf &lt;- x2$time_taken tpsf &lt;- xp$time_taken 6.3.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- df$y xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1ssf &lt;- x1$time_taken t2ssf &lt;- x2$time_taken tpssf &lt;- xp$time_taken 6.4 500,000 rows 6.4.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = dfb) y &lt;- dfb$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t150 &lt;- x1$time_taken t250 &lt;- x2$time_taken tp50 &lt;- xp$time_taken 6.4.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- dfb$y xs &lt;- model.matrix(~ . -1 - y, data = dfb) xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1s50 &lt;- x1$time_taken t2s50 &lt;- x2$time_taken tps50 &lt;- xp$time_taken 6.5 Presentations Print them out nrounds is 100 for all 10ks matrix t1sf ## [1] 3.433683 t2sf ## [1] 6.90127 tpsf ## [1] 2.992989 sparse t1ssf ## [1] 2.550287 t2ssf ## [1] 6.555671 tpssf ## [1] 2.82231 500ks matrix t150 ## [1] 1536.92 t250 ## [1] 223.5567 tp50 ## [1] 439.4427 sparse t1s50 ## [1] 1124.695 t2s50 ## [1] 203.3919 tps50 ## [1] 337.0733 6.6 Analysis 6.6.1 Graphs # Data preparation results &lt;- data.frame( dataset_size = c(rep(&quot;10k&quot;, 6), rep(&quot;500k&quot;, 6)), matrix_type = rep(c(&quot;Normal&quot;, &quot;Sparse&quot;), each = 3, times = 2), method = rep(c(&quot;No PP&quot;, &quot;XGBoost PP&quot;, &quot;Own PP&quot;), times = 4), execution_time = c(t1sf, t2sf, tpsf, t1ssf, t2ssf, tpssf, t150, t250, tp50, t1s50, t2s50, tps50) ) # Graph for comparing methods on 10k rows dataset ggplot(subset(results, dataset_size == &quot;10k&quot;), aes(x = method, y = execution_time, fill = matrix_type)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time Comparison for 10k Rows Dataset&quot;, x = &quot;Method&quot;, y = &quot;Execution Time (seconds)&quot;, fill = &quot;Matrix Type&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Graph for comparing methods on 500k rows dataset ggplot(subset(results, dataset_size == &quot;500k&quot;), aes(x = method, y = execution_time, fill = matrix_type)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time Comparison for 500k Rows Dataset&quot;, x = &quot;Method&quot;, y = &quot;Execution Time (seconds)&quot;, fill = &quot;Matrix Type&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Update the results data frame to include execution time per row results$execution_time_per_row &lt;- with(results, execution_time / ifelse(dataset_size == &quot;10k&quot;, 10000, 500000)) # Re-define the plotting function to use execution_time_per_row plot_dataset_size_effect_per_row &lt;- function(data, method_name) { ggplot(subset(data, method == method_name), aes(x = dataset_size, y = execution_time_per_row, fill = dataset_size)) + geom_bar(stat = &quot;identity&quot;) + labs(title = paste(&quot;Effect of Dataset Size on&quot;, method_name, &quot;Execution Time Per Row&quot;), x = &quot;Dataset Size&quot;, y = &quot;Execution Time Per Row (seconds)&quot;, fill = &quot;Dataset Size&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) } # Separate results for normal and sparse matrices normal_results_per_row &lt;- subset(results, matrix_type == &quot;Normal&quot;) sparse_results_per_row &lt;- subset(results, matrix_type == &quot;Sparse&quot;) # Plotting for each method, now on a per-row basis plot_dataset_size_effect_per_row(normal_results_per_row, &quot;No PP&quot;) plot_dataset_size_effect_per_row(normal_results_per_row, &quot;XGBoost PP&quot;) plot_dataset_size_effect_per_row(normal_results_per_row, &quot;Own PP&quot;) # Plotting for No PP, XGBoost PP, and Own PP methods for Sparse Matrix plot_dataset_size_effect_per_row(sparse_results_per_row, &quot;No PP&quot;) + ggtitle(&quot;No Parallel Processing (Sparse Matrix)&quot;) plot_dataset_size_effect_per_row(sparse_results_per_row, &quot;XGBoost PP&quot;) + ggtitle(&quot;XGBoost Parallel Processing (Sparse Matrix)&quot;) plot_dataset_size_effect_per_row(sparse_results_per_row, &quot;Own PP&quot;) + ggtitle(&quot;Own Parallel Processing (Sparse Matrix)&quot;) 6.6.2 Takaways With factor collums sparse seems to be better. With numeric collums though it would be worse. XGB is better with internal processing when nrows is huge. When nrows is smaller though we should probably be using gbm anyways. Also times include how many loops we are doing which is related to numcores of the machine being used if you look at our codes 6.6.3 Things to change Test light gbm and gbm with these. test if only numerical data kills a sparse matrix. What about doparrallel with numcores set inside the xgboost funtions. What about purrr or other ways of parrallel processing. Do correlations in the data(the data actually being predictable have a effect). Does scaling help with speed? Test purrr/furrr Test these vs h2o "],["multi-variable-classification.html", "Chapter 7 Multi Variable Classification 7.1 Data 7.2 AUCs 7.3 Confusion Table", " Chapter 7 Multi Variable Classification 7.1 Data library(randomForest) library(ROCR) library(dplyr) library(caret) # Load the Wine Quality dataset url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot; df &lt;- read.csv(url, sep=&quot;;&quot;) names(df)[names(df) == &quot;quality&quot;] &lt;- &quot;y&quot; df &lt;- df[c(&quot;y&quot;, setdiff(names(df), &quot;y&quot;))] df$y &lt;- as.factor(df$y) glimpse(df) ## Rows: 1,599 ## Columns: 12 ## $ y &lt;fct&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 7… ## $ fixed.acidity &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5… ## $ volatile.acidity &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, … ## $ citric.acid &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0… ## $ residual.sugar &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1,… ## $ chlorides &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, … ## $ free.sulfur.dioxide &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16… ## $ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102,… ## $ density &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0… ## $ pH &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3… ## $ sulphates &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0… ## $ alcohol &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.… table(df$y) ## ## 3 4 5 6 7 8 ## 10 53 681 638 199 18 we need to drop 3 and 8 df&lt;- df[!df$y %in% c(3, 8), ] df$y &lt;- droplevels(df$y) 7.2 AUCs n &lt;- nrow(df) ind &lt;- unique(sample(n, n, T)) train &lt;- df[ind, ] test &lt;- df[-ind, ] rf &lt;- randomForest(y ~ ., data = train, ntree = 500) phat &lt;- predict(rf, test, type = &quot;prob&quot;) auc_list &lt;- list() for (level in levels(test$y)) { # Making the current level the positive class actual &lt;- ifelse(test$y == level, 1, 0) pred &lt;- prediction(phat[, level], actual) perf &lt;- performance(pred, measure = &quot;auc&quot;) auc_list[[level]] &lt;- as.numeric(perf@y.values) } auc_list ## $`4` ## [1] 0.828346 ## ## $`5` ## [1] 0.8633025 ## ## $`6` ## [1] 0.8037732 ## ## $`7` ## [1] 0.9092694 # Prepare plotting par(mfrow = c(2, ceiling(length(levels(test$y)) / 2))) # Adjust layout based on number of levels # Loop through each level to plot ROC curve for (level in levels(test$y)) { actual &lt;- ifelse(test$y == level, 1, 0) pred &lt;- prediction(phat[, level], actual) perf &lt;- performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) # Plotting the ROC curve plot(perf, col = &quot;red&quot;, main = paste(&quot;ROC Curve for&quot;, level)) abline(a = 0, b = 1, lty = 2) # Adding a diagonal line } 7.3 Confusion Table r &lt;- 100 n &lt;- nrow(df) num_classes &lt;- length(levels(df$y)) auc_matrix &lt;- matrix(0, nrow = r, ncol = num_classes) colnames(auc_matrix) &lt;- levels(df$y) cm_list &lt;- vector(&quot;list&quot;, r) for (i in 1:r) { ind &lt;- sample(n, n, replace = TRUE) tr &lt;- df[ind, ] ts &lt;- df[-ind, ] rf &lt;- randomForest(y ~ ., data = tr, ntree = 500) ph &lt;- predict(rf, ts, type = &quot;prob&quot;) pc &lt;- apply(ph, 1, which.max) pc &lt;- levels(ts$y)[pc] cm &lt;- confusionMatrix(factor(pc), factor(ts$y)) cm_list[[i]] &lt;- as.matrix(cm$table) } # Calculate the average confusion matrix cm_avg &lt;- Reduce(&quot;+&quot;, cm_list) / length(cm_list) cm_avg ## Reference ## Prediction 4 5 6 7 ## 4 0.18 0.29 0.35 0.06 ## 5 13.06 194.52 55.72 4.07 ## 6 6.14 51.46 163.33 35.52 ## 7 0.28 1.81 13.91 34.49 We see huge imbalances in data in the target variable but i just wanted to leave this here as a basic idea. in truth we would drop some categories and deal with it better. "],["h20.html", "Chapter 8 h20 8.1 Set-Up 8.2 EDA 8.3 Models h20", " Chapter 8 h20 8.1 Set-Up library(readr) library(dplyr) library(tidyr) library(ggplot2) library(DataExplorer) library(caret) library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 library(tidyr) library(readr) data &lt;- read_csv(&quot;machine failure.csv&quot;) ## Rows: 10000 Columns: 14 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (2): Product ID, Type ## dbl (12): UDI, Air temperature [K], Process temperature [K], Rotational spee... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(data) ## Rows: 10,000 ## Columns: 14 ## $ UDI &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1… ## $ `Product ID` &lt;chr&gt; &quot;M14860&quot;, &quot;L47181&quot;, &quot;L47182&quot;, &quot;L47183&quot;, &quot;L47… ## $ Type &lt;chr&gt; &quot;M&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;M&quot;, &quot;L&quot;, &quot;L&quot;, &quot;M&quot;,… ## $ `Air temperature [K]` &lt;dbl&gt; 298.1, 298.2, 298.1, 298.2, 298.2, 298.1, 29… ## $ `Process temperature [K]` &lt;dbl&gt; 308.6, 308.7, 308.5, 308.6, 308.7, 308.6, 30… ## $ `Rotational speed [rpm]` &lt;dbl&gt; 1551, 1408, 1498, 1433, 1408, 1425, 1558, 15… ## $ `Torque [Nm]` &lt;dbl&gt; 42.8, 46.3, 49.4, 39.5, 40.0, 41.9, 42.4, 40… ## $ `Tool wear [min]` &lt;dbl&gt; 0, 3, 5, 7, 9, 11, 14, 16, 18, 21, 24, 29, 3… ## $ `Machine failure` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ TWF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ HDF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ PWF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OSF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ RNF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ID: A unique identifier for each observation in the dataset. Product Id: A combined identifier that starts with the machine type followed by a numeric identifier. Type: Categorizes the type of machine, which can affect its failure rates and operational characteristics. Air Temperature [K]: The ambient air temperature around the machine, measured in Kelvin, which might influence machine performance. Process Temperature [K]: The operational temperature of the machine during the process, also measured in Kelvin. Rotational Speed [rpm]: Indicates how fast the machine operates, measured in rotations per minute. Torque [Nm]: Measures the twisting force that causes rotation, in Newton-meters. High torque may indicate higher operational stress. Tool Wear [min]: Tracks the amount of wear on the machine’s tools, suggesting when maintenance or replacement might be necessary. Machine Failure: The target variable indicating if a failure occurred (1) or not (0). Additional columns related to specific failure types include: TWF (Tool Wear Failure): Indicates failures due to the tool wearing out. HDF (Heat Dissipation Failure): Relates to failures caused by inadequate heat dissipation. PWF (Power Failure): Associated with failures due to power issues. OSF (Overstrain Failure): Indicates failures from overstressing the machine. RNF (Random Failure): Captures failures that are random or do not fit into other specified categories. library(janitor) ## Warning: package &#39;janitor&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test data &lt;- data %&gt;% clean_names() data &lt;- data %&gt;% mutate(across(c(type, twf, hdf, pwf, osf, rnf, machine_failure), as.factor)) data &lt;- data[,-(1:2)] # kill the two ids data &lt;- data %&gt;% rename(y = machine_failure) %&gt;% relocate(y, .before = 1) The data is ready for gerneral usage introduce(data) ## # A tibble: 1 × 9 ## rows columns discrete_columns continuous_columns all_missing_columns ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 10000 12 7 5 0 ## # ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;, ## # total_observations &lt;int&gt;, memory_usage &lt;dbl&gt; plot_missing(data) 8.2 EDA # Histograms for numerical variables plot_histogram(data) # For factor variables, use bar plots plot_bar(data) # Summary statistics for numerical data data %&gt;% summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median, IQR = IQR, min = min, max = max))) ## # A tibble: 1 × 30 ## air_temperature_k_mean air_temperature_k_sd air_temperature_k_median ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 300. 2.00 300. ## # ℹ 27 more variables: air_temperature_k_IQR &lt;dbl&gt;, ## # air_temperature_k_min &lt;dbl&gt;, air_temperature_k_max &lt;dbl&gt;, ## # process_temperature_k_mean &lt;dbl&gt;, process_temperature_k_sd &lt;dbl&gt;, ## # process_temperature_k_median &lt;dbl&gt;, process_temperature_k_IQR &lt;dbl&gt;, ## # process_temperature_k_min &lt;dbl&gt;, process_temperature_k_max &lt;dbl&gt;, ## # rotational_speed_rpm_mean &lt;dbl&gt;, rotational_speed_rpm_sd &lt;dbl&gt;, ## # rotational_speed_rpm_median &lt;dbl&gt;, rotational_speed_rpm_IQR &lt;dbl&gt;, … # Identify zero variance features zero_var_indices &lt;- nearZeroVar(data, saveMetrics = TRUE) # View the metrics to determine which variables have zero or near-zero variance print(zero_var_indices) ## freqRatio percentUnique zeroVar nzv ## y 28.498525 0.02 FALSE TRUE ## type 2.002002 0.03 FALSE FALSE ## air_temperature_k 1.207792 0.93 FALSE FALSE ## process_temperature_k 1.161172 0.82 FALSE FALSE ## rotational_speed_rpm 1.116279 9.41 FALSE FALSE ## torque_nm 1.040000 5.77 FALSE FALSE ## tool_wear_min 1.739130 2.46 FALSE FALSE ## twf 216.391304 0.02 FALSE TRUE ## hdf 85.956522 0.02 FALSE TRUE ## pwf 104.263158 0.02 FALSE TRUE ## osf 101.040816 0.02 FALSE TRUE ## rnf 525.315789 0.02 FALSE TRUE # Optionally, print the names of columns with zero variance zero_var_columns &lt;- names(data)[zero_var_indices$nzv] print(zero_var_columns) ## [1] &quot;y&quot; &quot;twf&quot; &quot;hdf&quot; &quot;pwf&quot; &quot;osf&quot; &quot;rnf&quot; for now we wont care about the type of failure we just will care about whether it failed or not # right now i am forced to use the numerical positions of the collumns due to issues knitting into a book. in practice we want to use select and name the columns data &lt;- data[, -(8:12)] 8.3 Models h20 library(h2o) ## ## ---------------------------------------------------------------------- ## ## Your next step is to start H2O: ## &gt; h2o.init() ## ## For H2O package documentation, ask for help: ## &gt; ??h2o ## ## After starting H2O, you can use the Web UI at http://localhost:54321 ## For more information visit https://docs.h2o.ai ## ## ---------------------------------------------------------------------- ## ## Attaching package: &#39;h2o&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## day, hour, month, week, year ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, sd, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, %in%, &amp;&amp;, ||, apply, as.factor, as.numeric, colnames, ## colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log, ## log10, log1p, log2, round, signif, trunc #dir.create(&quot;C:/Users/simon/Dropbox/github_ML/Codes&amp;Processes/h2o_logs&quot;) h2o.init(max_mem_size = &quot;12g&quot;, log_dir = &quot;C:/Users/simon/Dropbox/github_ML/Codes&amp;Processes/h2o_logs&quot;) ## Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 1 hours 40 minutes ## H2O cluster timezone: America/Halifax ## H2O data parsing timezone: UTC ## H2O cluster version: 3.46.0.1 ## H2O cluster version age: 9 months and 3 days ## H2O cluster name: H2O_started_from_R_simon_ndv362 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 12.00 GB ## H2O cluster total cores: 20 ## H2O cluster allowed cores: 20 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## R Version: R version 4.3.2 (2023-10-31 ucrt) ## Warning in h2o.clusterInfo(): ## Your H2O cluster version is (9 months and 3 days) old. There may be a newer version available. ## Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html data_h2o &lt;- as.h2o(data) ## | | | 0% | |======================================================================| 100% data_h2o$y &lt;- as.factor(data_h2o$y) 8.3.1 Random Forest auc_values &lt;- c() for (i in 1:100) { # Split the data into train and test sets splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8) train &lt;- splits[[1]] test &lt;- splits[[2]] # Train a Random Forest model rf_model &lt;- h2o.randomForest( x = names(train)[-which(names(train) == &quot;y&quot;)], y = &quot;y&quot;, training_frame = train, ntrees = 1200 ) # Perform predictions predictions &lt;- h2o.predict(rf_model, test) # Get performance object perf &lt;- h2o.performance(rf_model, newdata = test) # Calculate AUC auc_values[i] &lt;- h2o.auc(perf) } ## | | | 0% | |=== | 4% | |================== | 26% | |============================================== | 66% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========= | 14% | |======================== | 35% | |========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================== | 58% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================= | 36% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========== | 15% | |======================== | 35% | |====================================================== | 77% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========================== | 39% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================== | 37% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |======================================================== | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |======================================================== | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 36% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============================ | 40% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 36% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |========================== | 37% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================= | 35% | |======================================================= | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 16% | |========================== | 38% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========== | 14% | |======================== | 35% | |======================================================= | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 39% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 39% | |=========================================== | 62% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 39% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 39% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 38% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 38% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 38% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============================= | 41% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |============================================================ | 86% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 38% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================== | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================= | 35% | |======================================================= | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==================== | 28% | |========================================== | 60% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 39% | |============================================================ | 86% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 39% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |========================== | 37% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |======================== | 35% | |====================================================== | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================= | 36% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====================== | 31% | |============================================= | 64% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |========================== | 38% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 36% | |========================================================== | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============================= | 41% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |============================ | 40% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |=========================== | 38% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 16% | |========================== | 37% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |======================================================== | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 16% | |========================== | 38% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 16% | |========================== | 38% | |============================================================ | 86% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 35% | |======================================== | 57% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========== | 15% | |======================== | 35% | |====================================================== | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |====================================================== | 77% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========== | 15% | |========================= | 36% | |======================================================= | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 39% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |======================================================== | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============================ | 40% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========== | 15% | |======================== | 35% | |===================================================== | 75% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================= | 36% | |====================================================== | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |========================================================== | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 36% | |====================================================== | 77% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |======================================================== | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |====================================================== | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================= | 35% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========== | 15% | |========================= | 35% | |======================================================= | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========================== | 39% | |============================================================= | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 38% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============ | 17% | |=========================== | 38% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 16% | |========================== | 37% | |======================================================= | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=========== | 15% | |========================= | 36% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% # Calculate the mean AUC value mean(auc_values) ## [1] 0.9705556 8.3.2 Gradient Boosting Machine note that cartesian uses all the grid values. the random version just selects random values from the grid which is very valid # Define the hyperparameter grid with added ntrees and learn_rate_annealing options hyper_params &lt;- list( max_depth = seq(3, 6, by = 3), min_rows = c(10), learn_rate = seq(0.01, 0.2, by = 0.04), learn_rate_annealing = seq(0.99, 1, length.out = 2), col_sample_rate_per_tree = seq(0.75, 1, length.out = 2), col_sample_rate = seq(1, 1, length.out = 1), ntrees = c(100, 250, 500, 750), min_split_improvement = c(1e-5) ) search_criteria &lt;- list(strategy = &quot;Cartesian&quot;) How is column sampling implemented for GBM? For an example model using: 100-column dataset col_sample_rate_per_tree=0.754 col_sample_rate=0.8 (Refers to available columns after per-tree sampling) For each tree, the floor is used to determine the number - in this example, (0.754 * 100)=75 out of the 100 - of columns that are randomly picked, and then the floor is used to determine the number - in this case, (0.754 * 0.8 * 100)=60 - of columns that are then randomly chosen for each split decision (out of the 75). # Grid search setup with added parameters grid_result &lt;- h2o.grid( algorithm = &quot;gbm&quot;, grid_id = &quot;gbm_grid_search_extended&quot;, x = setdiff(names(data_h2o), &quot;y&quot;), y = &quot;y&quot;, training_frame = data_h2o, hyper_params = hyper_params, search_criteria = search_criteria, nfolds = 5 ) ## | | | 0% | |======================================================================| 100% # Retrieve the grid results, sorted by AUC full_grid &lt;- h2o.getGrid(grid_id = &quot;gbm_grid_search_extended&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE) # Extract the grid results into a data frame full_summary_df &lt;- as.data.frame(full_grid@summary_table) # Print the structure of the data frame print(str(full_summary_df)) ## &#39;data.frame&#39;: 160 obs. of 10 variables: ## $ col_sample_rate : num 1 1 1 1 1 1 1 1 1 1 ... ## $ col_sample_rate_per_tree: num 1 0.75 1 0.75 0.75 0.75 1 1 1 1 ... ## $ learn_rate : num 0.13 0.13 0.05 0.13 0.17 0.09 0.13 0.09 0.13 0.17 ... ## $ learn_rate_annealing : num 0.99 0.99 1 0.99 0.99 0.99 0.99 0.99 1 0.99 ... ## $ max_depth : num 6 6 6 6 6 6 6 6 6 6 ... ## $ min_rows : num 10 10 10 10 10 10 10 10 10 10 ... ## $ min_split_improvement : num 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 ... ## $ ntrees : num 250 500 250 750 750 500 500 250 100 500 ... ## $ model_ids : chr &quot;gbm_grid_search_extended_model_68&quot; &quot;gbm_grid_search_extended_model_107&quot; &quot;gbm_grid_search_extended_model_74&quot; &quot;gbm_grid_search_extended_model_147&quot; ... ## $ auc : num 0.98 0.977 0.977 0.976 0.976 ... ## - attr(*, &quot;header&quot;)= chr &quot;Hyper-Parameter Search Summary&quot; ## - attr(*, &quot;formats&quot;)= chr [1:10] &quot;%.5f&quot; &quot;%.5f&quot; &quot;%.5f&quot; &quot;%.5f&quot; ... ## - attr(*, &quot;description&quot;)= chr &quot;ordered by decreasing auc&quot; ## NULL splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8) train &lt;- splits[[1]] test &lt;- splits[[2]] best_model &lt;- h2o.getModel(full_grid@model_ids[[1]]) predictions &lt;- h2o.predict(best_model, test) ## | | | 0% | |======================================================================| 100% actual &lt;- factor(as.vector(test$y), levels = c(&quot;1&quot;, &quot;0&quot;)) predicted &lt;- factor(as.vector(predictions$predict), levels = c(&quot;1&quot;, &quot;0&quot;)) # maxed f1 score confusion_matrix &lt;- table(Actual = actual, Predicted = predicted) print(confusion_matrix) ## Predicted ## Actual 1 0 ## 1 72 1 ## 0 0 1925 # Access the best model from the grid search best_model_id &lt;- grid_result@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) # Retrieve the panel with all parameters used by the best model best_params &lt;- best_model@allparameters # Display winning hyperparameters best_params &lt;- best_model@allparameters cat(&quot;Winning Hyperparameters:\\n&quot;) ## Winning Hyperparameters: cat(sprintf(&quot;Max Depth: %d\\n&quot;, best_params$max_depth)) ## Max Depth: 6 cat(sprintf(&quot;Min Rows: %d\\n&quot;, best_params$min_rows)) ## Min Rows: 10 cat(sprintf(&quot;Learn Rate: %.2f\\n&quot;, best_params$learn_rate)) ## Learn Rate: 0.13 cat(sprintf(&quot;Learn Rate Annealing: %.2f\\n&quot;, best_params$learn_rate_annealing)) ## Learn Rate Annealing: 0.99 cat(sprintf(&quot;Sample Rate: %.2f\\n&quot;, best_params$col_sample_rate_per_tree)) ## Sample Rate: 1.00 cat(sprintf(&quot;Column Sample Rate: %.2f\\n&quot;, best_params$col_sample_rate)) ## Column Sample Rate: 1.00 cat(sprintf(&quot;Number of Trees (ntrees): %d\\n&quot;, best_params$ntrees)) ## Number of Trees (ntrees): 250 cat(sprintf(&quot;Minimum Split Improvement: %.1e\\n&quot;, best_params$min_split_improvement)) ## Minimum Split Improvement: 1.0e-05 # Retrieve the parameters used by the best model # Loop to perform repeated train-test split and model evaluation r &lt;- 100 auc_scores &lt;- numeric(r) for (i in 1:r) { splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8, seed = i) train &lt;- splits[[1]] test &lt;- splits[[2]] # Train the model with the best parameters from the grid search model &lt;- h2o.gbm( x = setdiff(names(train), &quot;y&quot;), y = &quot;y&quot;, training_frame = train, validation_frame = test, learn_rate = best_params$learn_rate, max_depth = best_params$max_depth, col_sample_rate_per_tree = best_params$col_sample_rate_per_tree, col_sample_rate = best_params$col_sample_rate, ntrees = best_params$ntrees, learn_rate_annealing = best_params$learn_rate_annealing, min_split_improvement = best_params$min_split_improvement ) perf &lt;- h2o.performance(model, newdata = test) auc_scores[i] &lt;- h2o.auc(perf) } ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================= | 47% | |======================================================================| 100% ## | | | 0% | |=============================== | 45% | |======================================================================| 100% ## | | | 0% | |=============================== | 45% | |======================================================================| 100% ## | | | 0% | |============================ | 40% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 45% | |======================================================================| 100% ## | | | 0% | |=================================== | 50% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 45% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================== | 42% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |============================ | 40% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |============================= | 41% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |======================================================================| 100% ## | | | 0% | |============================== | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================== | 42% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================= | 41% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================= | 41% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |============================ | 40% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=========================== | 39% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |============================== | 42% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 42% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================= | 47% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================= | 42% | |======================================================================| 100% ## | | | 0% | |============================= | 41% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |======================================================================| 100% ## | | | 0% | |================================= | 47% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 45% | |======================================================================| 100% ## | | | 0% | |============================== | 43% | |======================================================================| 100% ## | | | 0% | |================================ | 46% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |=============================== | 44% | |======================================================================| 100% ## | | | 0% | |============================ | 40% | |======================================================================| 100% mean_auc &lt;- mean(auc_scores) print(paste(&quot;Average AUC over&quot;, r, &quot;trials:&quot;, mean_auc)) ## [1] &quot;Average AUC over 100 trials: 0.975179909924385&quot; 8.3.3 AutoML # Step 1: Define your target variable y &lt;- &quot;y&quot; # Step 2: Define predictors x &lt;- setdiff(names(data_h2o), y) # Step 3: Run AutoML aml &lt;- h2o.automl( x = x, y = y, training_frame = data_h2o, max_models = 20, max_runtime_secs = 3600*5 ) ## | | | 0% | |== | 3% ## 01:53:37.874: AutoML: XGBoost is not available; skipping it. | |====== | 9% | |========== | 15% | |================ | 24% | |=================== | 26% | |=============================== | 44% | |===================================== | 53% | |=========================================== | 62% | |====================================================== | 76% | |======================================================== | 79% | |======================================================================| 100% # Step 4: View the AutoML Leaderboard lb &lt;- h2o.get_leaderboard(aml, extra_columns = &quot;ALL&quot;) print(lb) ## model_id auc logloss ## 1 GBM_grid_1_AutoML_1_20241217_15337_model_5 0.9785118 0.05020290 ## 2 StackedEnsemble_BestOfFamily_1_AutoML_1_20241217_15337 0.9764793 0.04961507 ## 3 StackedEnsemble_AllModels_1_AutoML_1_20241217_15337 0.9762244 0.04832240 ## 4 GBM_3_AutoML_1_20241217_15337 0.9734979 0.05358725 ## 5 GBM_4_AutoML_1_20241217_15337 0.9708714 0.05677718 ## 6 GBM_1_AutoML_1_20241217_15337 0.9705430 0.06252544 ## aucpr mean_per_class_error rmse mse training_time_ms ## 1 0.7958558 0.1492782 0.1148742 0.01319607 367 ## 2 0.8063697 0.1532890 0.1132089 0.01281626 1980 ## 3 0.8138875 0.1333128 0.1110668 0.01233584 4403 ## 4 0.7886287 0.1492265 0.1160056 0.01345729 298 ## 5 0.7758282 0.1639240 0.1191760 0.01420293 294 ## 6 0.6912423 0.1634067 0.1318574 0.01738638 448 ## predict_time_per_row_ms algo ## 1 0.008128 GBM ## 2 0.009886 StackedEnsemble ## 3 0.012892 StackedEnsemble ## 4 0.007161 GBM ## 5 0.007765 GBM ## 6 0.015873 GBM ## ## [22 rows x 10 columns] # Step 5: Get the best model best_model &lt;- aml@leader h2o.performance(best_model, data_h2o) ## H2OBinomialMetrics: gbm ## ## MSE: 0.00452924 ## RMSE: 0.06729963 ## LogLoss: 0.01869065 ## Mean Per-Class Error: 0.04075459 ## AUC: 0.9991194 ## AUCPR: 0.9794259 ## Gini: 0.9982388 ## R^2: 0.8617059 ## AIC: NaN ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 9643 18 0.001863 =18/9661 ## 1 27 312 0.079646 =27/339 ## Totals 9670 330 0.004500 =45/10000 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.347916 0.932735 165 ## 2 max f2 0.253541 0.941860 187 ## 3 max f0point5 0.470818 0.956123 140 ## 4 max accuracy 0.358436 0.995500 162 ## 5 max precision 0.993408 1.000000 0 ## 6 max recall 0.033332 1.000000 313 ## 7 max specificity 0.993408 1.000000 0 ## 8 max absolute_mcc 0.347916 0.930498 165 ## 9 max min_per_class_accuracy 0.126158 0.986130 236 ## 10 max mean_per_class_accuracy 0.125019 0.988588 237 ## 11 max tns 0.993408 9661.000000 0 ## 12 max fns 0.993408 337.000000 0 ## 13 max fps 0.000230 9661.000000 399 ## 14 max tps 0.033332 339.000000 313 ## 15 max tnr 0.993408 1.000000 0 ## 16 max fnr 0.993408 0.994100 0 ## 17 max fpr 0.000230 1.000000 399 ## 18 max tpr 0.033332 1.000000 313 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # Retrieve the best model from AutoML best_model &lt;- aml@leader # Initialize a vector to store AUC scores or any other performance metric performance_scores &lt;- numeric(100) # Loop to perform the train-test split, predict, and calculate performance 100 times for (i in 1:100) { # Split the data into 80% training and 20% testing splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8) train &lt;- splits[[1]] test &lt;- splits[[2]] pred &lt;- h2o.predict(best_model, test) # Evaluate performance perf &lt;- h2o.performance(best_model, newdata = test) auc_score &lt;- h2o.auc(perf) performance_scores[i] &lt;- auc_score } ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% # Calculate average performance across all splits average_performance &lt;- mean(performance_scores) print(paste(&quot;Average AUC over 100 trials: &quot;, average_performance)) ## [1] &quot;Average AUC over 100 trials: 0.999094809504338&quot; now we get the info about the winning model best_model@model_id ## [1] &quot;GBM_grid_1_AutoML_1_20241217_15337_model_5&quot; best_model@parameters ## $model_id ## [1] &quot;GBM_grid_1_AutoML_1_20241217_15337_model_5&quot; ## ## $training_frame ## [1] &quot;AutoML_1_20241217_15337_training_RTMP_sid_88fc_5&quot; ## ## $nfolds ## [1] 5 ## ## $keep_cross_validation_models ## [1] FALSE ## ## $keep_cross_validation_predictions ## [1] TRUE ## ## $score_tree_interval ## [1] 5 ## ## $fold_assignment ## [1] &quot;Modulo&quot; ## ## $ntrees ## [1] 71 ## ## $max_depth ## [1] 11 ## ## $min_rows ## [1] 30 ## ## $stopping_metric ## [1] &quot;logloss&quot; ## ## $stopping_tolerance ## [1] 0.01 ## ## $seed ## [1] &quot;6732902278549859389&quot; ## ## $distribution ## [1] &quot;bernoulli&quot; ## ## $sample_rate ## [1] 0.7 ## ## $histogram_type ## [1] &quot;UniformAdaptive&quot; ## ## $categorical_encoding ## [1] &quot;Enum&quot; ## ## $calibration_method ## [1] &quot;PlattScaling&quot; ## ## $x ## [1] &quot;type&quot; &quot;air_temperature_k&quot; &quot;process_temperature_k&quot; ## [4] &quot;rotational_speed_rpm&quot; &quot;torque_nm&quot; &quot;tool_wear_min&quot; ## ## $y ## [1] &quot;y&quot; its stacked ensambled so we need to get the models that are in the stack lets get the base models base_models &lt;- best_model@model$base_models base_models ## NULL now the meta one metalearner &lt;- best_model@model$metalearner metalearner ## NULL = GLM: Generalized Linear Model DRF: Distributed Random Forest GBM: Gradient Boosting Machine DeepLearning: Neural Networks StackedEnsemble: Ensemble methods XGBoost: eXtreme Gradient Boosting (if available in your H2O installation 8.3.4 XGBoost we need to set up the gpus for this # Check if XGBoost is available h2o.xgboost.available() ## [1] &quot;Cannot build a XGboost model - no backend found.&quot; ## [1] FALSE h2o.shutdown(prompt = FALSE) "],["econometrics.html", "Chapter 9 08_Econometrics 9.1 A. California Test Scores 9.2 Auto Correlation", " Chapter 9 08_Econometrics 9.1 A. California Test Scores 9.1.1 Introduction 9.1.1.1 Purpose The Purpose of this section is display my final assignment from ECON 4403 (econometrics) at SMU. I will also be adding some other important elements of what i have learned. I will not be going crazy with a large amount of theory but i will be acknowledging some of the main issues. 9.1.1.2 The data The California Standardized Testing and Reporting (STAR) dataset contains data on test performance, school characteristics and student demographic backgrounds. The data used here are from all 420 K-6 and K-8 districts in California with data available for 1998 and 1999. Test scores are the average of the reading and math scores on the Stanford 9 standardized test administered to 5th grade students. School characteristics (averaged across the district) include enrollment, number of teachers (measured as “full-time-equivalents”), number of computers per classroom, and expenditures per student. The studentteacher ratio used here is the number of full-time equivalent teachers in the district, divided by the number of students. Demographic variables for the students also are averaged across the district. The demographic variables include the percentage of students in the public assistance program CalWorks (formerly AFDC), the percentage of students that qualify for a reduced price lunch, and the percentage of students that are English Learners (that is, students for whom English is a second language). All of these data were obtained from the California Department of Education www.cde.ca.gov. Series in Data Set: DIST_CODE: District Code; READ_SCR: Average reading Score; MATH_SCR: Average math Score; COUNTY: County; DISTRICT: District; GR_SPAN: Grade Span of District; ENRL_TOT: Total enrollment; TEACHERS: Number of teachers; COMPUTER: Number of computers; TESTSCR: Average test Score (= (READ_SCR+MATH_SCR)/2 ); COMP_STU: Computer per student ( = COMPUTER/ENRL_TOT); EXPN_STU: Expenditures per student ($’S); STR: Student teacher ratio (ENRL_TOT/TEACHERS); EL_PCT: Percent of English learners; MEAL_PCT: Percent qualifying for reduced-price lunch; 1 CALW_PCT: Percent qualifying for CALWORKS; AVGINC: District average income (in $1000’S); library(readr) cas &lt;- data.frame(read_csv(&quot;caschool.csv&quot;)) ## Rows: 420 Columns: 18 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (3): county, district, gr_span ## dbl (15): Observation Number, dist_cod, enrl_tot, teachers, calw_pct, meal_p... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. str(cas) ## &#39;data.frame&#39;: 420 obs. of 18 variables: ## $ Observation.Number: num 1 2 3 4 5 6 7 8 9 10 ... ## $ dist_cod : num 75119 61499 61549 61457 61523 ... ## $ county : chr &quot;Alameda&quot; &quot;Butte&quot; &quot;Butte&quot; &quot;Butte&quot; ... ## $ district : chr &quot;Sunol Glen Unified&quot; &quot;Manzanita Elementary&quot; &quot;Thermalito Union Elementary&quot; &quot;Golden Feather Union Elementary&quot; ... ## $ gr_span : chr &quot;KK-08&quot; &quot;KK-08&quot; &quot;KK-08&quot; &quot;KK-08&quot; ... ## $ enrl_tot : num 195 240 1550 243 1335 ... ## $ teachers : num 10.9 11.1 82.9 14 71.5 ... ## $ calw_pct : num 0.51 15.42 55.03 36.48 33.11 ... ## $ meal_pct : num 2.04 47.92 76.32 77.05 78.43 ... ## $ computer : num 67 101 169 85 171 25 28 66 35 0 ... ## $ testscr : num 691 661 644 648 641 ... ## $ comp_stu : num 0.344 0.421 0.109 0.35 0.128 ... ## $ expn_stu : num 6385 5099 5502 7102 5236 ... ## $ str : num 17.9 21.5 18.7 17.4 18.7 ... ## $ avginc : num 22.69 9.82 8.98 8.98 9.08 ... ## $ el_pct : num 0 4.58 30 0 13.86 ... ## $ read_scr : num 692 660 636 652 642 ... ## $ math_scr : num 690 662 651 644 640 ... So first we need to Drop some collums out of Principle. read_scr and math_scr create testscr. cas &lt;- cas[, !names(cas) %in% c(&quot;read_scr&quot;, &quot;math_scr&quot;)] Now District and Country are very specific so they will have a large number of levels. Levels for county table(cas$county) ## ## Alameda Butte Calaveras Contra Costa El Dorado ## 1 6 1 7 10 ## Fresno Glenn Humboldt Imperial Inyo ## 12 3 17 6 1 ## Kern Kings Lake Lassen Los Angeles ## 27 9 2 5 27 ## Madera Marin Mendocino Merced Monterey ## 5 8 1 11 7 ## Nevada Orange Placer Riverside Sacramento ## 9 11 11 4 7 ## San Benito San Bernardino San Diego San Joaquin San Luis Obispo ## 3 10 21 6 2 ## San Mateo Santa Barbara Santa Clara Santa Cruz Shasta ## 17 11 20 7 13 ## Siskiyou Sonoma Stanislaus Sutter Tehama ## 9 29 7 6 8 ## Trinity Tulare Tuolumne Ventura Yuba ## 2 24 6 9 2 For district almost every every observation has its own level. We will group the counties These groupings come from https://www.calbhbc.org/region-map-and-listing.html # Define the regions as vectors of county names according to your specification superior &lt;- c(&quot;Butte&quot;, &quot;Colusa&quot;, &quot;Del Norte&quot;, &quot;Glenn&quot;, &quot;Humboldt&quot;, &quot;Lake&quot;, &quot;Lassen&quot;, &quot;Mendocino&quot;, &quot;Modoc&quot;, &quot;Nevada&quot;, &quot;Plumas&quot;, &quot;Shasta&quot;, &quot;Sierra&quot;, &quot;Siskiyou&quot;, &quot;Tehama&quot;, &quot;Trinity&quot;) central &lt;- c(&quot;Alpine&quot;, &quot;Amador&quot;, &quot;Calaveras&quot;, &quot;El Dorado&quot;, &quot;Fresno&quot;, &quot;Inyo&quot;, &quot;Kings&quot;, &quot;Madera&quot;, &quot;Mariposa&quot;, &quot;Merced&quot;, &quot;Mono&quot;, &quot;Placer&quot;, &quot;Sacramento&quot;, &quot;San Joaquin&quot;, &quot;Stanislaus&quot;, &quot;Sutter&quot;, &quot;Yuba&quot;, &quot;Tulare&quot;, &quot;Tuolumne&quot;, &quot;Yolo&quot;) bay_area &lt;- c(&quot;Alameda&quot;, &quot;Contra Costa&quot;, &quot;Marin&quot;, &quot;Monterey&quot;, &quot;Napa&quot;, &quot;San Benito&quot;, &quot;San Francisco&quot;, &quot;San Mateo&quot;, &quot;Santa Clara&quot;, &quot;Santa Cruz&quot;, &quot;Solano&quot;, &quot;Sonoma&quot;, &quot;Berkeley&quot;) southern &lt;- c(&quot;Imperial&quot;, &quot;Kern&quot;, &quot;Orange&quot;, &quot;Riverside&quot;, &quot;San Bernardino&quot;, &quot;San Diego&quot;, &quot;San Luis Obispo&quot;, &quot;Santa Barbara&quot;, &quot;Ventura&quot;, &quot;Tri-City&quot;) los_angeles &lt;- c(&quot;Los Angeles&quot;) # Assuming &#39;cas&#39; is your dataframe with columns &#39;county&#39; for the county names # and &#39;observation&#39; for the counts # Create a new column for region based on the county cas$region &lt;- ifelse(cas$county %in% superior, &quot;Superior&quot;, ifelse(cas$county %in% central, &quot;Central&quot;, ifelse(cas$county %in% bay_area, &quot;Bay Area&quot;, ifelse(cas$county %in% southern, &quot;Southern&quot;, ifelse(cas$county %in% los_angeles, &quot;Los Angeles&quot;, &quot;Other&quot;))))) table(cas$region) ## ## Bay Area Central Los Angeles Southern Superior ## 99 118 27 101 75 So now we can try using this region variable. However we have other ways of measuring location so we must proceed with caution. Now we will drop district and county # using dplyr library(dplyr) cas &lt;- cas[, !names(cas) %in% c(&quot;district&quot;, &quot;county&quot;)] we can Also drop district code and observation number cas &lt;- cas[, !names(cas) %in% c(&quot;dist_cod&quot;, &quot;Observation.Number&quot;)] Now we need to one hot code the region and gr_span variables. # One-hot encoding using model.matrix cas &lt;- data.frame(cas, model.matrix(~ gr_span + region - 1, data = cas)) # now remove the original variables cas &lt;- cas[, !names(cas) %in% c(&quot;gr_span&quot;, &quot;region&quot;)] glimpse(cas) ## Rows: 420 ## Columns: 17 ## $ enrl_tot &lt;dbl&gt; 195, 240, 1550, 243, 1335, 137, 195, 888, 379, 2247,… ## $ teachers &lt;dbl&gt; 10.90, 11.15, 82.90, 14.00, 71.50, 6.40, 10.00, 42.5… ## $ calw_pct &lt;dbl&gt; 0.5102, 15.4167, 55.0323, 36.4754, 33.1086, 12.3188,… ## $ meal_pct &lt;dbl&gt; 2.0408, 47.9167, 76.3226, 77.0492, 78.4270, 86.9565,… ## $ computer &lt;dbl&gt; 67, 101, 169, 85, 171, 25, 28, 66, 35, 0, 86, 56, 25… ## $ testscr &lt;dbl&gt; 690.80, 661.20, 643.60, 647.70, 640.85, 605.55, 606.… ## $ comp_stu &lt;dbl&gt; 0.34358975, 0.42083332, 0.10903226, 0.34979424, 0.12… ## $ expn_stu &lt;dbl&gt; 6384.911, 5099.381, 5501.955, 7101.831, 5235.988, 55… ## $ str &lt;dbl&gt; 17.88991, 21.52466, 18.69723, 17.35714, 18.67133, 21… ## $ avginc &lt;dbl&gt; 22.690001, 9.824000, 8.978000, 8.978000, 9.080333, 1… ## $ el_pct &lt;dbl&gt; 0.000000, 4.583333, 30.000002, 0.000000, 13.857677, … ## $ gr_spanKK.06 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gr_spanKK.08 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1… ## $ regionCentral &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0… ## $ regionLos.Angeles &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ regionSouthern &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1… ## $ regionSuperior &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Now we can move into the theory of what SHOULD be estimators of test scores. 9.1.2 Theory While i will not be going deep at all in this section i want to brush on some of the main points. Theoretically what should influence test scores? How hard the student works. If a student is the type of person to study many hours in preperation for tests we would expect this student to do much better then other students. as a note this could be increased by someones previous grades. we should look at that bias. How smart the student is. This is speaking on that if a student is naturally gifted in a subject we would expect them to do better then other students. Capital invested into a student. If a student is being given more resources then other students we would expect them to do better. i.e. if i have a computer that will help me study. The school environment of the student. If a student is in a bad school environment we would expect them to do worse then other students. i.e. if you don’t have other students also striving to achieve good grades you may not strive to achieve those grades either. The child’s family environment. If a child is in a bad family environment we would expect them to do worse then other students. i.e. if a child is in a family that does not value education they may not value education either. The native language skills of the child. If a child is not a native english speaker we would expect them to do worse then other students. 9.1.3 Building The models Now we will build the models. First we must decide if we want to build the model on a per/student basis or a totals basis. For simplicity we will build the model on a per/student basis as it would help capture some of the relationships better. So we can drop the items that are totals cas &lt;- cas[, !names(cas) %in% c(&quot;enrl_tot&quot;, &quot;teachers&quot;, &quot;computer&quot;)] So lets build our first unrestricted model UR &lt;- lm(testscr ~ ., data = cas) summary(UR) ## ## Call: ## lm(formula = testscr ~ ., data = cas) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.9628 -4.9063 -0.2445 4.8772 26.7645 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.525e+02 9.208e+00 70.860 &lt; 2e-16 *** ## calw_pct -1.181e-01 5.666e-02 -2.084 0.03779 * ## meal_pct -3.716e-01 3.587e-02 -10.358 &lt; 2e-16 *** ## comp_stu 1.130e+01 6.710e+00 1.683 0.09308 . ## expn_stu 1.157e-03 8.869e-04 1.305 0.19265 ## str 3.282e-02 2.880e-01 0.114 0.90934 ## avginc 7.369e-01 9.138e-02 8.064 8.34e-15 *** ## el_pct -1.526e-01 3.606e-02 -4.233 2.85e-05 *** ## gr_spanKK.06 3.926e+00 1.195e+00 3.284 0.00111 ** ## gr_spanKK.08 NA NA NA NA ## regionCentral 1.788e+00 1.400e+00 1.277 0.20223 ## regionLos.Angeles 2.825e-01 1.924e+00 0.147 0.88335 ## regionSouthern 1.373e+00 1.293e+00 1.062 0.28885 ## regionSuperior 6.809e+00 1.640e+00 4.152 4.02e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.147 on 407 degrees of freedom ## Multiple R-squared: 0.8224, Adjusted R-squared: 0.8172 ## F-statistic: 157.1 on 12 and 407 DF, p-value: &lt; 2.2e-16 We can also do this with linear algebra.I will be using function but i wanted to put it here to show the process and how you have to change the data when using linear algebra. casl &lt;- cas #We must satesfy the full rank condition casl$regionSuperior &lt;- NULL casl$gr_spanKK.06 &lt;- NULL X &lt;- as.matrix(casl[, names(casl) != &quot;testscr&quot;]) X &lt;- cbind(1, X) Y &lt;- cas$testscr beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y beta_hat ## [,1] ## 6.647366e+02 ## calw_pct -9.086445e-02 ## meal_pct -3.534084e-01 ## comp_stu 1.208425e+01 ## expn_stu 9.328987e-04 ## str -1.204137e-01 ## avginc 6.121253e-01 ## el_pct -2.136800e-01 ## gr_spanKK.08 -3.309759e+00 ## regionCentral -1.965127e+00 ## regionLos.Angeles -2.363906e+00 ## regionSouthern -1.180958e+00 Now we will apply our theory and use proxies to create a more restricted mode. 9.1.4 Proofs 9.1.5 Tests 9.1.6 Significances and Tests 9.1.7 Conclusion 9.2 Auto Correlation "],["parrallel-processing.html", "Chapter 10 Parrallel Processing 10.1 1. parallel with parLapply 10.2 2. foreach with doParallel 10.3 3. future with future.apply 10.4 4. purrr with furrr 10.5 5. purrr Synchronously 10.6 comparisions", " Chapter 10 Parrallel Processing compute_means &lt;- function(n) { replicate(n, mean(runif(1e7))) } 10.1 1. parallel with parLapply library(parallel) # Create a cluster cl &lt;- makeCluster(detectCores() - 1) clusterExport(cl, varlist = &quot;compute_means&quot;) start_time &lt;- Sys.time() results &lt;- parLapply(cl, 1:100, function(x) compute_means(1)) end_time &lt;- Sys.time() stopCluster(cl) time_parallel &lt;- end_time - start_time 10.2 2. foreach with doParallel i think you need to export packages in this one library(foreach) library(doParallel) cl &lt;- makeCluster(detectCores() - 1) registerDoParallel(cl) start_time &lt;- Sys.time() results &lt;- foreach(i = 1:100, .combine = &#39;c&#39;) %dopar% { compute_means(1) } end_time &lt;- Sys.time() stopCluster(cl) time_foreach &lt;- end_time - start_time 10.3 3. future with future.apply library(future) ## ## Attaching package: &#39;future&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## cluster library(future.apply) plan(multisession, workers = detectCores() - 1) options(future.rng.onMisuse = &quot;ignore&quot;, future.seed = TRUE) # for the random number generating start_time &lt;- Sys.time() results &lt;- future_lapply(1:100, function(x) compute_means(1)) end_time &lt;- Sys.time() time_future &lt;- end_time - start_time 10.4 4. purrr with furrr library(furrr) library(future) plan(multisession, workers = detectCores() - 1) options(future.rng.onMisuse = &quot;ignore&quot;, future.seed = TRUE) # for the random number generating start_time &lt;- Sys.time() results &lt;- future_map_dbl(1:100, ~ compute_means(1)) end_time &lt;- Sys.time() time_furrr &lt;- end_time - start_time 10.5 5. purrr Synchronously library(purrr) start_time &lt;- Sys.time() results &lt;- map_dbl(1:100, ~ compute_means(1)) end_time &lt;- Sys.time() time_purrr &lt;- end_time - start_time 10.6 comparisions with no PP start_time &lt;- Sys.time() compute_means(1) ## [1] 0.4999286 end_time &lt;- Sys.time() No_parrellel_time &lt;- end_time - start_time Obviously we see the results of overhead print(paste(&quot;Time using parallel: &quot;, time_parallel)) ## [1] &quot;Time using parallel: 2.00679397583008&quot; print(paste(&quot;Time using foreach/doParallel: &quot;, time_foreach)) ## [1] &quot;Time using foreach/doParallel: 2.04868912696838&quot; print(paste(&quot;Time using future/future.apply: &quot;, time_future)) ## [1] &quot;Time using future/future.apply: 17.2621829509735&quot; print(paste(&quot;Time using furrr/future: &quot;, time_furrr)) ## [1] &quot;Time using furrr/future: 19.8600029945374&quot; print(paste(&quot;Time using purrr: &quot;, time_purrr)) ## [1] &quot;Time using purrr: 10.2333540916443&quot; print(paste(&quot;Time using NormalFunction: &quot;, No_parrellel_time)) ## [1] &quot;Time using NormalFunction: 0.0972850322723389&quot; "],["randomforestinsight.html", "Chapter 11 10_RandomForestInsight", " Chapter 11 10_RandomForestInsight # Load necessary libraries library(titanic) ## Warning: package &#39;titanic&#39; was built under R version 4.3.3 library(dplyr) library(randomForest) library(randomForestExplainer) ## Warning: package &#39;randomForestExplainer&#39; was built under R version 4.3.3 this is the basic set up of some data # Load necessary libraries library(titanic) # Load the Titanic dataset data(&quot;titanic_train&quot;) # Create copy for cleaning data_clean &lt;- titanic_train # Handle missing values data_clean$Age[is.na(data_clean$Age)] &lt;- median(data_clean$Age, na.rm = TRUE) data_clean$Embarked[is.na(data_clean$Embarked) | data_clean$Embarked == &quot;&quot;] &lt;- &quot;S&quot; data_clean$Fare[is.na(data_clean$Fare)] &lt;- median(data_clean$Fare, na.rm = TRUE) # Extract title and create binary indicators data_clean$Title &lt;- ifelse( grepl(&quot;, Mr\\\\.&quot;, data_clean$Name), &quot;Mr&quot;, ifelse(grepl(&quot;, Mrs\\\\.&quot;, data_clean$Name), &quot;Mrs&quot;, ifelse(grepl(&quot;, Miss\\\\.&quot;, data_clean$Name), &quot;Miss&quot;, ifelse(grepl(&quot;, Master\\\\.&quot;, data_clean$Name), &quot;Master&quot;, &quot;Other&quot;))) ) # Create binary title indicators data_clean$Title_Mr &lt;- as.factor(ifelse(data_clean$Title == &quot;Mr&quot;, 1, 0)) data_clean$Title_Mrs &lt;- as.factor(ifelse(data_clean$Title == &quot;Mrs&quot;, 1, 0)) data_clean$Title_Miss &lt;- as.factor(ifelse(data_clean$Title == &quot;Miss&quot;, 1, 0)) data_clean$Title_Master &lt;- as.factor(ifelse(data_clean$Title == &quot;Master&quot;, 1, 0)) data_clean$Title_Other &lt;- as.factor(ifelse(data_clean$Title == &quot;Other&quot;, 1, 0)) # Remove unnecessary columns cols_to_keep &lt;- !(names(data_clean) %in% c(&quot;Cabin&quot;, &quot;Ticket&quot;, &quot;Name&quot;, &quot;Title&quot;)) data_clean &lt;- data_clean[, cols_to_keep] # Convert to factors data_clean$Survived &lt;- as.factor(data_clean$Survived) data_clean$Pclass &lt;- as.factor(data_clean$Pclass) data_clean$Sex &lt;- as.factor(data_clean$Sex) data_clean$Embarked &lt;- as.factor(data_clean$Embarked) # Create FamilySize data_clean$FamilySize &lt;- data_clean$SibSp + data_clean$Parch + 1 # Remove first column data_clean &lt;- data_clean[, -1] # Verify the structure and summary of the cleaned dataset str(data_clean) ## &#39;data.frame&#39;: 891 obs. of 14 variables: ## $ Survived : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 1 1 1 2 2 ... ## $ Pclass : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 1 3 1 3 3 1 3 3 2 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ... ## $ Age : num 22 38 26 35 35 28 54 2 27 14 ... ## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ... ## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ... ## $ Embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ... ## $ Title_Mr : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 2 2 2 1 1 1 ... ## $ Title_Mrs : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 2 1 1 1 1 2 2 ... ## $ Title_Miss : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 2 1 1 1 1 1 1 1 ... ## $ Title_Master: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 2 1 1 ... ## $ Title_Other : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ FamilySize : num 2 2 1 2 1 1 1 5 3 2 ... summary(data_clean) ## Survived Pclass Sex Age SibSp Parch ## 0:549 1:216 female:314 Min. : 0.42 Min. :0.000 Min. :0.0000 ## 1:342 2:184 male :577 1st Qu.:22.00 1st Qu.:0.000 1st Qu.:0.0000 ## 3:491 Median :28.00 Median :0.000 Median :0.0000 ## Mean :29.36 Mean :0.523 Mean :0.3816 ## 3rd Qu.:35.00 3rd Qu.:1.000 3rd Qu.:0.0000 ## Max. :80.00 Max. :8.000 Max. :6.0000 ## Fare Embarked Title_Mr Title_Mrs Title_Miss Title_Master ## Min. : 0.00 C:168 0:374 0:766 0:709 0:851 ## 1st Qu.: 7.91 Q: 77 1:517 1:125 1:182 1: 40 ## Median : 14.45 S:646 ## Mean : 32.20 ## 3rd Qu.: 31.00 ## Max. :512.33 ## Title_Other FamilySize ## 0:864 Min. : 1.000 ## 1: 27 1st Qu.: 1.000 ## Median : 1.000 ## Mean : 1.905 ## 3rd Qu.: 2.000 ## Max. :11.000 Now we will use randomforest to predict the survival of the passengers and use random forest explainer to understand it more This is out titanic data that was cleaned up str(data) ## tibble [10,000 × 7] (S3: tbl_df/tbl/data.frame) ## $ y : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ type : Factor w/ 3 levels &quot;H&quot;,&quot;L&quot;,&quot;M&quot;: 3 2 2 2 2 3 2 2 3 3 ... ## $ air_temperature_k : num [1:10000] 298 298 298 298 298 ... ## $ process_temperature_k: num [1:10000] 309 309 308 309 309 ... ## $ rotational_speed_rpm : num [1:10000] 1551 1408 1498 1433 1408 ... ## $ torque_nm : num [1:10000] 42.8 46.3 49.4 39.5 40 41.9 42.4 40.2 28.6 28 ... ## $ tool_wear_min : num [1:10000] 0 3 5 7 9 11 14 16 18 21 ... MDI MDA PDPs "],["sequential-data-embedding.html", "Chapter 12 Sequential Data Embedding", " Chapter 12 Sequential Data Embedding With time series data we cannot shuffle the data due to the temporal nature of the data. This means we cannot shuffle the data. While models like ARIMA do exist they are not strong enough for forcasting. we want to get our data to the point where we can use our machine learning models on it. first lets make a toy dataset to work with. it will have a y and one X Since we set rho to be less then 1 our fake data is stationary. If rho is = 1 then we have non-stationary data and must decompose it to get to this position. You can take the first differance in that case. # Stationary data rho &lt; 1 n &lt;- 100 rho_y &lt;- 0.80 rho_x &lt;- 0.75 # A different rho for X y &lt;- numeric(n) x &lt;- numeric(n) e_y &lt;- rnorm(n, 0, 1) # White noise for y e_x &lt;- rnorm(n, 0, 1) # White noise for X # Generate y and X as separate autoregressive processes for (j in 1:(n - 1)) { y[j + 1] &lt;- y[j] * rho_y + e_y[j] x[j + 1] &lt;- x[j] * rho_x + e_x[j] } ylagged &lt;- y[2:n] xlagged &lt;- x[2:n] # y over time plot(y[1:n], type = &quot;l&quot;, col = &quot;red&quot;, ylab = &quot;y&quot;, xlab = &quot;t&quot;, main = &quot;y over time&quot;) # x over time plot(x[1:n], type = &quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;x&quot;, xlab = &quot;t&quot;, main = &quot;x over time&quot;) So we begin by predicting y + 1 but we must first prove we can embed and shuffle the data we will use 2 points back of y and three points back of x to predict the next point of y. y2 &lt;- 1:10 x2 &lt;- 11:20 # Create the embedded matrix for Y and X y_emb &lt;- embed(y2, 4) x_emb &lt;- embed(x2, 4) colnames(y_emb) &lt;- c(&quot;Y(t)&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;, &quot;Y(t-3)&quot;) colnames(x_emb) &lt;- c(&quot;X(t)&quot;, &quot;X(t-1)&quot;, &quot;X(t-2)&quot;, &quot;X(t-3)&quot;) fd &lt;- cbind(y_emb, x_emb) head(fd) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 4 3 2 1 14 13 12 11 ## [2,] 5 4 3 2 15 14 13 12 ## [3,] 6 5 4 3 16 15 14 13 ## [4,] 7 6 5 4 17 16 15 14 ## [5,] 8 7 6 5 18 17 16 15 ## [6,] 9 8 7 6 19 18 17 16 now for each point we are predicting(y + 1), (y+2), and (y+3) we need to edit the data for predicting point y(t+1) the formula is \\[ \\hat{y}(t+1) = f\\left( y(t), y(t-1), y(t-2), x(t), x(t-1), x(t-2), x(t-3) \\right) \\] and the data is head(fd) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 4 3 2 1 14 13 12 11 ## [2,] 5 4 3 2 15 14 13 12 ## [3,] 6 5 4 3 16 15 14 13 ## [4,] 7 6 5 4 17 16 15 14 ## [5,] 8 7 6 5 18 17 16 15 ## [6,] 9 8 7 6 19 18 17 16 in this case row seven is y where t = 10… 10(t) - 3(lags) = 7(rows) for the seccond point we are predicting y(t+2) the formula is \\[ \\hat{y}(t+2) = f\\left( y(t), y(t-1), y(t-2), x(t), x(t-1), x(t-2), x(t-3) \\right) \\] and the data is # Shift the first column (Y(t)) up by one position sd &lt;- fd[-nrow(fd), ] # Remove the last row sd[, 1] &lt;- fd[-1, 1] # Shift the first column up by one # Display the result to verify head(sd) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 5 3 2 1 14 13 12 11 ## [2,] 6 4 3 2 15 14 13 12 ## [3,] 7 5 4 3 16 15 14 13 ## [4,] 8 6 5 4 17 16 15 14 ## [5,] 9 7 6 5 18 17 16 15 ## [6,] 10 8 7 6 19 18 17 16 \\[ \\hat{y}(t+3) = f\\left( y(t), y(t-1), y(t-2), x(t), x(t-1), x(t-2), x(t-3) \\right) \\] # Shift the first column (Y(t+2)) up by one position to prepare for y(t+3) td &lt;- sd[-nrow(sd), ] # Remove the last row td[, 1] &lt;- sd[-1, 1] # Shift the first column up by one # Display the result to verify head(td) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 6 3 2 1 14 13 12 11 ## [2,] 7 4 3 2 15 14 13 12 ## [3,] 8 5 4 3 16 15 14 13 ## [4,] 9 6 5 4 17 16 15 14 ## [5,] 10 7 6 5 18 17 16 15 now we will aplly this to our data and prove that we can shuffle the data y_emb &lt;- embed(y, 4) x_emb &lt;- embed(x, 4) colnames(y_emb) &lt;- c(&quot;Y(t)&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;, &quot;Y(t-3)&quot;) colnames(x_emb) &lt;- c(&quot;X(t)&quot;, &quot;X(t-1)&quot;, &quot;X(t-2)&quot;, &quot;X(t-3)&quot;) They each lose 3 rows at the end of the data set. now get the data set for predicting y(t+1) same as before now y(t+2) and y(t+3) # First, embed the original data for y and x y_emb &lt;- embed(y, 4) x_emb &lt;- embed(x, 4) colnames(y_emb) &lt;- c(&quot;Yt&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;, &quot;Y(t-3)&quot;) colnames(x_emb) &lt;- c(&quot;X(t)&quot;, &quot;X(t-1)&quot;, &quot;X(t-2)&quot;, &quot;X(t-3)&quot;) fd &lt;- cbind(y_emb, x_emb) # Shift the first column (Y(t)) up by one position for y(t+2) sd &lt;- fd[-nrow(fd), ] # Remove the last row sd[, 1] &lt;- fd[-1, 1] # Shift the first column up by one # Display the resulting dataset head(sd) ## Yt Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) ## [1,] 1.929294 0.5787050 1.7820067 0.0000000 0.1548195 1.5972208 0.2145459 ## [2,] 4.146824 0.8104935 0.5787050 1.7820067 0.9563507 0.1548195 1.5972208 ## [3,] 3.861559 1.9292935 0.8104935 0.5787050 -0.4656051 0.9563507 0.1548195 ## [4,] 3.775874 4.1468239 1.9292935 0.8104935 0.2394009 -0.4656051 0.9563507 ## [5,] 3.837588 3.8615592 4.1468239 1.9292935 1.4626364 0.2394009 -0.4656051 ## [6,] 3.010381 3.7758736 3.8615592 4.1468239 -0.8892229 1.4626364 0.2394009 ## X(t-3) ## [1,] 0.0000000 ## [2,] 0.2145459 ## [3,] 1.5972208 ## [4,] 0.1548195 ## [5,] 0.9563507 ## [6,] -0.4656051 # Shift the first column (Y(t+2)) up by one position to prepare for y(t+3) td &lt;- sd[-nrow(sd), ] # Remove the last row td[, 1] &lt;- sd[-1, 1] # Shift the first column up by one # Display the resulting dataset head(td) ## Yt Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) ## [1,] 4.146824 0.5787050 1.7820067 0.0000000 0.1548195 1.5972208 0.2145459 ## [2,] 3.861559 0.8104935 0.5787050 1.7820067 0.9563507 0.1548195 1.5972208 ## [3,] 3.775874 1.9292935 0.8104935 0.5787050 -0.4656051 0.9563507 0.1548195 ## [4,] 3.837588 4.1468239 1.9292935 0.8104935 0.2394009 -0.4656051 0.9563507 ## [5,] 3.010381 3.8615592 4.1468239 1.9292935 1.4626364 0.2394009 -0.4656051 ## [6,] 2.971880 3.7758736 3.8615592 4.1468239 -0.8892229 1.4626364 0.2394009 ## X(t-3) ## [1,] 0.0000000 ## [2,] 0.2145459 ## [3,] 1.5972208 ## [4,] 0.1548195 ## [5,] 0.9563507 ## [6,] -0.4656051 apply lm to each of the data sets then shuffle and check the coeficients # Fit a linear model to predict Y(t+1) fd &lt;- as.data.frame(fd) sd &lt;- as.data.frame(sd) td &lt;- as.data.frame(td) lm1 &lt;- lm(Yt ~ ., data = fd) lm2 &lt;- lm(Yt ~ ., data = sd) lm3 &lt;- lm(Yt ~ ., data = td) summary(lm1) ## ## Call: ## lm(formula = Yt ~ ., data = fd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.92947 -0.51938 0.00807 0.72227 2.36213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.09117 0.10707 0.851 0.397 ## `Y(t-1)` 0.85598 0.10690 8.007 4.21e-12 *** ## `Y(t-2)` -0.09809 0.14038 -0.699 0.487 ## `Y(t-3)` 0.07958 0.10640 0.748 0.456 ## `X(t)` -0.14637 0.11209 -1.306 0.195 ## `X(t-1)` 0.19891 0.13247 1.502 0.137 ## `X(t-2)` -0.01600 0.13123 -0.122 0.903 ## `X(t-3)` 0.04416 0.11058 0.399 0.691 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.998 on 89 degrees of freedom ## Multiple R-squared: 0.6996, Adjusted R-squared: 0.676 ## F-statistic: 29.62 on 7 and 89 DF, p-value: &lt; 2.2e-16 summary(lm2) ## ## Call: ## lm(formula = Yt ~ ., data = sd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27684 -0.95502 0.01087 0.84341 3.04907 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.146103 0.138208 1.057 0.2933 ## `Y(t-1)` 0.643308 0.138307 4.651 1.15e-05 *** ## `Y(t-2)` -0.133187 0.180526 -0.738 0.4626 ## `Y(t-3)` 0.233729 0.137782 1.696 0.0934 . ## `X(t)` -0.006291 0.144171 -0.044 0.9653 ## `X(t-1)` 0.195887 0.170375 1.150 0.2534 ## `X(t-2)` 0.099337 0.169022 0.588 0.5582 ## `X(t-3)` -0.144014 0.142646 -1.010 0.3155 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.283 on 88 degrees of freedom ## Multiple R-squared: 0.509, Adjusted R-squared: 0.4699 ## F-statistic: 13.03 on 7 and 88 DF, p-value: 2.128e-11 summary(lm3) ## ## Call: ## lm(formula = Yt ~ ., data = td) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8378 -0.8876 -0.1540 0.7352 3.6533 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.17271 0.15236 1.134 0.2601 ## `Y(t-1)` 0.38835 0.15293 2.539 0.0129 * ## `Y(t-2)` 0.10300 0.19877 0.518 0.6057 ## `Y(t-3)` 0.18286 0.15115 1.210 0.2296 ## `X(t)` 0.10152 0.15843 0.641 0.5233 ## `X(t-1)` 0.26238 0.18708 1.402 0.1643 ## `X(t-2)` -0.02088 0.18596 -0.112 0.9109 ## `X(t-3)` -0.23294 0.15649 -1.489 0.1402 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.408 on 87 degrees of freedom ## Multiple R-squared: 0.412, Adjusted R-squared: 0.3647 ## F-statistic: 8.709 on 7 and 87 DF, p-value: 4.541e-08 # Shuffle the data fd &lt;- fd[sample(nrow(fd)), ] sd &lt;- sd[sample(nrow(sd)), ] td &lt;- td[sample(nrow(td)), ] lm1 &lt;- lm(Yt ~ ., data = fd) lm2 &lt;- lm(Yt ~ ., data = sd) lm3 &lt;- lm(Yt ~ ., data = td) summary(lm1) ## ## Call: ## lm(formula = Yt ~ ., data = fd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.92947 -0.51938 0.00807 0.72227 2.36213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.09117 0.10707 0.851 0.397 ## `Y(t-1)` 0.85598 0.10690 8.007 4.21e-12 *** ## `Y(t-2)` -0.09809 0.14038 -0.699 0.487 ## `Y(t-3)` 0.07958 0.10640 0.748 0.456 ## `X(t)` -0.14637 0.11209 -1.306 0.195 ## `X(t-1)` 0.19891 0.13247 1.502 0.137 ## `X(t-2)` -0.01600 0.13123 -0.122 0.903 ## `X(t-3)` 0.04416 0.11058 0.399 0.691 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.998 on 89 degrees of freedom ## Multiple R-squared: 0.6996, Adjusted R-squared: 0.676 ## F-statistic: 29.62 on 7 and 89 DF, p-value: &lt; 2.2e-16 summary(lm2) ## ## Call: ## lm(formula = Yt ~ ., data = sd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.27684 -0.95502 0.01087 0.84341 3.04907 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.146103 0.138208 1.057 0.2933 ## `Y(t-1)` 0.643308 0.138307 4.651 1.15e-05 *** ## `Y(t-2)` -0.133187 0.180526 -0.738 0.4626 ## `Y(t-3)` 0.233729 0.137782 1.696 0.0934 . ## `X(t)` -0.006291 0.144171 -0.044 0.9653 ## `X(t-1)` 0.195887 0.170375 1.150 0.2534 ## `X(t-2)` 0.099337 0.169022 0.588 0.5582 ## `X(t-3)` -0.144014 0.142646 -1.010 0.3155 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.283 on 88 degrees of freedom ## Multiple R-squared: 0.509, Adjusted R-squared: 0.4699 ## F-statistic: 13.03 on 7 and 88 DF, p-value: 2.128e-11 summary(lm3) ## ## Call: ## lm(formula = Yt ~ ., data = td) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8378 -0.8876 -0.1540 0.7352 3.6533 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.17271 0.15236 1.134 0.2601 ## `Y(t-1)` 0.38835 0.15293 2.539 0.0129 * ## `Y(t-2)` 0.10300 0.19877 0.518 0.6057 ## `Y(t-3)` 0.18286 0.15115 1.210 0.2296 ## `X(t)` 0.10152 0.15843 0.641 0.5233 ## `X(t-1)` 0.26238 0.18708 1.402 0.1643 ## `X(t-2)` -0.02088 0.18596 -0.112 0.9109 ## `X(t-3)` -0.23294 0.15649 -1.489 0.1402 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.408 on 87 degrees of freedom ## Multiple R-squared: 0.412, Adjusted R-squared: 0.3647 ## F-statistic: 8.709 on 7 and 87 DF, p-value: 4.541e-08 Now at this point we can use any machine learning model we want to predict the next three points in the time series. "],["hcc_survey_analysis.html", "Chapter 13 HCC_Survey_Analysis 13.1 Our data and Purpose 13.2 Exploratory Data Analysis 13.3 Modeling Random Forest 13.4 Global Importance 13.5 Local Importance 13.6 Partial Dependence Plots 13.7 Random Forest Explainer 13.8 Final Notes", " Chapter 13 HCC_Survey_Analysis 13.1 Our data and Purpose Load our libraries that we will use library(readr) library(dplyr) library(DataExplorer) library(tidyr) library(ggplot2) library(caret) library(randomForest) library(randomForestExplainer) library(pdp) ## Warning: package &#39;pdp&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;pdp&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## partial # Read and clean column names HCC_Survey &lt;- read_csv(&quot;HCC_Survey.csv&quot;) ## Rows: 46 Columns: 83 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (8): Time, Location, Campus, Involvment, Faith, Formation_Helpful, Eff_... ## dbl (75): cg_SMU, cg_Summit, cg_Other, cg_Dal, cg_Social, cg_Parish, cg_MW, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. names(HCC_Survey) &lt;- gsub(&quot; &quot;, &quot;_&quot;, names(HCC_Survey)) names(HCC_Survey) &lt;- gsub(&quot;&#39;&quot;, &quot;&quot;, names(HCC_Survey)) names(HCC_Survey) &lt;- gsub(&quot;[()]&quot;, &quot;&quot;, names(HCC_Survey)) names(HCC_Survey) &lt;- make.names(names(HCC_Survey), unique = TRUE) # Select initial columns cols_to_keep &lt;- c(&quot;Involvment&quot;, &quot;Relations&quot;, grep(&quot;^cg_|^dsc_|^frm_|^srv_&quot;, names(HCC_Survey), value = TRUE)) HCC_Survey &lt;- HCC_Survey[, cols_to_keep] # Create Total_cgs cg_cols &lt;- grep(&quot;^cg_&quot;, names(HCC_Survey), value = TRUE) HCC_Survey$Total_cgs &lt;- rowSums(HCC_Survey[, cg_cols], na.rm = TRUE) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% cg_cols] # Create CCO cco_cols &lt;- grep(&quot;^frm_CCO&quot;, names(HCC_Survey), value = TRUE) HCC_Survey$CCO &lt;- ifelse(rowSums(HCC_Survey[, cco_cols], na.rm = TRUE) &gt; 0, 1, 0) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% cco_cols] # Create srv_CCO srv_cco_cols &lt;- c(&quot;srv_Leading_a_faith_study&quot;, &quot;srv_CCO_Exec&quot;, &quot;srv_CCO_Events&quot;) HCC_Survey$srv_CCO &lt;- ifelse(rowSums(HCC_Survey[, srv_cco_cols], na.rm = TRUE) &gt; 0, 1, 0) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% srv_cco_cols] # Remove specified columns and rename cols_to_remove &lt;- c(&quot;dsc_Other&quot;, &quot;srv_Other_please_indicate&quot;, &quot;srv_I_didnt_serve_in_ministry&quot;) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% cols_to_remove] names(HCC_Survey)[names(HCC_Survey) == &quot;srv_HCC_Volunteering_ex._CLT&quot;] &lt;- &quot;srv_HCC&quot; # Create Total_dsc and Total_frm dsc_cols &lt;- grep(&quot;^dsc_&quot;, names(HCC_Survey), value = TRUE) frm_cols &lt;- grep(&quot;^frm_&quot;, names(HCC_Survey), value = TRUE) data &lt;- HCC_Survey data$Total_dsc &lt;- rowSums(data[, dsc_cols], na.rm = TRUE) data$Total_frm &lt;- rowSums(data[, frm_cols], na.rm = TRUE) data &lt;- data[, !names(data) %in% c(dsc_cols, frm_cols)] # Convert factors and combine Impressions into Casual data$Involvment &lt;- factor(replace(data$Involvment, data$Involvment == &quot;Impressions&quot;, &quot;Casual&quot;)) data$Relations &lt;- factor(data$Relations) # Sort columns alphabetically data &lt;- data[, sort(names(data))] # Display structure and create data_rf glimpse(data) ## Rows: 46 ## Columns: 9 ## $ CCO &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,… ## $ Involvment &lt;fct&gt; Casual, Casual, Casual, Casual, Committed, Engaged… ## $ Relations &lt;fct&gt; 3 to 6, 3 to 6, 3 to 6, 3 to 6, 3 to 6, More than … ## $ srv_CCO &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,… ## $ srv_HCC &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ srv_Parish_ministry &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,… ## $ Total_cgs &lt;dbl&gt; 3, 3, 4, 4, 6, 5, 4, 4, 1, 1, 3, 3, 4, 4, 5, 4, 6,… ## $ Total_dsc &lt;dbl&gt; 7, 3, 9, 5, 9, 5, 4, 9, 5, 10, 10, 6, 4, 6, 10, 10… ## $ Total_frm &lt;dbl&gt; 4, 1, 0, 0, 3, 1, 3, 4, 0, 6, 3, 1, 2, 2, 2, 3, 6,… data_rf &lt;- data write the data then re read it write.csv(data, &quot;proccessed_surveyHCC.csv&quot;) The following data has been already pre-proccessed data &lt;- read.csv(&quot;proccessed_surveyHCC.csv&quot;) glimpse(data) ## Rows: 46 ## Columns: 10 ## $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ CCO &lt;int&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,… ## $ Involvment &lt;chr&gt; &quot;Casual&quot;, &quot;Casual&quot;, &quot;Casual&quot;, &quot;Casual&quot;, &quot;Committed… ## $ Relations &lt;chr&gt; &quot;3 to 6&quot;, &quot;3 to 6&quot;, &quot;3 to 6&quot;, &quot;3 to 6&quot;, &quot;3 to 6&quot;, … ## $ srv_CCO &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,… ## $ srv_HCC &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ srv_Parish_ministry &lt;int&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,… ## $ Total_cgs &lt;int&gt; 3, 3, 4, 4, 6, 5, 4, 4, 1, 1, 3, 3, 4, 4, 5, 4, 6,… ## $ Total_dsc &lt;int&gt; 7, 3, 9, 5, 9, 5, 4, 9, 5, 10, 10, 6, 4, 6, 10, 10… ## $ Total_frm &lt;int&gt; 4, 1, 0, 0, 3, 1, 3, 4, 0, 6, 3, 1, 2, 2, 2, 3, 6,… CCO: whether someone has taken a CCO fairht study or not Involvment: this is categorical and represents the level of involvement of the person Relations: This counts the number of people a individual identifies as having a close relationship with currently srv_CCO: This is a binary variable that represents whether someone has served in a CCO ministry or not srv_HCC: This is a binary variable that represents whether someone has served in a HCC ministry or not srv_Parish_ministry: This is a binary variable that represents whether someone has served in a Parish ministry or not Total_cgs: This is the total number of community groups a person is apart of. Total_dsc: This is the total number of spiritual disciplines a person has in their life currently Total_frm: This is the total number of formations that a person has attended via HCC. This is our data we will use 13.2 Exploratory Data Analysis # First read the data and convert categorical variables to factors data &lt;- read.csv(&quot;proccessed_surveyHCC.csv&quot;, stringsAsFactors = FALSE) data$Involvment &lt;- as.factor(data$Involvment) data$Relations &lt;- as.factor(data$Relations) # Create the model matrix for factors only factor_cols &lt;- sapply(data, is.factor) factor_data &lt;- data[, factor_cols, drop = FALSE] dummy_vars &lt;- model.matrix(~., data = factor_data)[, -1] # Remove intercept # Combine the dummy variables with the non-factor columns numeric_cols &lt;- sapply(data, is.numeric) data_encoded &lt;- cbind( as.data.frame(dummy_vars), data[, numeric_cols, drop = FALSE] ) # Make sure Total_dsc is included if(!&quot;Total_dsc&quot; %in% names(data_encoded)) { data_encoded$Total_dsc &lt;- data$Total_dsc } # Calculate correlations cols_for_cor &lt;- setdiff(names(data_encoded), &quot;Total_dsc&quot;) correlations_list &lt;- sapply(data_encoded[cols_for_cor], function(x) { cor(x, data_encoded$Total_dsc, use = &quot;complete.obs&quot;) }) # Create correlation data frame correlations &lt;- data.frame( column = names(correlations_list), correlation = unlist(correlations_list) ) # Sort by absolute correlation correlations &lt;- correlations[order(abs(correlations$correlation), decreasing = TRUE), ] rownames(correlations) &lt;- NULL # If you want to keep the ggplot visualization: library(ggplot2) ggplot(correlations, aes(x = reorder(column, correlation), y = correlation)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + coord_flip() + labs(title = &quot;Correlation of All Columns with Total Disciplines&quot;, x = &quot;Column&quot;, y = &quot;Correlation&quot;) + theme_minimal() # Print correlations print(correlations) ## column correlation ## 1 Total_frm 0.62062501 ## 2 srv_Parish_ministry 0.59772804 ## 3 Total_cgs 0.47432114 ## 4 InvolvmentCommitted 0.44987501 ## 5 srv_HCC 0.44519710 ## 6 srv_CCO 0.41644453 ## 7 Relations7 to 10 0.24458238 ## 8 CCO 0.21960995 ## 9 Relations3 to 6 -0.16737169 ## 10 RelationsMore than 10 0.08832141 ## 11 X -0.05220109 ## 12 InvolvmentEngaged -0.01372312 13.3 Modeling Random Forest # Train the Random Forest model with 1500 trees rf_model &lt;- randomForest(Total_dsc ~ ., data = data_rf, ntree = 1500, importance = TRUE, localImp = TRUE) # Print the model summary print(rf_model) ## ## Call: ## randomForest(formula = Total_dsc ~ ., data = data_rf, ntree = 1500, importance = TRUE, localImp = TRUE) ## Type of random forest: regression ## Number of trees: 1500 ## No. of variables tried at each split: 2 ## ## Mean of squared residuals: 5.236783 ## % Var explained: 47.83 # Calculate the RMSE rmse &lt;- sqrt(mean((rf_model$predicted - data_rf$Total_dsc)^2)) rmse ## [1] 2.288402 # absolute error abs_error &lt;- mean(abs(rf_model$predicted - data_rf$Total_dsc)) abs_error ## [1] 1.842136 13.4 Global Importance Get MDI # Get the Mean Decrease in Impurity (MDI) mdi &lt;- importance(rf_model, type = 2) mdi ## IncNodePurity ## CCO 16.13296 ## Involvment 36.69981 ## Relations 34.75069 ## srv_CCO 28.76142 ## srv_HCC 24.06673 ## srv_Parish_ministry 71.74472 ## Total_cgs 52.24009 ## Total_frm 93.05213 get MDA # Get the Mean Decrease in Accuracy (MDA) mda &lt;- importance(rf_model, type = 1) mda ## %IncMSE ## CCO 8.0530544 ## Involvment 11.5370935 ## Relations 0.9051989 ## srv_CCO 10.2868360 ## srv_HCC 10.3221455 ## srv_Parish_ministry 25.0387096 ## Total_cgs 10.8179434 ## Total_frm 24.3258848 Plot Mda and mdi # Get MDI (Mean Decrease in Node Impurity) mdi &lt;- importance(rf_model, type = 2) # Get MDA (Mean Decrease in Accuracy) mda &lt;- importance(rf_model, type = 1) # Convert the importance values to a data frame for easier plotting importance_df &lt;- as.data.frame(mdi) importance_df$Feature &lt;- rownames(importance_df) importance_df$MDA &lt;- mda[, 1] # Rename the MDI column appropriately colnames(importance_df)[1] &lt;- &quot;IncNodePurity&quot; # Sort the data frames by MDA and MDI mda_df &lt;- importance_df %&gt;% arrange(desc(MDA)) mdi_df &lt;- importance_df %&gt;% arrange(desc(IncNodePurity)) # Plotting MDA ggplot(mda_df, aes(x = reorder(Feature, MDA), y = MDA)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;skyblue&quot;) + coord_flip() + labs(title = &quot;Feature Importance: Mean Decrease in Accuracy (MDA)&quot;, x = &quot;Feature&quot;, y = &quot;Mean Decrease in Accuracy&quot;) + theme_minimal() # Plotting MDI ggplot(mdi_df, aes(x = reorder(Feature, IncNodePurity), y = IncNodePurity)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;salmon&quot;) + coord_flip() + labs(title = &quot;Feature Importance: Mean Decrease in Node Impurity (MDI)&quot;, x = &quot;Feature&quot;, y = &quot;Mean Decrease in Node Impurity&quot;) + theme_minimal() MDA doesn’t rely on the internal structure of the model but rather on the model’s performance with altered data. MDI computes importance scores based on how much each feature contributes to homogeneity in nodes across all trees. For classification, this is often measured by the Gini impurity, and for regression, it can be measured by the variance reduction. 13.5 Local Importance # Extract the local importance local_importance &lt;- rf_model$localImp # View the dimensions of local_importance to understand its structure dim(local_importance) ## [1] 8 46 8 features and 46 observations library(ggplot2) library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths # Assuming local_importance is a matrix or data frame with observations as columns and features as rows local_importance_df &lt;- as.data.frame(local_importance) local_importance_df$Feature &lt;- rownames(local_importance_df) local_importance_melted &lt;- melt(local_importance_df, id.vars = &quot;Feature&quot;) ggplot(local_importance_melted, aes(x = variable, y = Feature, fill = value)) + geom_tile() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;blue&quot;) + labs(title = &quot;Heatmap of Local Importance&quot;, x = &quot;Observation&quot;, y = &quot;Feature&quot;) + theme_minimal() ggplot(local_importance_melted, aes(x = Feature, y = value)) + geom_violin(fill = &quot;lightgreen&quot;) + coord_flip() + labs(title = &quot;Violin Plot of Local Importance by Feature&quot;, x = &quot;Feature&quot;, y = &quot;Local Importance&quot;) + theme_minimal() avg_importance &lt;- rowMeans(local_importance) ggplot(data.frame(Feature = names(avg_importance), AvgImportance = avg_importance), aes(x = reorder(Feature, AvgImportance), y = AvgImportance)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;orange&quot;) + coord_flip() + labs(title = &quot;Average Local Importance by Feature&quot;, x = &quot;Feature&quot;, y = &quot;Average Local Importance&quot;) + theme_minimal() tfi &lt;- local_importance[&quot;Total_frm&quot;, ] tfv &lt;- data_rf$Total_frm # Create a data frame for plotting importance_frm_df &lt;- data.frame(Total_frm = tfv, Total_frmImportance = tfi) ggplot(importance_frm_df, aes(x = Total_frm, y = Total_frmImportance)) + geom_point(alpha = 0.5) + geom_smooth() + theme_minimal() + labs(title = &quot;Local Importance of &#39;Total_frm&#39;&quot;, x = &quot;Total_frm&quot;, y = &quot;Local Importance of Total_frm&quot;) ## `geom_smooth()` using method = ## &#39;loess&#39; and formula = &#39;y ~ x&#39; # Extract local importance for &#39;srv_Parish_ministry&#39; spi &lt;- local_importance[&quot;srv_Parish_ministry&quot;, ] spv &lt;- data_rf$srv_Parish_ministry # Create a data frame for plotting imp_df &lt;- data.frame(SPM = spv, Imp = spi) # Plot the violin plot ggplot(imp_df, aes(x = factor(SPM), y = Imp)) + geom_violin(fill = &quot;lightgreen&quot;) + theme_minimal() + labs(title = &quot;Local Importance by &#39;srv_Parish_ministry&#39; Status&quot;, x = &quot;srv_Parish_ministry (0 or 1)&quot;, y = &quot;Local Importance&quot;) # Plot the jitter plot with boxplot overlay ggplot(imp_df, aes(x = factor(SPM), y = Imp)) + geom_jitter(width = 0.2, alpha = 0.5) + geom_boxplot(outlier.shape = NA, fill = &quot;lightblue&quot;, alpha = 0.3) + theme_minimal() + labs(title = &quot;Local Importance by &#39;srv_Parish_ministry&#39; Status&quot;, x = &quot;srv_Parish_ministry (0 or 1)&quot;, y = &quot;Local Importance&quot;) 13.6 Partial Dependence Plots Partial Dependence Plots (PDPs): PDPs help you understand the relationship between a feature (or features) and the target variable in a machine learning model, such as a Random Forest. Specifically, they show how the predicted outcome varies with changes in a particular feature, while averaging out the effects of all other features in the model. Single-Feature PDP: For a single feature, the PDP shows the marginal effect of that feature on the predicted outcome. How It Works: The model’s predictions are averaged over different values of the feature of interest, holding all other features constant. This allows you to see whether the relationship between the feature and the target is linear, monotonic, or more complex. pdp_total_frm &lt;- partial(rf_model, pred.var = &quot;Total_frm&quot;) plot(pdp_total_frm, type = &quot;l&quot;, main = &quot;Partial Dependence of Total_frm&quot;, xlab = &quot;Total_frm&quot;, ylab = &quot;Predicted Total_dsc&quot;) # Generate PDP data for the binary variable &#39;srv_Parish_ministry&#39; pdp_srv_parish &lt;- partial(rf_model, pred.var = &quot;srv_Parish_ministry&quot;, plot = FALSE) # Convert the result to a data frame pdp_df &lt;- as.data.frame(pdp_srv_parish) # Calculate the difference between the two bars difference &lt;- round(diff(pdp_df$yhat), 2) ggplot(pdp_df, aes(x = factor(srv_Parish_ministry), y = yhat)) + geom_bar(stat = &quot;identity&quot;, aes(fill = factor(srv_Parish_ministry)), width = 0.4) + scale_fill_manual(values = c(&quot;0&quot; = &quot;red&quot;, &quot;1&quot; = &quot;green&quot;)) + # Custom colors for the bars geom_text(aes(label = round(yhat, 2)), vjust = -0.5, size = 5) + # Display the exact values on top of the bars expand_limits(y = max(pdp_df$yhat) * 1.2) + # Expand y-axis limits slightly annotate(&quot;text&quot;, x = 1.5, y = max(pdp_df$yhat) * 1.1, label = paste(&quot;Difference of Serving\\nin Parish Ministry: &quot;, difference), size = 4, color = &quot;black&quot;) + # Smaller size and stacked text theme_minimal(base_size = 12) + theme(panel.background = element_rect(fill = &quot;grey&quot;, color = NA), # Grey background plot.background = element_rect(fill = &quot;grey&quot;, color = NA), legend.position = &quot;none&quot;) + # Remove the legend labs(title = &quot;Partial Dependence of srv_Parish_ministry&quot;, x = &quot;srv_Parish_ministry (0 or 1)&quot;, y = &quot;Predicted Total_dsc&quot;) 13.7 Random Forest Explainer # Extract the minimum depth distribution of variables min_depth_frame &lt;- min_depth_distribution(rf_model) # Measure variable importance impf &lt;- measure_importance(rf_model) impf ## variable mean_min_depth no_of_nodes mse_increase ## 1 CCO 3.002598 1296 0.29362330 ## 2 Involvment 2.159763 2232 0.62082038 ## 3 Relations 2.186941 2801 0.04516093 ## 4 srv_CCO 2.525087 1609 0.51454140 ## 5 srv_HCC 2.934163 1124 0.47609797 ## 6 srv_Parish_ministry 1.835050 1954 1.82452498 ## 7 Total_cgs 1.863270 3110 0.66980898 ## 8 Total_frm 1.562947 3262 2.21378233 ## node_purity_increase no_of_trees times_a_root p_value ## 1 16.13296 1024 35 1.000000e+00 ## 2 36.69981 1287 202 9.211960e-02 ## 3 34.75069 1340 92 6.113672e-44 ## 4 28.76142 1194 167 1.000000e+00 ## 5 24.06673 928 151 1.000000e+00 ## 6 71.74472 1348 294 9.999998e-01 ## 7 52.24009 1359 219 2.375626e-92 ## 8 93.05213 1398 340 3.452951e-122 plot_multi_way_importance(impf, x_measure = &quot;mean_min_depth&quot;, y_measure = &quot;node_purity_increase&quot;, #its regressional so not gini size_measure = &quot;p_value&quot;, no_of_labels = 6) ## Warning: Using alpha for a discrete ## variable is not advised. node_purity_increase: The total increase in node purity (reduction in variance or MSE) attributed to the variable. mean_min_depth: The average minimum depth at which the variable is used to split a node. look more at this plot_min_depth_distribution(min_depth_frame, mean_sample = &quot;all_trees&quot;, k = 20, main = &quot;Distribution of Minimal Depth and Its Mean&quot;) 13.8 Final Notes We dont loop the RF because it is on whole data and the amount of trees in the RF Model "],["what_is_stacked_ensambled.html", "Chapter 14 13_What_Is_Stacked_Ensambled", " Chapter 14 13_What_Is_Stacked_Ensambled WE USE RMSE NOT RMSPE… it can be prediction but chat gpt thinks its percentage and is a idiot Stacked ensemble learning, or stacking, involves training multiple different types of models (e.g., decision trees, logistic regression, neural networks) and then combining their predictions using a “meta-model” or “meta-learner.” The meta-model learns to make the final prediction by considering the outputs of the individual models as input features. This approach often leads to better performance than any single model or bagging method like random forest. Homogeneity vs. Heterogeneity: Bagging typically uses homogeneous models (e.g., multiple CART trees), while stacked ensembles use heterogeneous models (e.g., GBM, random forest, neural networks). # Load the required libraries library(randomForest) library(gbm) library(parallel) library(dplyr) library(MASS) library(caret) # Get the number of cores minus 1 nc &lt;- detectCores() - 1 # Load the Boston dataset data &lt;- Boston # Convert the &#39;chas&#39; column to a factor data$chas &lt;- as.factor(data$chas) # Scale all numeric columns in the data numeric_columns &lt;- sapply(data, is.numeric) data[numeric_columns] &lt;- scale(data[numeric_columns]) # Verify that the column names are correct colnames(data) &lt;- gsub(&quot;scale\\\\((.+)\\\\)&quot;, &quot;\\\\1&quot;, colnames(data)) # Now, the &#39;medv&#39; column should still be named &#39;medv&#39; # Train linear regression model lm_mod &lt;- lm(medv ~ ., data = data) # Train random forest model rf_mod &lt;- randomForest(medv ~ ., data = data) # Train gradient boosting model gbm_mod &lt;- gbm(medv ~ ., data = data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1) # Generate predictions lm_pred &lt;- predict(lm_mod, data) rf_pred &lt;- predict(rf_mod, data) gbm_pred &lt;- predict(gbm_mod, data, n.trees = 100) # Combine predictions into a data frame preds &lt;- data.frame(lm_pred, rf_pred, gbm_pred) # Train linear regression as meta-model meta_lm &lt;- lm(medv ~ ., data = cbind(preds, medv = data$medv)) # Predict using meta-model meta_lm_pred &lt;- predict(meta_lm, preds) # Calculate RMSPE (now as RMSE since the percentage part is removed) rmspe_lm &lt;- sqrt(mean((meta_lm_pred - data$medv)^2)) # Train random forest as meta-model meta_rf &lt;- randomForest(medv ~ ., data = cbind(preds, medv = data$medv)) # Predict using meta-model meta_rf_pred &lt;- predict(meta_rf, preds) # Calculate RMSPE (now as RMSE since the percentage part is removed) rmspe_rf &lt;- sqrt(mean((meta_rf_pred - data$medv)^2)) # Train gradient boosting as meta-model meta_gbm &lt;- gbm(medv ~ ., data = cbind(preds, medv = data$medv), distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1) # Predict using meta-model meta_gbm_pred &lt;- predict(meta_gbm, preds, n.trees = 100) # Calculate RMSPE (now as RMSE since the percentage part is removed) rmspe_gbm &lt;- sqrt(mean((meta_gbm_pred - data$medv)^2)) # Calculate RMSPE for base models (now as RMSE since the percentage part is removed) rmspe_base_lm &lt;- sqrt(mean((lm_pred - data$medv)^2)) rmspe_base_rf &lt;- sqrt(mean((rf_pred - data$medv)^2)) rmspe_base_gbm &lt;- sqrt(mean((gbm_pred - data$medv)^2)) # Find the best base model&#39;s RMSPE best_base_rmspe &lt;- min(rmspe_base_lm, rmspe_base_rf, rmspe_base_gbm) worst_base_rmspe &lt;- max(rmspe_base_lm, rmspe_base_rf, rmspe_base_gbm) # Print the RMSPEs print(rmspe_lm) ## [1] 0.1014164 print(rmspe_rf) ## [1] 0.08832115 print(rmspe_gbm) ## [1] 0.1279383 print(best_base_rmspe) ## [1] 0.1526848 print(worst_base_rmspe) ## [1] 0.5087679 Now we see that the stacked ensemble models have lower RMSPE compared to the individual base models. However the GBM model wasnt tuned at all. what is the effect of tuning the GBM model? # Hyperparameter tuning grid grid &lt;- expand.grid( n.trees = seq(50, 200, by = 50), shrinkage = seq(0.05, 0.15, by = 0.04), interaction.depth = seq(1, 3, by = 2) ) results &lt;- data.frame(n.trees = integer(), shrinkage = numeric(), interaction.depth = integer(), cv.error = numeric()) # Loop over the grid for (i in 1:nrow(grid)) { cat(&quot;\\rProgress:&quot;, i, &quot;/&quot;, nrow(grid), &quot;iterations completed&quot;) gbm_mod &lt;- gbm(medv ~ ., data = data, distribution = &quot;gaussian&quot;, n.trees = grid$n.trees[i], shrinkage = grid$shrinkage[i], interaction.depth = grid$interaction.depth[i], cv.folds = 3, n.cores = nc, verbose = FALSE) # Get the cross-validation error cv_error &lt;- min(gbm_mod$cv.error) # Store the results results &lt;- rbind(results, data.frame(n.trees = grid$n.trees[i], shrinkage = grid$shrinkage[i], interaction.depth = grid$interaction.depth[i], cv.error = cv_error)) } ## Progress: 1 / 24 iterations completed Progress: 2 / 24 iterations completed Progress: 3 / 24 iterations completed Progress: 4 / 24 iterations completed Progress: 5 / 24 iterations completed Progress: 6 / 24 iterations completed Progress: 7 / 24 iterations completed Progress: 8 / 24 iterations completed Progress: 9 / 24 iterations completed Progress: 10 / 24 iterations completed Progress: 11 / 24 iterations completed Progress: 12 / 24 iterations completed Progress: 13 / 24 iterations completed Progress: 14 / 24 iterations completed Progress: 15 / 24 iterations completed Progress: 16 / 24 iterations completed Progress: 17 / 24 iterations completed Progress: 18 / 24 iterations completed Progress: 19 / 24 iterations completed Progress: 20 / 24 iterations completed Progress: 21 / 24 iterations completed Progress: 22 / 24 iterations completed Progress: 23 / 24 iterations completed Progress: 24 / 24 iterations completed # Find the best hyperparameters - base R version best_params &lt;- results[order(results$cv.error), ][1, ] print(best_params) ## n.trees shrinkage interaction.depth cv.error ## 24 200 0.13 3 0.12826 rmspe_t2 &lt;- c() for(i in 1:1000){ train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train the GBM model with the best hyperparameters best_gbm &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, verbose = FALSE) # Predict on the test set test_pred &lt;- predict(best_gbm, test_data, n.trees = 100) # Calculate RMSPE on the test set (now as RMSE since the percentage part is removed) rmspe_t2[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } mean(rmspe_t2) ## [1] 0.4297122 rmspe_test &lt;- c() for(i in 1:1000){ train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train the GBM model with the best hyperparameters best_gbm &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth) # Predict on the test set test_pred &lt;- predict(best_gbm, test_data, n.trees = best_params$n.trees) # Calculate RMSPE on the test set (now as RMSE since the percentage part is removed) rmspe_test[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } mean(rmspe_test) ## [1] 0.3751048 Now we see that we have a slightly tuned gbm model. SO will this better tuned base model effect the stacked ensemble model? # Store RMSPE results for the two stacked ensembles rmspe_ensemble_original &lt;- c() rmspe_ensemble_tuned &lt;- c() for(i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Original Base Models lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1, verbose = FALSE) # Generate predictions lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) gbm_pred &lt;- predict(gbm_mod, test_data, n.trees = 100) # Combine predictions into a data frame preds &lt;- data.frame(lm_pred, rf_pred, gbm_pred) # Train and evaluate original stacked ensemble meta_lm &lt;- lm(medv ~ ., data = cbind(preds, medv = test_data$medv)) meta_lm_pred &lt;- predict(meta_lm, preds) rmspe_ensemble_original[i] &lt;- sqrt(mean((meta_lm_pred - test_data$medv)^2)) # Train and evaluate stacked ensemble with tuned GBM model tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) gbm_pred_tuned &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) # Combine predictions with tuned GBM preds_tuned &lt;- data.frame(lm_pred, rf_pred, gbm_pred_tuned) meta_lm_tuned &lt;- lm(medv ~ ., data = cbind(preds_tuned, medv = test_data$medv)) meta_lm_pred_tuned &lt;- predict(meta_lm_tuned, preds_tuned) rmspe_ensemble_tuned[i] &lt;- sqrt(mean((meta_lm_pred_tuned - test_data$medv)^2)) } ## Progress: 1 iterations completed Progress: 2 iterations completed Progress: 3 iterations completed Progress: 4 iterations completed Progress: 5 iterations completed Progress: 6 iterations completed Progress: 7 iterations completed Progress: 8 iterations completed Progress: 9 iterations completed Progress: 10 iterations completed Progress: 11 iterations completed Progress: 12 iterations completed Progress: 13 iterations completed Progress: 14 iterations completed Progress: 15 iterations completed Progress: 16 iterations completed Progress: 17 iterations completed Progress: 18 iterations completed Progress: 19 iterations completed Progress: 20 iterations completed Progress: 21 iterations completed Progress: 22 iterations completed Progress: 23 iterations completed Progress: 24 iterations completed Progress: 25 iterations completed Progress: 26 iterations completed Progress: 27 iterations completed Progress: 28 iterations completed Progress: 29 iterations completed Progress: 30 iterations completed Progress: 31 iterations completed Progress: 32 iterations completed Progress: 33 iterations completed Progress: 34 iterations completed Progress: 35 iterations completed Progress: 36 iterations completed Progress: 37 iterations completed Progress: 38 iterations completed Progress: 39 iterations completed Progress: 40 iterations completed Progress: 41 iterations completed Progress: 42 iterations completed Progress: 43 iterations completed Progress: 44 iterations completed Progress: 45 iterations completed Progress: 46 iterations completed Progress: 47 iterations completed Progress: 48 iterations completed Progress: 49 iterations completed Progress: 50 iterations completed Progress: 51 iterations completed Progress: 52 iterations completed Progress: 53 iterations completed Progress: 54 iterations completed Progress: 55 iterations completed Progress: 56 iterations completed Progress: 57 iterations completed Progress: 58 iterations completed Progress: 59 iterations completed Progress: 60 iterations completed Progress: 61 iterations completed Progress: 62 iterations completed Progress: 63 iterations completed Progress: 64 iterations completed Progress: 65 iterations completed Progress: 66 iterations completed Progress: 67 iterations completed Progress: 68 iterations completed Progress: 69 iterations completed Progress: 70 iterations completed Progress: 71 iterations completed Progress: 72 iterations completed Progress: 73 iterations completed Progress: 74 iterations completed Progress: 75 iterations completed Progress: 76 iterations completed Progress: 77 iterations completed Progress: 78 iterations completed Progress: 79 iterations completed Progress: 80 iterations completed Progress: 81 iterations completed Progress: 82 iterations completed Progress: 83 iterations completed Progress: 84 iterations completed Progress: 85 iterations completed Progress: 86 iterations completed Progress: 87 iterations completed Progress: 88 iterations completed Progress: 89 iterations completed Progress: 90 iterations completed Progress: 91 iterations completed Progress: 92 iterations completed Progress: 93 iterations completed Progress: 94 iterations completed Progress: 95 iterations completed Progress: 96 iterations completed Progress: 97 iterations completed Progress: 98 iterations completed Progress: 99 iterations completed Progress: 100 iterations completed Progress: 101 iterations completed Progress: 102 iterations completed Progress: 103 iterations completed Progress: 104 iterations completed Progress: 105 iterations completed Progress: 106 iterations completed Progress: 107 iterations completed Progress: 108 iterations completed Progress: 109 iterations completed Progress: 110 iterations completed Progress: 111 iterations completed Progress: 112 iterations completed Progress: 113 iterations completed Progress: 114 iterations completed Progress: 115 iterations completed Progress: 116 iterations completed Progress: 117 iterations completed Progress: 118 iterations completed Progress: 119 iterations completed Progress: 120 iterations completed Progress: 121 iterations completed Progress: 122 iterations completed Progress: 123 iterations completed Progress: 124 iterations completed Progress: 125 iterations completed Progress: 126 iterations completed Progress: 127 iterations completed Progress: 128 iterations completed Progress: 129 iterations completed Progress: 130 iterations completed Progress: 131 iterations completed Progress: 132 iterations completed Progress: 133 iterations completed Progress: 134 iterations completed Progress: 135 iterations completed Progress: 136 iterations completed Progress: 137 iterations completed Progress: 138 iterations completed Progress: 139 iterations completed Progress: 140 iterations completed Progress: 141 iterations completed Progress: 142 iterations completed Progress: 143 iterations completed Progress: 144 iterations completed Progress: 145 iterations completed Progress: 146 iterations completed Progress: 147 iterations completed Progress: 148 iterations completed Progress: 149 iterations completed Progress: 150 iterations completed Progress: 151 iterations completed Progress: 152 iterations completed Progress: 153 iterations completed Progress: 154 iterations completed Progress: 155 iterations completed Progress: 156 iterations completed Progress: 157 iterations completed Progress: 158 iterations completed Progress: 159 iterations completed Progress: 160 iterations completed Progress: 161 iterations completed Progress: 162 iterations completed Progress: 163 iterations completed Progress: 164 iterations completed Progress: 165 iterations completed Progress: 166 iterations completed Progress: 167 iterations completed Progress: 168 iterations completed Progress: 169 iterations completed Progress: 170 iterations completed Progress: 171 iterations completed Progress: 172 iterations completed Progress: 173 iterations completed Progress: 174 iterations completed Progress: 175 iterations completed Progress: 176 iterations completed Progress: 177 iterations completed Progress: 178 iterations completed Progress: 179 iterations completed Progress: 180 iterations completed Progress: 181 iterations completed Progress: 182 iterations completed Progress: 183 iterations completed Progress: 184 iterations completed Progress: 185 iterations completed Progress: 186 iterations completed Progress: 187 iterations completed Progress: 188 iterations completed Progress: 189 iterations completed Progress: 190 iterations completed Progress: 191 iterations completed Progress: 192 iterations completed Progress: 193 iterations completed Progress: 194 iterations completed Progress: 195 iterations completed Progress: 196 iterations completed Progress: 197 iterations completed Progress: 198 iterations completed Progress: 199 iterations completed Progress: 200 iterations completed Progress: 201 iterations completed Progress: 202 iterations completed Progress: 203 iterations completed Progress: 204 iterations completed Progress: 205 iterations completed Progress: 206 iterations completed Progress: 207 iterations completed Progress: 208 iterations completed Progress: 209 iterations completed Progress: 210 iterations completed Progress: 211 iterations completed Progress: 212 iterations completed Progress: 213 iterations completed Progress: 214 iterations completed Progress: 215 iterations completed Progress: 216 iterations completed Progress: 217 iterations completed Progress: 218 iterations completed Progress: 219 iterations completed Progress: 220 iterations completed Progress: 221 iterations completed Progress: 222 iterations completed Progress: 223 iterations completed Progress: 224 iterations completed Progress: 225 iterations completed Progress: 226 iterations completed Progress: 227 iterations completed Progress: 228 iterations completed Progress: 229 iterations completed Progress: 230 iterations completed Progress: 231 iterations completed Progress: 232 iterations completed Progress: 233 iterations completed Progress: 234 iterations completed Progress: 235 iterations completed Progress: 236 iterations completed Progress: 237 iterations completed Progress: 238 iterations completed Progress: 239 iterations completed Progress: 240 iterations completed Progress: 241 iterations completed Progress: 242 iterations completed Progress: 243 iterations completed Progress: 244 iterations completed Progress: 245 iterations completed Progress: 246 iterations completed Progress: 247 iterations completed Progress: 248 iterations completed Progress: 249 iterations completed Progress: 250 iterations completed Progress: 251 iterations completed Progress: 252 iterations completed Progress: 253 iterations completed Progress: 254 iterations completed Progress: 255 iterations completed Progress: 256 iterations completed Progress: 257 iterations completed Progress: 258 iterations completed Progress: 259 iterations completed Progress: 260 iterations completed Progress: 261 iterations completed Progress: 262 iterations completed Progress: 263 iterations completed Progress: 264 iterations completed Progress: 265 iterations completed Progress: 266 iterations completed Progress: 267 iterations completed Progress: 268 iterations completed Progress: 269 iterations completed Progress: 270 iterations completed Progress: 271 iterations completed Progress: 272 iterations completed Progress: 273 iterations completed Progress: 274 iterations completed Progress: 275 iterations completed Progress: 276 iterations completed Progress: 277 iterations completed Progress: 278 iterations completed Progress: 279 iterations completed Progress: 280 iterations completed Progress: 281 iterations completed Progress: 282 iterations completed Progress: 283 iterations completed Progress: 284 iterations completed Progress: 285 iterations completed Progress: 286 iterations completed Progress: 287 iterations completed Progress: 288 iterations completed Progress: 289 iterations completed Progress: 290 iterations completed Progress: 291 iterations completed Progress: 292 iterations completed Progress: 293 iterations completed Progress: 294 iterations completed Progress: 295 iterations completed Progress: 296 iterations completed Progress: 297 iterations completed Progress: 298 iterations completed Progress: 299 iterations completed Progress: 300 iterations completed Progress: 301 iterations completed Progress: 302 iterations completed Progress: 303 iterations completed Progress: 304 iterations completed Progress: 305 iterations completed Progress: 306 iterations completed Progress: 307 iterations completed Progress: 308 iterations completed Progress: 309 iterations completed Progress: 310 iterations completed Progress: 311 iterations completed Progress: 312 iterations completed Progress: 313 iterations completed Progress: 314 iterations completed Progress: 315 iterations completed Progress: 316 iterations completed Progress: 317 iterations completed Progress: 318 iterations completed Progress: 319 iterations completed Progress: 320 iterations completed Progress: 321 iterations completed Progress: 322 iterations completed Progress: 323 iterations completed Progress: 324 iterations completed Progress: 325 iterations completed Progress: 326 iterations completed Progress: 327 iterations completed Progress: 328 iterations completed Progress: 329 iterations completed Progress: 330 iterations completed Progress: 331 iterations completed Progress: 332 iterations completed Progress: 333 iterations completed Progress: 334 iterations completed Progress: 335 iterations completed Progress: 336 iterations completed Progress: 337 iterations completed Progress: 338 iterations completed Progress: 339 iterations completed Progress: 340 iterations completed Progress: 341 iterations completed Progress: 342 iterations completed Progress: 343 iterations completed Progress: 344 iterations completed Progress: 345 iterations completed Progress: 346 iterations completed Progress: 347 iterations completed Progress: 348 iterations completed Progress: 349 iterations completed Progress: 350 iterations completed Progress: 351 iterations completed Progress: 352 iterations completed Progress: 353 iterations completed Progress: 354 iterations completed Progress: 355 iterations completed Progress: 356 iterations completed Progress: 357 iterations completed Progress: 358 iterations completed Progress: 359 iterations completed Progress: 360 iterations completed Progress: 361 iterations completed Progress: 362 iterations completed Progress: 363 iterations completed Progress: 364 iterations completed Progress: 365 iterations completed Progress: 366 iterations completed Progress: 367 iterations completed Progress: 368 iterations completed Progress: 369 iterations completed Progress: 370 iterations completed Progress: 371 iterations completed Progress: 372 iterations completed Progress: 373 iterations completed Progress: 374 iterations completed Progress: 375 iterations completed Progress: 376 iterations completed Progress: 377 iterations completed Progress: 378 iterations completed Progress: 379 iterations completed Progress: 380 iterations completed Progress: 381 iterations completed Progress: 382 iterations completed Progress: 383 iterations completed Progress: 384 iterations completed Progress: 385 iterations completed Progress: 386 iterations completed Progress: 387 iterations completed Progress: 388 iterations completed Progress: 389 iterations completed Progress: 390 iterations completed Progress: 391 iterations completed Progress: 392 iterations completed Progress: 393 iterations completed Progress: 394 iterations completed Progress: 395 iterations completed Progress: 396 iterations completed Progress: 397 iterations completed Progress: 398 iterations completed Progress: 399 iterations completed Progress: 400 iterations completed Progress: 401 iterations completed Progress: 402 iterations completed Progress: 403 iterations completed Progress: 404 iterations completed Progress: 405 iterations completed Progress: 406 iterations completed Progress: 407 iterations completed Progress: 408 iterations completed Progress: 409 iterations completed Progress: 410 iterations completed Progress: 411 iterations completed Progress: 412 iterations completed Progress: 413 iterations completed Progress: 414 iterations completed Progress: 415 iterations completed Progress: 416 iterations completed Progress: 417 iterations completed Progress: 418 iterations completed Progress: 419 iterations completed Progress: 420 iterations completed Progress: 421 iterations completed Progress: 422 iterations completed Progress: 423 iterations completed Progress: 424 iterations completed Progress: 425 iterations completed Progress: 426 iterations completed Progress: 427 iterations completed Progress: 428 iterations completed Progress: 429 iterations completed Progress: 430 iterations completed Progress: 431 iterations completed Progress: 432 iterations completed Progress: 433 iterations completed Progress: 434 iterations completed Progress: 435 iterations completed Progress: 436 iterations completed Progress: 437 iterations completed Progress: 438 iterations completed Progress: 439 iterations completed Progress: 440 iterations completed Progress: 441 iterations completed Progress: 442 iterations completed Progress: 443 iterations completed Progress: 444 iterations completed Progress: 445 iterations completed Progress: 446 iterations completed Progress: 447 iterations completed Progress: 448 iterations completed Progress: 449 iterations completed Progress: 450 iterations completed Progress: 451 iterations completed Progress: 452 iterations completed Progress: 453 iterations completed Progress: 454 iterations completed Progress: 455 iterations completed Progress: 456 iterations completed Progress: 457 iterations completed Progress: 458 iterations completed Progress: 459 iterations completed Progress: 460 iterations completed Progress: 461 iterations completed Progress: 462 iterations completed Progress: 463 iterations completed Progress: 464 iterations completed Progress: 465 iterations completed Progress: 466 iterations completed Progress: 467 iterations completed Progress: 468 iterations completed Progress: 469 iterations completed Progress: 470 iterations completed Progress: 471 iterations completed Progress: 472 iterations completed Progress: 473 iterations completed Progress: 474 iterations completed Progress: 475 iterations completed Progress: 476 iterations completed Progress: 477 iterations completed Progress: 478 iterations completed Progress: 479 iterations completed Progress: 480 iterations completed Progress: 481 iterations completed Progress: 482 iterations completed Progress: 483 iterations completed Progress: 484 iterations completed Progress: 485 iterations completed Progress: 486 iterations completed Progress: 487 iterations completed Progress: 488 iterations completed Progress: 489 iterations completed Progress: 490 iterations completed Progress: 491 iterations completed Progress: 492 iterations completed Progress: 493 iterations completed Progress: 494 iterations completed Progress: 495 iterations completed Progress: 496 iterations completed Progress: 497 iterations completed Progress: 498 iterations completed Progress: 499 iterations completed Progress: 500 iterations completed Progress: 501 iterations completed Progress: 502 iterations completed Progress: 503 iterations completed Progress: 504 iterations completed Progress: 505 iterations completed Progress: 506 iterations completed Progress: 507 iterations completed Progress: 508 iterations completed Progress: 509 iterations completed Progress: 510 iterations completed Progress: 511 iterations completed Progress: 512 iterations completed Progress: 513 iterations completed Progress: 514 iterations completed Progress: 515 iterations completed Progress: 516 iterations completed Progress: 517 iterations completed Progress: 518 iterations completed Progress: 519 iterations completed Progress: 520 iterations completed Progress: 521 iterations completed Progress: 522 iterations completed Progress: 523 iterations completed Progress: 524 iterations completed Progress: 525 iterations completed Progress: 526 iterations completed Progress: 527 iterations completed Progress: 528 iterations completed Progress: 529 iterations completed Progress: 530 iterations completed Progress: 531 iterations completed Progress: 532 iterations completed Progress: 533 iterations completed Progress: 534 iterations completed Progress: 535 iterations completed Progress: 536 iterations completed Progress: 537 iterations completed Progress: 538 iterations completed Progress: 539 iterations completed Progress: 540 iterations completed Progress: 541 iterations completed Progress: 542 iterations completed Progress: 543 iterations completed Progress: 544 iterations completed Progress: 545 iterations completed Progress: 546 iterations completed Progress: 547 iterations completed Progress: 548 iterations completed Progress: 549 iterations completed Progress: 550 iterations completed Progress: 551 iterations completed Progress: 552 iterations completed Progress: 553 iterations completed Progress: 554 iterations completed Progress: 555 iterations completed Progress: 556 iterations completed Progress: 557 iterations completed Progress: 558 iterations completed Progress: 559 iterations completed Progress: 560 iterations completed Progress: 561 iterations completed Progress: 562 iterations completed Progress: 563 iterations completed Progress: 564 iterations completed Progress: 565 iterations completed Progress: 566 iterations completed Progress: 567 iterations completed Progress: 568 iterations completed Progress: 569 iterations completed Progress: 570 iterations completed Progress: 571 iterations completed Progress: 572 iterations completed Progress: 573 iterations completed Progress: 574 iterations completed Progress: 575 iterations completed Progress: 576 iterations completed Progress: 577 iterations completed Progress: 578 iterations completed Progress: 579 iterations completed Progress: 580 iterations completed Progress: 581 iterations completed Progress: 582 iterations completed Progress: 583 iterations completed Progress: 584 iterations completed Progress: 585 iterations completed Progress: 586 iterations completed Progress: 587 iterations completed Progress: 588 iterations completed Progress: 589 iterations completed Progress: 590 iterations completed Progress: 591 iterations completed Progress: 592 iterations completed Progress: 593 iterations completed Progress: 594 iterations completed Progress: 595 iterations completed Progress: 596 iterations completed Progress: 597 iterations completed Progress: 598 iterations completed Progress: 599 iterations completed Progress: 600 iterations completed Progress: 601 iterations completed Progress: 602 iterations completed Progress: 603 iterations completed Progress: 604 iterations completed Progress: 605 iterations completed Progress: 606 iterations completed Progress: 607 iterations completed Progress: 608 iterations completed Progress: 609 iterations completed Progress: 610 iterations completed Progress: 611 iterations completed Progress: 612 iterations completed Progress: 613 iterations completed Progress: 614 iterations completed Progress: 615 iterations completed Progress: 616 iterations completed Progress: 617 iterations completed Progress: 618 iterations completed Progress: 619 iterations completed Progress: 620 iterations completed Progress: 621 iterations completed Progress: 622 iterations completed Progress: 623 iterations completed Progress: 624 iterations completed Progress: 625 iterations completed Progress: 626 iterations completed Progress: 627 iterations completed Progress: 628 iterations completed Progress: 629 iterations completed Progress: 630 iterations completed Progress: 631 iterations completed Progress: 632 iterations completed Progress: 633 iterations completed Progress: 634 iterations completed Progress: 635 iterations completed Progress: 636 iterations completed Progress: 637 iterations completed Progress: 638 iterations completed Progress: 639 iterations completed Progress: 640 iterations completed Progress: 641 iterations completed Progress: 642 iterations completed Progress: 643 iterations completed Progress: 644 iterations completed Progress: 645 iterations completed Progress: 646 iterations completed Progress: 647 iterations completed Progress: 648 iterations completed Progress: 649 iterations completed Progress: 650 iterations completed Progress: 651 iterations completed Progress: 652 iterations completed Progress: 653 iterations completed Progress: 654 iterations completed Progress: 655 iterations completed Progress: 656 iterations completed Progress: 657 iterations completed Progress: 658 iterations completed Progress: 659 iterations completed Progress: 660 iterations completed Progress: 661 iterations completed Progress: 662 iterations completed Progress: 663 iterations completed Progress: 664 iterations completed Progress: 665 iterations completed Progress: 666 iterations completed Progress: 667 iterations completed Progress: 668 iterations completed Progress: 669 iterations completed Progress: 670 iterations completed Progress: 671 iterations completed Progress: 672 iterations completed Progress: 673 iterations completed Progress: 674 iterations completed Progress: 675 iterations completed Progress: 676 iterations completed Progress: 677 iterations completed Progress: 678 iterations completed Progress: 679 iterations completed Progress: 680 iterations completed Progress: 681 iterations completed Progress: 682 iterations completed Progress: 683 iterations completed Progress: 684 iterations completed Progress: 685 iterations completed Progress: 686 iterations completed Progress: 687 iterations completed Progress: 688 iterations completed Progress: 689 iterations completed Progress: 690 iterations completed Progress: 691 iterations completed Progress: 692 iterations completed Progress: 693 iterations completed Progress: 694 iterations completed Progress: 695 iterations completed Progress: 696 iterations completed Progress: 697 iterations completed Progress: 698 iterations completed Progress: 699 iterations completed Progress: 700 iterations completed Progress: 701 iterations completed Progress: 702 iterations completed Progress: 703 iterations completed Progress: 704 iterations completed Progress: 705 iterations completed Progress: 706 iterations completed Progress: 707 iterations completed Progress: 708 iterations completed Progress: 709 iterations completed Progress: 710 iterations completed Progress: 711 iterations completed Progress: 712 iterations completed Progress: 713 iterations completed Progress: 714 iterations completed Progress: 715 iterations completed Progress: 716 iterations completed Progress: 717 iterations completed Progress: 718 iterations completed Progress: 719 iterations completed Progress: 720 iterations completed Progress: 721 iterations completed Progress: 722 iterations completed Progress: 723 iterations completed Progress: 724 iterations completed Progress: 725 iterations completed Progress: 726 iterations completed Progress: 727 iterations completed Progress: 728 iterations completed Progress: 729 iterations completed Progress: 730 iterations completed Progress: 731 iterations completed Progress: 732 iterations completed Progress: 733 iterations completed Progress: 734 iterations completed Progress: 735 iterations completed Progress: 736 iterations completed Progress: 737 iterations completed Progress: 738 iterations completed Progress: 739 iterations completed Progress: 740 iterations completed Progress: 741 iterations completed Progress: 742 iterations completed Progress: 743 iterations completed Progress: 744 iterations completed Progress: 745 iterations completed Progress: 746 iterations completed Progress: 747 iterations completed Progress: 748 iterations completed Progress: 749 iterations completed Progress: 750 iterations completed Progress: 751 iterations completed Progress: 752 iterations completed Progress: 753 iterations completed Progress: 754 iterations completed Progress: 755 iterations completed Progress: 756 iterations completed Progress: 757 iterations completed Progress: 758 iterations completed Progress: 759 iterations completed Progress: 760 iterations completed Progress: 761 iterations completed Progress: 762 iterations completed Progress: 763 iterations completed Progress: 764 iterations completed Progress: 765 iterations completed Progress: 766 iterations completed Progress: 767 iterations completed Progress: 768 iterations completed Progress: 769 iterations completed Progress: 770 iterations completed Progress: 771 iterations completed Progress: 772 iterations completed Progress: 773 iterations completed Progress: 774 iterations completed Progress: 775 iterations completed Progress: 776 iterations completed Progress: 777 iterations completed Progress: 778 iterations completed Progress: 779 iterations completed Progress: 780 iterations completed Progress: 781 iterations completed Progress: 782 iterations completed Progress: 783 iterations completed Progress: 784 iterations completed Progress: 785 iterations completed Progress: 786 iterations completed Progress: 787 iterations completed Progress: 788 iterations completed Progress: 789 iterations completed Progress: 790 iterations completed Progress: 791 iterations completed Progress: 792 iterations completed Progress: 793 iterations completed Progress: 794 iterations completed Progress: 795 iterations completed Progress: 796 iterations completed Progress: 797 iterations completed Progress: 798 iterations completed Progress: 799 iterations completed Progress: 800 iterations completed Progress: 801 iterations completed Progress: 802 iterations completed Progress: 803 iterations completed Progress: 804 iterations completed Progress: 805 iterations completed Progress: 806 iterations completed Progress: 807 iterations completed Progress: 808 iterations completed Progress: 809 iterations completed Progress: 810 iterations completed Progress: 811 iterations completed Progress: 812 iterations completed Progress: 813 iterations completed Progress: 814 iterations completed Progress: 815 iterations completed Progress: 816 iterations completed Progress: 817 iterations completed Progress: 818 iterations completed Progress: 819 iterations completed Progress: 820 iterations completed Progress: 821 iterations completed Progress: 822 iterations completed Progress: 823 iterations completed Progress: 824 iterations completed Progress: 825 iterations completed Progress: 826 iterations completed Progress: 827 iterations completed Progress: 828 iterations completed Progress: 829 iterations completed Progress: 830 iterations completed Progress: 831 iterations completed Progress: 832 iterations completed Progress: 833 iterations completed Progress: 834 iterations completed Progress: 835 iterations completed Progress: 836 iterations completed Progress: 837 iterations completed Progress: 838 iterations completed Progress: 839 iterations completed Progress: 840 iterations completed Progress: 841 iterations completed Progress: 842 iterations completed Progress: 843 iterations completed Progress: 844 iterations completed Progress: 845 iterations completed Progress: 846 iterations completed Progress: 847 iterations completed Progress: 848 iterations completed Progress: 849 iterations completed Progress: 850 iterations completed Progress: 851 iterations completed Progress: 852 iterations completed Progress: 853 iterations completed Progress: 854 iterations completed Progress: 855 iterations completed Progress: 856 iterations completed Progress: 857 iterations completed Progress: 858 iterations completed Progress: 859 iterations completed Progress: 860 iterations completed Progress: 861 iterations completed Progress: 862 iterations completed Progress: 863 iterations completed Progress: 864 iterations completed Progress: 865 iterations completed Progress: 866 iterations completed Progress: 867 iterations completed Progress: 868 iterations completed Progress: 869 iterations completed Progress: 870 iterations completed Progress: 871 iterations completed Progress: 872 iterations completed Progress: 873 iterations completed Progress: 874 iterations completed Progress: 875 iterations completed Progress: 876 iterations completed Progress: 877 iterations completed Progress: 878 iterations completed Progress: 879 iterations completed Progress: 880 iterations completed Progress: 881 iterations completed Progress: 882 iterations completed Progress: 883 iterations completed Progress: 884 iterations completed Progress: 885 iterations completed Progress: 886 iterations completed Progress: 887 iterations completed Progress: 888 iterations completed Progress: 889 iterations completed Progress: 890 iterations completed Progress: 891 iterations completed Progress: 892 iterations completed Progress: 893 iterations completed Progress: 894 iterations completed Progress: 895 iterations completed Progress: 896 iterations completed Progress: 897 iterations completed Progress: 898 iterations completed Progress: 899 iterations completed Progress: 900 iterations completed Progress: 901 iterations completed Progress: 902 iterations completed Progress: 903 iterations completed Progress: 904 iterations completed Progress: 905 iterations completed Progress: 906 iterations completed Progress: 907 iterations completed Progress: 908 iterations completed Progress: 909 iterations completed Progress: 910 iterations completed Progress: 911 iterations completed Progress: 912 iterations completed Progress: 913 iterations completed Progress: 914 iterations completed Progress: 915 iterations completed Progress: 916 iterations completed Progress: 917 iterations completed Progress: 918 iterations completed Progress: 919 iterations completed Progress: 920 iterations completed Progress: 921 iterations completed Progress: 922 iterations completed Progress: 923 iterations completed Progress: 924 iterations completed Progress: 925 iterations completed Progress: 926 iterations completed Progress: 927 iterations completed Progress: 928 iterations completed Progress: 929 iterations completed Progress: 930 iterations completed Progress: 931 iterations completed Progress: 932 iterations completed Progress: 933 iterations completed Progress: 934 iterations completed Progress: 935 iterations completed Progress: 936 iterations completed Progress: 937 iterations completed Progress: 938 iterations completed Progress: 939 iterations completed Progress: 940 iterations completed Progress: 941 iterations completed Progress: 942 iterations completed Progress: 943 iterations completed Progress: 944 iterations completed Progress: 945 iterations completed Progress: 946 iterations completed Progress: 947 iterations completed Progress: 948 iterations completed Progress: 949 iterations completed Progress: 950 iterations completed Progress: 951 iterations completed Progress: 952 iterations completed Progress: 953 iterations completed Progress: 954 iterations completed Progress: 955 iterations completed Progress: 956 iterations completed Progress: 957 iterations completed Progress: 958 iterations completed Progress: 959 iterations completed Progress: 960 iterations completed Progress: 961 iterations completed Progress: 962 iterations completed Progress: 963 iterations completed Progress: 964 iterations completed Progress: 965 iterations completed Progress: 966 iterations completed Progress: 967 iterations completed Progress: 968 iterations completed Progress: 969 iterations completed Progress: 970 iterations completed Progress: 971 iterations completed Progress: 972 iterations completed Progress: 973 iterations completed Progress: 974 iterations completed Progress: 975 iterations completed Progress: 976 iterations completed Progress: 977 iterations completed Progress: 978 iterations completed Progress: 979 iterations completed Progress: 980 iterations completed Progress: 981 iterations completed Progress: 982 iterations completed Progress: 983 iterations completed Progress: 984 iterations completed Progress: 985 iterations completed Progress: 986 iterations completed Progress: 987 iterations completed Progress: 988 iterations completed Progress: 989 iterations completed Progress: 990 iterations completed Progress: 991 iterations completed Progress: 992 iterations completed Progress: 993 iterations completed Progress: 994 iterations completed Progress: 995 iterations completed Progress: 996 iterations completed Progress: 997 iterations completed Progress: 998 iterations completed Progress: 999 iterations completed Progress: 1000 iterations completed # Calculate mean RMSPE for both ensembles mean_rmspe_ensemble_original &lt;- mean(rmspe_ensemble_original) mean_rmspe_ensemble_tuned &lt;- mean(rmspe_ensemble_tuned) # Print the results print(paste(&quot;Mean RMSPE of Original Stacked Ensemble:&quot;, mean_rmspe_ensemble_original)) ## [1] &quot;Mean RMSPE of Original Stacked Ensemble: 0.353459843644142&quot; print(paste(&quot;Mean RMSPE of Tuned Stacked Ensemble:&quot;, mean_rmspe_ensemble_tuned)) ## [1] &quot;Mean RMSPE of Tuned Stacked Ensemble: 0.346391469391828&quot; We can see a difference in the average RMSPE now we will ask the question does adding a model that is simply worse and similar to the other models help the ensemble model? we will now stacked ensambled all 4 base models that we have # Store RMSPE results for the stacked ensemble with all four models rmspe_ensemble_four &lt;- c() for(i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) # Train both GBM models on the bootstrapped training data base_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1, verbose = FALSE) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) base_gbm_pred &lt;- predict(base_gbm_mod, test_data, n.trees = 100) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, base_gbm_pred, tuned_gbm_pred) # Train and evaluate the stacked ensemble with all four models meta_four &lt;- lm(medv ~ ., data = cbind(preds_four, medv = test_data$medv)) meta_four_pred &lt;- predict(meta_four, preds_four) # Calculate RMSPE for the ensemble with all four models rmspe_ensemble_four[i] &lt;- sqrt(mean((meta_four_pred - test_data$medv)^2)) } ## Progress: 1 iterations completed Progress: 2 iterations completed Progress: 3 iterations completed Progress: 4 iterations completed Progress: 5 iterations completed Progress: 6 iterations completed Progress: 7 iterations completed Progress: 8 iterations completed Progress: 9 iterations completed Progress: 10 iterations completed Progress: 11 iterations completed Progress: 12 iterations completed Progress: 13 iterations completed Progress: 14 iterations completed Progress: 15 iterations completed Progress: 16 iterations completed Progress: 17 iterations completed Progress: 18 iterations completed Progress: 19 iterations completed Progress: 20 iterations completed Progress: 21 iterations completed Progress: 22 iterations completed Progress: 23 iterations completed Progress: 24 iterations completed Progress: 25 iterations completed Progress: 26 iterations completed Progress: 27 iterations completed Progress: 28 iterations completed Progress: 29 iterations completed Progress: 30 iterations completed Progress: 31 iterations completed Progress: 32 iterations completed Progress: 33 iterations completed Progress: 34 iterations completed Progress: 35 iterations completed Progress: 36 iterations completed Progress: 37 iterations completed Progress: 38 iterations completed Progress: 39 iterations completed Progress: 40 iterations completed Progress: 41 iterations completed Progress: 42 iterations completed Progress: 43 iterations completed Progress: 44 iterations completed Progress: 45 iterations completed Progress: 46 iterations completed Progress: 47 iterations completed Progress: 48 iterations completed Progress: 49 iterations completed Progress: 50 iterations completed Progress: 51 iterations completed Progress: 52 iterations completed Progress: 53 iterations completed Progress: 54 iterations completed Progress: 55 iterations completed Progress: 56 iterations completed Progress: 57 iterations completed Progress: 58 iterations completed Progress: 59 iterations completed Progress: 60 iterations completed Progress: 61 iterations completed Progress: 62 iterations completed Progress: 63 iterations completed Progress: 64 iterations completed Progress: 65 iterations completed Progress: 66 iterations completed Progress: 67 iterations completed Progress: 68 iterations completed Progress: 69 iterations completed Progress: 70 iterations completed Progress: 71 iterations completed Progress: 72 iterations completed Progress: 73 iterations completed Progress: 74 iterations completed Progress: 75 iterations completed Progress: 76 iterations completed Progress: 77 iterations completed Progress: 78 iterations completed Progress: 79 iterations completed Progress: 80 iterations completed Progress: 81 iterations completed Progress: 82 iterations completed Progress: 83 iterations completed Progress: 84 iterations completed Progress: 85 iterations completed Progress: 86 iterations completed Progress: 87 iterations completed Progress: 88 iterations completed Progress: 89 iterations completed Progress: 90 iterations completed Progress: 91 iterations completed Progress: 92 iterations completed Progress: 93 iterations completed Progress: 94 iterations completed Progress: 95 iterations completed Progress: 96 iterations completed Progress: 97 iterations completed Progress: 98 iterations completed Progress: 99 iterations completed Progress: 100 iterations completed Progress: 101 iterations completed Progress: 102 iterations completed Progress: 103 iterations completed Progress: 104 iterations completed Progress: 105 iterations completed Progress: 106 iterations completed Progress: 107 iterations completed Progress: 108 iterations completed Progress: 109 iterations completed Progress: 110 iterations completed Progress: 111 iterations completed Progress: 112 iterations completed Progress: 113 iterations completed Progress: 114 iterations completed Progress: 115 iterations completed Progress: 116 iterations completed Progress: 117 iterations completed Progress: 118 iterations completed Progress: 119 iterations completed Progress: 120 iterations completed Progress: 121 iterations completed Progress: 122 iterations completed Progress: 123 iterations completed Progress: 124 iterations completed Progress: 125 iterations completed Progress: 126 iterations completed Progress: 127 iterations completed Progress: 128 iterations completed Progress: 129 iterations completed Progress: 130 iterations completed Progress: 131 iterations completed Progress: 132 iterations completed Progress: 133 iterations completed Progress: 134 iterations completed Progress: 135 iterations completed Progress: 136 iterations completed Progress: 137 iterations completed Progress: 138 iterations completed Progress: 139 iterations completed Progress: 140 iterations completed Progress: 141 iterations completed Progress: 142 iterations completed Progress: 143 iterations completed Progress: 144 iterations completed Progress: 145 iterations completed Progress: 146 iterations completed Progress: 147 iterations completed Progress: 148 iterations completed Progress: 149 iterations completed Progress: 150 iterations completed Progress: 151 iterations completed Progress: 152 iterations completed Progress: 153 iterations completed Progress: 154 iterations completed Progress: 155 iterations completed Progress: 156 iterations completed Progress: 157 iterations completed Progress: 158 iterations completed Progress: 159 iterations completed Progress: 160 iterations completed Progress: 161 iterations completed Progress: 162 iterations completed Progress: 163 iterations completed Progress: 164 iterations completed Progress: 165 iterations completed Progress: 166 iterations completed Progress: 167 iterations completed Progress: 168 iterations completed Progress: 169 iterations completed Progress: 170 iterations completed Progress: 171 iterations completed Progress: 172 iterations completed Progress: 173 iterations completed Progress: 174 iterations completed Progress: 175 iterations completed Progress: 176 iterations completed Progress: 177 iterations completed Progress: 178 iterations completed Progress: 179 iterations completed Progress: 180 iterations completed Progress: 181 iterations completed Progress: 182 iterations completed Progress: 183 iterations completed Progress: 184 iterations completed Progress: 185 iterations completed Progress: 186 iterations completed Progress: 187 iterations completed Progress: 188 iterations completed Progress: 189 iterations completed Progress: 190 iterations completed Progress: 191 iterations completed Progress: 192 iterations completed Progress: 193 iterations completed Progress: 194 iterations completed Progress: 195 iterations completed Progress: 196 iterations completed Progress: 197 iterations completed Progress: 198 iterations completed Progress: 199 iterations completed Progress: 200 iterations completed Progress: 201 iterations completed Progress: 202 iterations completed Progress: 203 iterations completed Progress: 204 iterations completed Progress: 205 iterations completed Progress: 206 iterations completed Progress: 207 iterations completed Progress: 208 iterations completed Progress: 209 iterations completed Progress: 210 iterations completed Progress: 211 iterations completed Progress: 212 iterations completed Progress: 213 iterations completed Progress: 214 iterations completed Progress: 215 iterations completed Progress: 216 iterations completed Progress: 217 iterations completed Progress: 218 iterations completed Progress: 219 iterations completed Progress: 220 iterations completed Progress: 221 iterations completed Progress: 222 iterations completed Progress: 223 iterations completed Progress: 224 iterations completed Progress: 225 iterations completed Progress: 226 iterations completed Progress: 227 iterations completed Progress: 228 iterations completed Progress: 229 iterations completed Progress: 230 iterations completed Progress: 231 iterations completed Progress: 232 iterations completed Progress: 233 iterations completed Progress: 234 iterations completed Progress: 235 iterations completed Progress: 236 iterations completed Progress: 237 iterations completed Progress: 238 iterations completed Progress: 239 iterations completed Progress: 240 iterations completed Progress: 241 iterations completed Progress: 242 iterations completed Progress: 243 iterations completed Progress: 244 iterations completed Progress: 245 iterations completed Progress: 246 iterations completed Progress: 247 iterations completed Progress: 248 iterations completed Progress: 249 iterations completed Progress: 250 iterations completed Progress: 251 iterations completed Progress: 252 iterations completed Progress: 253 iterations completed Progress: 254 iterations completed Progress: 255 iterations completed Progress: 256 iterations completed Progress: 257 iterations completed Progress: 258 iterations completed Progress: 259 iterations completed Progress: 260 iterations completed Progress: 261 iterations completed Progress: 262 iterations completed Progress: 263 iterations completed Progress: 264 iterations completed Progress: 265 iterations completed Progress: 266 iterations completed Progress: 267 iterations completed Progress: 268 iterations completed Progress: 269 iterations completed Progress: 270 iterations completed Progress: 271 iterations completed Progress: 272 iterations completed Progress: 273 iterations completed Progress: 274 iterations completed Progress: 275 iterations completed Progress: 276 iterations completed Progress: 277 iterations completed Progress: 278 iterations completed Progress: 279 iterations completed Progress: 280 iterations completed Progress: 281 iterations completed Progress: 282 iterations completed Progress: 283 iterations completed Progress: 284 iterations completed Progress: 285 iterations completed Progress: 286 iterations completed Progress: 287 iterations completed Progress: 288 iterations completed Progress: 289 iterations completed Progress: 290 iterations completed Progress: 291 iterations completed Progress: 292 iterations completed Progress: 293 iterations completed Progress: 294 iterations completed Progress: 295 iterations completed Progress: 296 iterations completed Progress: 297 iterations completed Progress: 298 iterations completed Progress: 299 iterations completed Progress: 300 iterations completed Progress: 301 iterations completed Progress: 302 iterations completed Progress: 303 iterations completed Progress: 304 iterations completed Progress: 305 iterations completed Progress: 306 iterations completed Progress: 307 iterations completed Progress: 308 iterations completed Progress: 309 iterations completed Progress: 310 iterations completed Progress: 311 iterations completed Progress: 312 iterations completed Progress: 313 iterations completed Progress: 314 iterations completed Progress: 315 iterations completed Progress: 316 iterations completed Progress: 317 iterations completed Progress: 318 iterations completed Progress: 319 iterations completed Progress: 320 iterations completed Progress: 321 iterations completed Progress: 322 iterations completed Progress: 323 iterations completed Progress: 324 iterations completed Progress: 325 iterations completed Progress: 326 iterations completed Progress: 327 iterations completed Progress: 328 iterations completed Progress: 329 iterations completed Progress: 330 iterations completed Progress: 331 iterations completed Progress: 332 iterations completed Progress: 333 iterations completed Progress: 334 iterations completed Progress: 335 iterations completed Progress: 336 iterations completed Progress: 337 iterations completed Progress: 338 iterations completed Progress: 339 iterations completed Progress: 340 iterations completed Progress: 341 iterations completed Progress: 342 iterations completed Progress: 343 iterations completed Progress: 344 iterations completed Progress: 345 iterations completed Progress: 346 iterations completed Progress: 347 iterations completed Progress: 348 iterations completed Progress: 349 iterations completed Progress: 350 iterations completed Progress: 351 iterations completed Progress: 352 iterations completed Progress: 353 iterations completed Progress: 354 iterations completed Progress: 355 iterations completed Progress: 356 iterations completed Progress: 357 iterations completed Progress: 358 iterations completed Progress: 359 iterations completed Progress: 360 iterations completed Progress: 361 iterations completed Progress: 362 iterations completed Progress: 363 iterations completed Progress: 364 iterations completed Progress: 365 iterations completed Progress: 366 iterations completed Progress: 367 iterations completed Progress: 368 iterations completed Progress: 369 iterations completed Progress: 370 iterations completed Progress: 371 iterations completed Progress: 372 iterations completed Progress: 373 iterations completed Progress: 374 iterations completed Progress: 375 iterations completed Progress: 376 iterations completed Progress: 377 iterations completed Progress: 378 iterations completed Progress: 379 iterations completed Progress: 380 iterations completed Progress: 381 iterations completed Progress: 382 iterations completed Progress: 383 iterations completed Progress: 384 iterations completed Progress: 385 iterations completed Progress: 386 iterations completed Progress: 387 iterations completed Progress: 388 iterations completed Progress: 389 iterations completed Progress: 390 iterations completed Progress: 391 iterations completed Progress: 392 iterations completed Progress: 393 iterations completed Progress: 394 iterations completed Progress: 395 iterations completed Progress: 396 iterations completed Progress: 397 iterations completed Progress: 398 iterations completed Progress: 399 iterations completed Progress: 400 iterations completed Progress: 401 iterations completed Progress: 402 iterations completed Progress: 403 iterations completed Progress: 404 iterations completed Progress: 405 iterations completed Progress: 406 iterations completed Progress: 407 iterations completed Progress: 408 iterations completed Progress: 409 iterations completed Progress: 410 iterations completed Progress: 411 iterations completed Progress: 412 iterations completed Progress: 413 iterations completed Progress: 414 iterations completed Progress: 415 iterations completed Progress: 416 iterations completed Progress: 417 iterations completed Progress: 418 iterations completed Progress: 419 iterations completed Progress: 420 iterations completed Progress: 421 iterations completed Progress: 422 iterations completed Progress: 423 iterations completed Progress: 424 iterations completed Progress: 425 iterations completed Progress: 426 iterations completed Progress: 427 iterations completed Progress: 428 iterations completed Progress: 429 iterations completed Progress: 430 iterations completed Progress: 431 iterations completed Progress: 432 iterations completed Progress: 433 iterations completed Progress: 434 iterations completed Progress: 435 iterations completed Progress: 436 iterations completed Progress: 437 iterations completed Progress: 438 iterations completed Progress: 439 iterations completed Progress: 440 iterations completed Progress: 441 iterations completed Progress: 442 iterations completed Progress: 443 iterations completed Progress: 444 iterations completed Progress: 445 iterations completed Progress: 446 iterations completed Progress: 447 iterations completed Progress: 448 iterations completed Progress: 449 iterations completed Progress: 450 iterations completed Progress: 451 iterations completed Progress: 452 iterations completed Progress: 453 iterations completed Progress: 454 iterations completed Progress: 455 iterations completed Progress: 456 iterations completed Progress: 457 iterations completed Progress: 458 iterations completed Progress: 459 iterations completed Progress: 460 iterations completed Progress: 461 iterations completed Progress: 462 iterations completed Progress: 463 iterations completed Progress: 464 iterations completed Progress: 465 iterations completed Progress: 466 iterations completed Progress: 467 iterations completed Progress: 468 iterations completed Progress: 469 iterations completed Progress: 470 iterations completed Progress: 471 iterations completed Progress: 472 iterations completed Progress: 473 iterations completed Progress: 474 iterations completed Progress: 475 iterations completed Progress: 476 iterations completed Progress: 477 iterations completed Progress: 478 iterations completed Progress: 479 iterations completed Progress: 480 iterations completed Progress: 481 iterations completed Progress: 482 iterations completed Progress: 483 iterations completed Progress: 484 iterations completed Progress: 485 iterations completed Progress: 486 iterations completed Progress: 487 iterations completed Progress: 488 iterations completed Progress: 489 iterations completed Progress: 490 iterations completed Progress: 491 iterations completed Progress: 492 iterations completed Progress: 493 iterations completed Progress: 494 iterations completed Progress: 495 iterations completed Progress: 496 iterations completed Progress: 497 iterations completed Progress: 498 iterations completed Progress: 499 iterations completed Progress: 500 iterations completed Progress: 501 iterations completed Progress: 502 iterations completed Progress: 503 iterations completed Progress: 504 iterations completed Progress: 505 iterations completed Progress: 506 iterations completed Progress: 507 iterations completed Progress: 508 iterations completed Progress: 509 iterations completed Progress: 510 iterations completed Progress: 511 iterations completed Progress: 512 iterations completed Progress: 513 iterations completed Progress: 514 iterations completed Progress: 515 iterations completed Progress: 516 iterations completed Progress: 517 iterations completed Progress: 518 iterations completed Progress: 519 iterations completed Progress: 520 iterations completed Progress: 521 iterations completed Progress: 522 iterations completed Progress: 523 iterations completed Progress: 524 iterations completed Progress: 525 iterations completed Progress: 526 iterations completed Progress: 527 iterations completed Progress: 528 iterations completed Progress: 529 iterations completed Progress: 530 iterations completed Progress: 531 iterations completed Progress: 532 iterations completed Progress: 533 iterations completed Progress: 534 iterations completed Progress: 535 iterations completed Progress: 536 iterations completed Progress: 537 iterations completed Progress: 538 iterations completed Progress: 539 iterations completed Progress: 540 iterations completed Progress: 541 iterations completed Progress: 542 iterations completed Progress: 543 iterations completed Progress: 544 iterations completed Progress: 545 iterations completed Progress: 546 iterations completed Progress: 547 iterations completed Progress: 548 iterations completed Progress: 549 iterations completed Progress: 550 iterations completed Progress: 551 iterations completed Progress: 552 iterations completed Progress: 553 iterations completed Progress: 554 iterations completed Progress: 555 iterations completed Progress: 556 iterations completed Progress: 557 iterations completed Progress: 558 iterations completed Progress: 559 iterations completed Progress: 560 iterations completed Progress: 561 iterations completed Progress: 562 iterations completed Progress: 563 iterations completed Progress: 564 iterations completed Progress: 565 iterations completed Progress: 566 iterations completed Progress: 567 iterations completed Progress: 568 iterations completed Progress: 569 iterations completed Progress: 570 iterations completed Progress: 571 iterations completed Progress: 572 iterations completed Progress: 573 iterations completed Progress: 574 iterations completed Progress: 575 iterations completed Progress: 576 iterations completed Progress: 577 iterations completed Progress: 578 iterations completed Progress: 579 iterations completed Progress: 580 iterations completed Progress: 581 iterations completed Progress: 582 iterations completed Progress: 583 iterations completed Progress: 584 iterations completed Progress: 585 iterations completed Progress: 586 iterations completed Progress: 587 iterations completed Progress: 588 iterations completed Progress: 589 iterations completed Progress: 590 iterations completed Progress: 591 iterations completed Progress: 592 iterations completed Progress: 593 iterations completed Progress: 594 iterations completed Progress: 595 iterations completed Progress: 596 iterations completed Progress: 597 iterations completed Progress: 598 iterations completed Progress: 599 iterations completed Progress: 600 iterations completed Progress: 601 iterations completed Progress: 602 iterations completed Progress: 603 iterations completed Progress: 604 iterations completed Progress: 605 iterations completed Progress: 606 iterations completed Progress: 607 iterations completed Progress: 608 iterations completed Progress: 609 iterations completed Progress: 610 iterations completed Progress: 611 iterations completed Progress: 612 iterations completed Progress: 613 iterations completed Progress: 614 iterations completed Progress: 615 iterations completed Progress: 616 iterations completed Progress: 617 iterations completed Progress: 618 iterations completed Progress: 619 iterations completed Progress: 620 iterations completed Progress: 621 iterations completed Progress: 622 iterations completed Progress: 623 iterations completed Progress: 624 iterations completed Progress: 625 iterations completed Progress: 626 iterations completed Progress: 627 iterations completed Progress: 628 iterations completed Progress: 629 iterations completed Progress: 630 iterations completed Progress: 631 iterations completed Progress: 632 iterations completed Progress: 633 iterations completed Progress: 634 iterations completed Progress: 635 iterations completed Progress: 636 iterations completed Progress: 637 iterations completed Progress: 638 iterations completed Progress: 639 iterations completed Progress: 640 iterations completed Progress: 641 iterations completed Progress: 642 iterations completed Progress: 643 iterations completed Progress: 644 iterations completed Progress: 645 iterations completed Progress: 646 iterations completed Progress: 647 iterations completed Progress: 648 iterations completed Progress: 649 iterations completed Progress: 650 iterations completed Progress: 651 iterations completed Progress: 652 iterations completed Progress: 653 iterations completed Progress: 654 iterations completed Progress: 655 iterations completed Progress: 656 iterations completed Progress: 657 iterations completed Progress: 658 iterations completed Progress: 659 iterations completed Progress: 660 iterations completed Progress: 661 iterations completed Progress: 662 iterations completed Progress: 663 iterations completed Progress: 664 iterations completed Progress: 665 iterations completed Progress: 666 iterations completed Progress: 667 iterations completed Progress: 668 iterations completed Progress: 669 iterations completed Progress: 670 iterations completed Progress: 671 iterations completed Progress: 672 iterations completed Progress: 673 iterations completed Progress: 674 iterations completed Progress: 675 iterations completed Progress: 676 iterations completed Progress: 677 iterations completed Progress: 678 iterations completed Progress: 679 iterations completed Progress: 680 iterations completed Progress: 681 iterations completed Progress: 682 iterations completed Progress: 683 iterations completed Progress: 684 iterations completed Progress: 685 iterations completed Progress: 686 iterations completed Progress: 687 iterations completed Progress: 688 iterations completed Progress: 689 iterations completed Progress: 690 iterations completed Progress: 691 iterations completed Progress: 692 iterations completed Progress: 693 iterations completed Progress: 694 iterations completed Progress: 695 iterations completed Progress: 696 iterations completed Progress: 697 iterations completed Progress: 698 iterations completed Progress: 699 iterations completed Progress: 700 iterations completed Progress: 701 iterations completed Progress: 702 iterations completed Progress: 703 iterations completed Progress: 704 iterations completed Progress: 705 iterations completed Progress: 706 iterations completed Progress: 707 iterations completed Progress: 708 iterations completed Progress: 709 iterations completed Progress: 710 iterations completed Progress: 711 iterations completed Progress: 712 iterations completed Progress: 713 iterations completed Progress: 714 iterations completed Progress: 715 iterations completed Progress: 716 iterations completed Progress: 717 iterations completed Progress: 718 iterations completed Progress: 719 iterations completed Progress: 720 iterations completed Progress: 721 iterations completed Progress: 722 iterations completed Progress: 723 iterations completed Progress: 724 iterations completed Progress: 725 iterations completed Progress: 726 iterations completed Progress: 727 iterations completed Progress: 728 iterations completed Progress: 729 iterations completed Progress: 730 iterations completed Progress: 731 iterations completed Progress: 732 iterations completed Progress: 733 iterations completed Progress: 734 iterations completed Progress: 735 iterations completed Progress: 736 iterations completed Progress: 737 iterations completed Progress: 738 iterations completed Progress: 739 iterations completed Progress: 740 iterations completed Progress: 741 iterations completed Progress: 742 iterations completed Progress: 743 iterations completed Progress: 744 iterations completed Progress: 745 iterations completed Progress: 746 iterations completed Progress: 747 iterations completed Progress: 748 iterations completed Progress: 749 iterations completed Progress: 750 iterations completed Progress: 751 iterations completed Progress: 752 iterations completed Progress: 753 iterations completed Progress: 754 iterations completed Progress: 755 iterations completed Progress: 756 iterations completed Progress: 757 iterations completed Progress: 758 iterations completed Progress: 759 iterations completed Progress: 760 iterations completed Progress: 761 iterations completed Progress: 762 iterations completed Progress: 763 iterations completed Progress: 764 iterations completed Progress: 765 iterations completed Progress: 766 iterations completed Progress: 767 iterations completed Progress: 768 iterations completed Progress: 769 iterations completed Progress: 770 iterations completed Progress: 771 iterations completed Progress: 772 iterations completed Progress: 773 iterations completed Progress: 774 iterations completed Progress: 775 iterations completed Progress: 776 iterations completed Progress: 777 iterations completed Progress: 778 iterations completed Progress: 779 iterations completed Progress: 780 iterations completed Progress: 781 iterations completed Progress: 782 iterations completed Progress: 783 iterations completed Progress: 784 iterations completed Progress: 785 iterations completed Progress: 786 iterations completed Progress: 787 iterations completed Progress: 788 iterations completed Progress: 789 iterations completed Progress: 790 iterations completed Progress: 791 iterations completed Progress: 792 iterations completed Progress: 793 iterations completed Progress: 794 iterations completed Progress: 795 iterations completed Progress: 796 iterations completed Progress: 797 iterations completed Progress: 798 iterations completed Progress: 799 iterations completed Progress: 800 iterations completed Progress: 801 iterations completed Progress: 802 iterations completed Progress: 803 iterations completed Progress: 804 iterations completed Progress: 805 iterations completed Progress: 806 iterations completed Progress: 807 iterations completed Progress: 808 iterations completed Progress: 809 iterations completed Progress: 810 iterations completed Progress: 811 iterations completed Progress: 812 iterations completed Progress: 813 iterations completed Progress: 814 iterations completed Progress: 815 iterations completed Progress: 816 iterations completed Progress: 817 iterations completed Progress: 818 iterations completed Progress: 819 iterations completed Progress: 820 iterations completed Progress: 821 iterations completed Progress: 822 iterations completed Progress: 823 iterations completed Progress: 824 iterations completed Progress: 825 iterations completed Progress: 826 iterations completed Progress: 827 iterations completed Progress: 828 iterations completed Progress: 829 iterations completed Progress: 830 iterations completed Progress: 831 iterations completed Progress: 832 iterations completed Progress: 833 iterations completed Progress: 834 iterations completed Progress: 835 iterations completed Progress: 836 iterations completed Progress: 837 iterations completed Progress: 838 iterations completed Progress: 839 iterations completed Progress: 840 iterations completed Progress: 841 iterations completed Progress: 842 iterations completed Progress: 843 iterations completed Progress: 844 iterations completed Progress: 845 iterations completed Progress: 846 iterations completed Progress: 847 iterations completed Progress: 848 iterations completed Progress: 849 iterations completed Progress: 850 iterations completed Progress: 851 iterations completed Progress: 852 iterations completed Progress: 853 iterations completed Progress: 854 iterations completed Progress: 855 iterations completed Progress: 856 iterations completed Progress: 857 iterations completed Progress: 858 iterations completed Progress: 859 iterations completed Progress: 860 iterations completed Progress: 861 iterations completed Progress: 862 iterations completed Progress: 863 iterations completed Progress: 864 iterations completed Progress: 865 iterations completed Progress: 866 iterations completed Progress: 867 iterations completed Progress: 868 iterations completed Progress: 869 iterations completed Progress: 870 iterations completed Progress: 871 iterations completed Progress: 872 iterations completed Progress: 873 iterations completed Progress: 874 iterations completed Progress: 875 iterations completed Progress: 876 iterations completed Progress: 877 iterations completed Progress: 878 iterations completed Progress: 879 iterations completed Progress: 880 iterations completed Progress: 881 iterations completed Progress: 882 iterations completed Progress: 883 iterations completed Progress: 884 iterations completed Progress: 885 iterations completed Progress: 886 iterations completed Progress: 887 iterations completed Progress: 888 iterations completed Progress: 889 iterations completed Progress: 890 iterations completed Progress: 891 iterations completed Progress: 892 iterations completed Progress: 893 iterations completed Progress: 894 iterations completed Progress: 895 iterations completed Progress: 896 iterations completed Progress: 897 iterations completed Progress: 898 iterations completed Progress: 899 iterations completed Progress: 900 iterations completed Progress: 901 iterations completed Progress: 902 iterations completed Progress: 903 iterations completed Progress: 904 iterations completed Progress: 905 iterations completed Progress: 906 iterations completed Progress: 907 iterations completed Progress: 908 iterations completed Progress: 909 iterations completed Progress: 910 iterations completed Progress: 911 iterations completed Progress: 912 iterations completed Progress: 913 iterations completed Progress: 914 iterations completed Progress: 915 iterations completed Progress: 916 iterations completed Progress: 917 iterations completed Progress: 918 iterations completed Progress: 919 iterations completed Progress: 920 iterations completed Progress: 921 iterations completed Progress: 922 iterations completed Progress: 923 iterations completed Progress: 924 iterations completed Progress: 925 iterations completed Progress: 926 iterations completed Progress: 927 iterations completed Progress: 928 iterations completed Progress: 929 iterations completed Progress: 930 iterations completed Progress: 931 iterations completed Progress: 932 iterations completed Progress: 933 iterations completed Progress: 934 iterations completed Progress: 935 iterations completed Progress: 936 iterations completed Progress: 937 iterations completed Progress: 938 iterations completed Progress: 939 iterations completed Progress: 940 iterations completed Progress: 941 iterations completed Progress: 942 iterations completed Progress: 943 iterations completed Progress: 944 iterations completed Progress: 945 iterations completed Progress: 946 iterations completed Progress: 947 iterations completed Progress: 948 iterations completed Progress: 949 iterations completed Progress: 950 iterations completed Progress: 951 iterations completed Progress: 952 iterations completed Progress: 953 iterations completed Progress: 954 iterations completed Progress: 955 iterations completed Progress: 956 iterations completed Progress: 957 iterations completed Progress: 958 iterations completed Progress: 959 iterations completed Progress: 960 iterations completed Progress: 961 iterations completed Progress: 962 iterations completed Progress: 963 iterations completed Progress: 964 iterations completed Progress: 965 iterations completed Progress: 966 iterations completed Progress: 967 iterations completed Progress: 968 iterations completed Progress: 969 iterations completed Progress: 970 iterations completed Progress: 971 iterations completed Progress: 972 iterations completed Progress: 973 iterations completed Progress: 974 iterations completed Progress: 975 iterations completed Progress: 976 iterations completed Progress: 977 iterations completed Progress: 978 iterations completed Progress: 979 iterations completed Progress: 980 iterations completed Progress: 981 iterations completed Progress: 982 iterations completed Progress: 983 iterations completed Progress: 984 iterations completed Progress: 985 iterations completed Progress: 986 iterations completed Progress: 987 iterations completed Progress: 988 iterations completed Progress: 989 iterations completed Progress: 990 iterations completed Progress: 991 iterations completed Progress: 992 iterations completed Progress: 993 iterations completed Progress: 994 iterations completed Progress: 995 iterations completed Progress: 996 iterations completed Progress: 997 iterations completed Progress: 998 iterations completed Progress: 999 iterations completed Progress: 1000 iterations completed # Calculate mean RMSPE for the ensemble with all four models mean_rmspe_ensemble_four &lt;- mean(rmspe_ensemble_four) # Print the results print(paste(&quot;Mean RMSPE of Stacked Ensemble with All Four Models:&quot;, mean_rmspe_ensemble_four)) ## [1] &quot;Mean RMSPE of Stacked Ensemble with All Four Models: 0.336746256053834&quot; So we do see a better rmspe probably due to having more base models. so lets look at the covariance matrix of the base models to see if they are correlated # Initialize the matrix to accumulate correlations cor_matrix_sum &lt;- matrix(0, nrow = 4, ncol = 4) for(i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) # Train both GBM models on the bootstrapped training data base_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1, verbose = FALSE) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) base_gbm_pred &lt;- predict(base_gbm_mod, test_data, n.trees = 100) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, base_gbm_pred, tuned_gbm_pred) # Calculate the correlation matrix for the current iteration cor_matrix &lt;- cor(preds_four) # Accumulate the correlation matrices cor_matrix_sum &lt;- cor_matrix_sum + cor_matrix } ## Progress: 1 iterations completed Progress: 2 iterations completed Progress: 3 iterations completed Progress: 4 iterations completed Progress: 5 iterations completed Progress: 6 iterations completed Progress: 7 iterations completed Progress: 8 iterations completed Progress: 9 iterations completed Progress: 10 iterations completed Progress: 11 iterations completed Progress: 12 iterations completed Progress: 13 iterations completed Progress: 14 iterations completed Progress: 15 iterations completed Progress: 16 iterations completed Progress: 17 iterations completed Progress: 18 iterations completed Progress: 19 iterations completed Progress: 20 iterations completed Progress: 21 iterations completed Progress: 22 iterations completed Progress: 23 iterations completed Progress: 24 iterations completed Progress: 25 iterations completed Progress: 26 iterations completed Progress: 27 iterations completed Progress: 28 iterations completed Progress: 29 iterations completed Progress: 30 iterations completed Progress: 31 iterations completed Progress: 32 iterations completed Progress: 33 iterations completed Progress: 34 iterations completed Progress: 35 iterations completed Progress: 36 iterations completed Progress: 37 iterations completed Progress: 38 iterations completed Progress: 39 iterations completed Progress: 40 iterations completed Progress: 41 iterations completed Progress: 42 iterations completed Progress: 43 iterations completed Progress: 44 iterations completed Progress: 45 iterations completed Progress: 46 iterations completed Progress: 47 iterations completed Progress: 48 iterations completed Progress: 49 iterations completed Progress: 50 iterations completed Progress: 51 iterations completed Progress: 52 iterations completed Progress: 53 iterations completed Progress: 54 iterations completed Progress: 55 iterations completed Progress: 56 iterations completed Progress: 57 iterations completed Progress: 58 iterations completed Progress: 59 iterations completed Progress: 60 iterations completed Progress: 61 iterations completed Progress: 62 iterations completed Progress: 63 iterations completed Progress: 64 iterations completed Progress: 65 iterations completed Progress: 66 iterations completed Progress: 67 iterations completed Progress: 68 iterations completed Progress: 69 iterations completed Progress: 70 iterations completed Progress: 71 iterations completed Progress: 72 iterations completed Progress: 73 iterations completed Progress: 74 iterations completed Progress: 75 iterations completed Progress: 76 iterations completed Progress: 77 iterations completed Progress: 78 iterations completed Progress: 79 iterations completed Progress: 80 iterations completed Progress: 81 iterations completed Progress: 82 iterations completed Progress: 83 iterations completed Progress: 84 iterations completed Progress: 85 iterations completed Progress: 86 iterations completed Progress: 87 iterations completed Progress: 88 iterations completed Progress: 89 iterations completed Progress: 90 iterations completed Progress: 91 iterations completed Progress: 92 iterations completed Progress: 93 iterations completed Progress: 94 iterations completed Progress: 95 iterations completed Progress: 96 iterations completed Progress: 97 iterations completed Progress: 98 iterations completed Progress: 99 iterations completed Progress: 100 iterations completed Progress: 101 iterations completed Progress: 102 iterations completed Progress: 103 iterations completed Progress: 104 iterations completed Progress: 105 iterations completed Progress: 106 iterations completed Progress: 107 iterations completed Progress: 108 iterations completed Progress: 109 iterations completed Progress: 110 iterations completed Progress: 111 iterations completed Progress: 112 iterations completed Progress: 113 iterations completed Progress: 114 iterations completed Progress: 115 iterations completed Progress: 116 iterations completed Progress: 117 iterations completed Progress: 118 iterations completed Progress: 119 iterations completed Progress: 120 iterations completed Progress: 121 iterations completed Progress: 122 iterations completed Progress: 123 iterations completed Progress: 124 iterations completed Progress: 125 iterations completed Progress: 126 iterations completed Progress: 127 iterations completed Progress: 128 iterations completed Progress: 129 iterations completed Progress: 130 iterations completed Progress: 131 iterations completed Progress: 132 iterations completed Progress: 133 iterations completed Progress: 134 iterations completed Progress: 135 iterations completed Progress: 136 iterations completed Progress: 137 iterations completed Progress: 138 iterations completed Progress: 139 iterations completed Progress: 140 iterations completed Progress: 141 iterations completed Progress: 142 iterations completed Progress: 143 iterations completed Progress: 144 iterations completed Progress: 145 iterations completed Progress: 146 iterations completed Progress: 147 iterations completed Progress: 148 iterations completed Progress: 149 iterations completed Progress: 150 iterations completed Progress: 151 iterations completed Progress: 152 iterations completed Progress: 153 iterations completed Progress: 154 iterations completed Progress: 155 iterations completed Progress: 156 iterations completed Progress: 157 iterations completed Progress: 158 iterations completed Progress: 159 iterations completed Progress: 160 iterations completed Progress: 161 iterations completed Progress: 162 iterations completed Progress: 163 iterations completed Progress: 164 iterations completed Progress: 165 iterations completed Progress: 166 iterations completed Progress: 167 iterations completed Progress: 168 iterations completed Progress: 169 iterations completed Progress: 170 iterations completed Progress: 171 iterations completed Progress: 172 iterations completed Progress: 173 iterations completed Progress: 174 iterations completed Progress: 175 iterations completed Progress: 176 iterations completed Progress: 177 iterations completed Progress: 178 iterations completed Progress: 179 iterations completed Progress: 180 iterations completed Progress: 181 iterations completed Progress: 182 iterations completed Progress: 183 iterations completed Progress: 184 iterations completed Progress: 185 iterations completed Progress: 186 iterations completed Progress: 187 iterations completed Progress: 188 iterations completed Progress: 189 iterations completed Progress: 190 iterations completed Progress: 191 iterations completed Progress: 192 iterations completed Progress: 193 iterations completed Progress: 194 iterations completed Progress: 195 iterations completed Progress: 196 iterations completed Progress: 197 iterations completed Progress: 198 iterations completed Progress: 199 iterations completed Progress: 200 iterations completed Progress: 201 iterations completed Progress: 202 iterations completed Progress: 203 iterations completed Progress: 204 iterations completed Progress: 205 iterations completed Progress: 206 iterations completed Progress: 207 iterations completed Progress: 208 iterations completed Progress: 209 iterations completed Progress: 210 iterations completed Progress: 211 iterations completed Progress: 212 iterations completed Progress: 213 iterations completed Progress: 214 iterations completed Progress: 215 iterations completed Progress: 216 iterations completed Progress: 217 iterations completed Progress: 218 iterations completed Progress: 219 iterations completed Progress: 220 iterations completed Progress: 221 iterations completed Progress: 222 iterations completed Progress: 223 iterations completed Progress: 224 iterations completed Progress: 225 iterations completed Progress: 226 iterations completed Progress: 227 iterations completed Progress: 228 iterations completed Progress: 229 iterations completed Progress: 230 iterations completed Progress: 231 iterations completed Progress: 232 iterations completed Progress: 233 iterations completed Progress: 234 iterations completed Progress: 235 iterations completed Progress: 236 iterations completed Progress: 237 iterations completed Progress: 238 iterations completed Progress: 239 iterations completed Progress: 240 iterations completed Progress: 241 iterations completed Progress: 242 iterations completed Progress: 243 iterations completed Progress: 244 iterations completed Progress: 245 iterations completed Progress: 246 iterations completed Progress: 247 iterations completed Progress: 248 iterations completed Progress: 249 iterations completed Progress: 250 iterations completed Progress: 251 iterations completed Progress: 252 iterations completed Progress: 253 iterations completed Progress: 254 iterations completed Progress: 255 iterations completed Progress: 256 iterations completed Progress: 257 iterations completed Progress: 258 iterations completed Progress: 259 iterations completed Progress: 260 iterations completed Progress: 261 iterations completed Progress: 262 iterations completed Progress: 263 iterations completed Progress: 264 iterations completed Progress: 265 iterations completed Progress: 266 iterations completed Progress: 267 iterations completed Progress: 268 iterations completed Progress: 269 iterations completed Progress: 270 iterations completed Progress: 271 iterations completed Progress: 272 iterations completed Progress: 273 iterations completed Progress: 274 iterations completed Progress: 275 iterations completed Progress: 276 iterations completed Progress: 277 iterations completed Progress: 278 iterations completed Progress: 279 iterations completed Progress: 280 iterations completed Progress: 281 iterations completed Progress: 282 iterations completed Progress: 283 iterations completed Progress: 284 iterations completed Progress: 285 iterations completed Progress: 286 iterations completed Progress: 287 iterations completed Progress: 288 iterations completed Progress: 289 iterations completed Progress: 290 iterations completed Progress: 291 iterations completed Progress: 292 iterations completed Progress: 293 iterations completed Progress: 294 iterations completed Progress: 295 iterations completed Progress: 296 iterations completed Progress: 297 iterations completed Progress: 298 iterations completed Progress: 299 iterations completed Progress: 300 iterations completed Progress: 301 iterations completed Progress: 302 iterations completed Progress: 303 iterations completed Progress: 304 iterations completed Progress: 305 iterations completed Progress: 306 iterations completed Progress: 307 iterations completed Progress: 308 iterations completed Progress: 309 iterations completed Progress: 310 iterations completed Progress: 311 iterations completed Progress: 312 iterations completed Progress: 313 iterations completed Progress: 314 iterations completed Progress: 315 iterations completed Progress: 316 iterations completed Progress: 317 iterations completed Progress: 318 iterations completed Progress: 319 iterations completed Progress: 320 iterations completed Progress: 321 iterations completed Progress: 322 iterations completed Progress: 323 iterations completed Progress: 324 iterations completed Progress: 325 iterations completed Progress: 326 iterations completed Progress: 327 iterations completed Progress: 328 iterations completed Progress: 329 iterations completed Progress: 330 iterations completed Progress: 331 iterations completed Progress: 332 iterations completed Progress: 333 iterations completed Progress: 334 iterations completed Progress: 335 iterations completed Progress: 336 iterations completed Progress: 337 iterations completed Progress: 338 iterations completed Progress: 339 iterations completed Progress: 340 iterations completed Progress: 341 iterations completed Progress: 342 iterations completed Progress: 343 iterations completed Progress: 344 iterations completed Progress: 345 iterations completed Progress: 346 iterations completed Progress: 347 iterations completed Progress: 348 iterations completed Progress: 349 iterations completed Progress: 350 iterations completed Progress: 351 iterations completed Progress: 352 iterations completed Progress: 353 iterations completed Progress: 354 iterations completed Progress: 355 iterations completed Progress: 356 iterations completed Progress: 357 iterations completed Progress: 358 iterations completed Progress: 359 iterations completed Progress: 360 iterations completed Progress: 361 iterations completed Progress: 362 iterations completed Progress: 363 iterations completed Progress: 364 iterations completed Progress: 365 iterations completed Progress: 366 iterations completed Progress: 367 iterations completed Progress: 368 iterations completed Progress: 369 iterations completed Progress: 370 iterations completed Progress: 371 iterations completed Progress: 372 iterations completed Progress: 373 iterations completed Progress: 374 iterations completed Progress: 375 iterations completed Progress: 376 iterations completed Progress: 377 iterations completed Progress: 378 iterations completed Progress: 379 iterations completed Progress: 380 iterations completed Progress: 381 iterations completed Progress: 382 iterations completed Progress: 383 iterations completed Progress: 384 iterations completed Progress: 385 iterations completed Progress: 386 iterations completed Progress: 387 iterations completed Progress: 388 iterations completed Progress: 389 iterations completed Progress: 390 iterations completed Progress: 391 iterations completed Progress: 392 iterations completed Progress: 393 iterations completed Progress: 394 iterations completed Progress: 395 iterations completed Progress: 396 iterations completed Progress: 397 iterations completed Progress: 398 iterations completed Progress: 399 iterations completed Progress: 400 iterations completed Progress: 401 iterations completed Progress: 402 iterations completed Progress: 403 iterations completed Progress: 404 iterations completed Progress: 405 iterations completed Progress: 406 iterations completed Progress: 407 iterations completed Progress: 408 iterations completed Progress: 409 iterations completed Progress: 410 iterations completed Progress: 411 iterations completed Progress: 412 iterations completed Progress: 413 iterations completed Progress: 414 iterations completed Progress: 415 iterations completed Progress: 416 iterations completed Progress: 417 iterations completed Progress: 418 iterations completed Progress: 419 iterations completed Progress: 420 iterations completed Progress: 421 iterations completed Progress: 422 iterations completed Progress: 423 iterations completed Progress: 424 iterations completed Progress: 425 iterations completed Progress: 426 iterations completed Progress: 427 iterations completed Progress: 428 iterations completed Progress: 429 iterations completed Progress: 430 iterations completed Progress: 431 iterations completed Progress: 432 iterations completed Progress: 433 iterations completed Progress: 434 iterations completed Progress: 435 iterations completed Progress: 436 iterations completed Progress: 437 iterations completed Progress: 438 iterations completed Progress: 439 iterations completed Progress: 440 iterations completed Progress: 441 iterations completed Progress: 442 iterations completed Progress: 443 iterations completed Progress: 444 iterations completed Progress: 445 iterations completed Progress: 446 iterations completed Progress: 447 iterations completed Progress: 448 iterations completed Progress: 449 iterations completed Progress: 450 iterations completed Progress: 451 iterations completed Progress: 452 iterations completed Progress: 453 iterations completed Progress: 454 iterations completed Progress: 455 iterations completed Progress: 456 iterations completed Progress: 457 iterations completed Progress: 458 iterations completed Progress: 459 iterations completed Progress: 460 iterations completed Progress: 461 iterations completed Progress: 462 iterations completed Progress: 463 iterations completed Progress: 464 iterations completed Progress: 465 iterations completed Progress: 466 iterations completed Progress: 467 iterations completed Progress: 468 iterations completed Progress: 469 iterations completed Progress: 470 iterations completed Progress: 471 iterations completed Progress: 472 iterations completed Progress: 473 iterations completed Progress: 474 iterations completed Progress: 475 iterations completed Progress: 476 iterations completed Progress: 477 iterations completed Progress: 478 iterations completed Progress: 479 iterations completed Progress: 480 iterations completed Progress: 481 iterations completed Progress: 482 iterations completed Progress: 483 iterations completed Progress: 484 iterations completed Progress: 485 iterations completed Progress: 486 iterations completed Progress: 487 iterations completed Progress: 488 iterations completed Progress: 489 iterations completed Progress: 490 iterations completed Progress: 491 iterations completed Progress: 492 iterations completed Progress: 493 iterations completed Progress: 494 iterations completed Progress: 495 iterations completed Progress: 496 iterations completed Progress: 497 iterations completed Progress: 498 iterations completed Progress: 499 iterations completed Progress: 500 iterations completed Progress: 501 iterations completed Progress: 502 iterations completed Progress: 503 iterations completed Progress: 504 iterations completed Progress: 505 iterations completed Progress: 506 iterations completed Progress: 507 iterations completed Progress: 508 iterations completed Progress: 509 iterations completed Progress: 510 iterations completed Progress: 511 iterations completed Progress: 512 iterations completed Progress: 513 iterations completed Progress: 514 iterations completed Progress: 515 iterations completed Progress: 516 iterations completed Progress: 517 iterations completed Progress: 518 iterations completed Progress: 519 iterations completed Progress: 520 iterations completed Progress: 521 iterations completed Progress: 522 iterations completed Progress: 523 iterations completed Progress: 524 iterations completed Progress: 525 iterations completed Progress: 526 iterations completed Progress: 527 iterations completed Progress: 528 iterations completed Progress: 529 iterations completed Progress: 530 iterations completed Progress: 531 iterations completed Progress: 532 iterations completed Progress: 533 iterations completed Progress: 534 iterations completed Progress: 535 iterations completed Progress: 536 iterations completed Progress: 537 iterations completed Progress: 538 iterations completed Progress: 539 iterations completed Progress: 540 iterations completed Progress: 541 iterations completed Progress: 542 iterations completed Progress: 543 iterations completed Progress: 544 iterations completed Progress: 545 iterations completed Progress: 546 iterations completed Progress: 547 iterations completed Progress: 548 iterations completed Progress: 549 iterations completed Progress: 550 iterations completed Progress: 551 iterations completed Progress: 552 iterations completed Progress: 553 iterations completed Progress: 554 iterations completed Progress: 555 iterations completed Progress: 556 iterations completed Progress: 557 iterations completed Progress: 558 iterations completed Progress: 559 iterations completed Progress: 560 iterations completed Progress: 561 iterations completed Progress: 562 iterations completed Progress: 563 iterations completed Progress: 564 iterations completed Progress: 565 iterations completed Progress: 566 iterations completed Progress: 567 iterations completed Progress: 568 iterations completed Progress: 569 iterations completed Progress: 570 iterations completed Progress: 571 iterations completed Progress: 572 iterations completed Progress: 573 iterations completed Progress: 574 iterations completed Progress: 575 iterations completed Progress: 576 iterations completed Progress: 577 iterations completed Progress: 578 iterations completed Progress: 579 iterations completed Progress: 580 iterations completed Progress: 581 iterations completed Progress: 582 iterations completed Progress: 583 iterations completed Progress: 584 iterations completed Progress: 585 iterations completed Progress: 586 iterations completed Progress: 587 iterations completed Progress: 588 iterations completed Progress: 589 iterations completed Progress: 590 iterations completed Progress: 591 iterations completed Progress: 592 iterations completed Progress: 593 iterations completed Progress: 594 iterations completed Progress: 595 iterations completed Progress: 596 iterations completed Progress: 597 iterations completed Progress: 598 iterations completed Progress: 599 iterations completed Progress: 600 iterations completed Progress: 601 iterations completed Progress: 602 iterations completed Progress: 603 iterations completed Progress: 604 iterations completed Progress: 605 iterations completed Progress: 606 iterations completed Progress: 607 iterations completed Progress: 608 iterations completed Progress: 609 iterations completed Progress: 610 iterations completed Progress: 611 iterations completed Progress: 612 iterations completed Progress: 613 iterations completed Progress: 614 iterations completed Progress: 615 iterations completed Progress: 616 iterations completed Progress: 617 iterations completed Progress: 618 iterations completed Progress: 619 iterations completed Progress: 620 iterations completed Progress: 621 iterations completed Progress: 622 iterations completed Progress: 623 iterations completed Progress: 624 iterations completed Progress: 625 iterations completed Progress: 626 iterations completed Progress: 627 iterations completed Progress: 628 iterations completed Progress: 629 iterations completed Progress: 630 iterations completed Progress: 631 iterations completed Progress: 632 iterations completed Progress: 633 iterations completed Progress: 634 iterations completed Progress: 635 iterations completed Progress: 636 iterations completed Progress: 637 iterations completed Progress: 638 iterations completed Progress: 639 iterations completed Progress: 640 iterations completed Progress: 641 iterations completed Progress: 642 iterations completed Progress: 643 iterations completed Progress: 644 iterations completed Progress: 645 iterations completed Progress: 646 iterations completed Progress: 647 iterations completed Progress: 648 iterations completed Progress: 649 iterations completed Progress: 650 iterations completed Progress: 651 iterations completed Progress: 652 iterations completed Progress: 653 iterations completed Progress: 654 iterations completed Progress: 655 iterations completed Progress: 656 iterations completed Progress: 657 iterations completed Progress: 658 iterations completed Progress: 659 iterations completed Progress: 660 iterations completed Progress: 661 iterations completed Progress: 662 iterations completed Progress: 663 iterations completed Progress: 664 iterations completed Progress: 665 iterations completed Progress: 666 iterations completed Progress: 667 iterations completed Progress: 668 iterations completed Progress: 669 iterations completed Progress: 670 iterations completed Progress: 671 iterations completed Progress: 672 iterations completed Progress: 673 iterations completed Progress: 674 iterations completed Progress: 675 iterations completed Progress: 676 iterations completed Progress: 677 iterations completed Progress: 678 iterations completed Progress: 679 iterations completed Progress: 680 iterations completed Progress: 681 iterations completed Progress: 682 iterations completed Progress: 683 iterations completed Progress: 684 iterations completed Progress: 685 iterations completed Progress: 686 iterations completed Progress: 687 iterations completed Progress: 688 iterations completed Progress: 689 iterations completed Progress: 690 iterations completed Progress: 691 iterations completed Progress: 692 iterations completed Progress: 693 iterations completed Progress: 694 iterations completed Progress: 695 iterations completed Progress: 696 iterations completed Progress: 697 iterations completed Progress: 698 iterations completed Progress: 699 iterations completed Progress: 700 iterations completed Progress: 701 iterations completed Progress: 702 iterations completed Progress: 703 iterations completed Progress: 704 iterations completed Progress: 705 iterations completed Progress: 706 iterations completed Progress: 707 iterations completed Progress: 708 iterations completed Progress: 709 iterations completed Progress: 710 iterations completed Progress: 711 iterations completed Progress: 712 iterations completed Progress: 713 iterations completed Progress: 714 iterations completed Progress: 715 iterations completed Progress: 716 iterations completed Progress: 717 iterations completed Progress: 718 iterations completed Progress: 719 iterations completed Progress: 720 iterations completed Progress: 721 iterations completed Progress: 722 iterations completed Progress: 723 iterations completed Progress: 724 iterations completed Progress: 725 iterations completed Progress: 726 iterations completed Progress: 727 iterations completed Progress: 728 iterations completed Progress: 729 iterations completed Progress: 730 iterations completed Progress: 731 iterations completed Progress: 732 iterations completed Progress: 733 iterations completed Progress: 734 iterations completed Progress: 735 iterations completed Progress: 736 iterations completed Progress: 737 iterations completed Progress: 738 iterations completed Progress: 739 iterations completed Progress: 740 iterations completed Progress: 741 iterations completed Progress: 742 iterations completed Progress: 743 iterations completed Progress: 744 iterations completed Progress: 745 iterations completed Progress: 746 iterations completed Progress: 747 iterations completed Progress: 748 iterations completed Progress: 749 iterations completed Progress: 750 iterations completed Progress: 751 iterations completed Progress: 752 iterations completed Progress: 753 iterations completed Progress: 754 iterations completed Progress: 755 iterations completed Progress: 756 iterations completed Progress: 757 iterations completed Progress: 758 iterations completed Progress: 759 iterations completed Progress: 760 iterations completed Progress: 761 iterations completed Progress: 762 iterations completed Progress: 763 iterations completed Progress: 764 iterations completed Progress: 765 iterations completed Progress: 766 iterations completed Progress: 767 iterations completed Progress: 768 iterations completed Progress: 769 iterations completed Progress: 770 iterations completed Progress: 771 iterations completed Progress: 772 iterations completed Progress: 773 iterations completed Progress: 774 iterations completed Progress: 775 iterations completed Progress: 776 iterations completed Progress: 777 iterations completed Progress: 778 iterations completed Progress: 779 iterations completed Progress: 780 iterations completed Progress: 781 iterations completed Progress: 782 iterations completed Progress: 783 iterations completed Progress: 784 iterations completed Progress: 785 iterations completed Progress: 786 iterations completed Progress: 787 iterations completed Progress: 788 iterations completed Progress: 789 iterations completed Progress: 790 iterations completed Progress: 791 iterations completed Progress: 792 iterations completed Progress: 793 iterations completed Progress: 794 iterations completed Progress: 795 iterations completed Progress: 796 iterations completed Progress: 797 iterations completed Progress: 798 iterations completed Progress: 799 iterations completed Progress: 800 iterations completed Progress: 801 iterations completed Progress: 802 iterations completed Progress: 803 iterations completed Progress: 804 iterations completed Progress: 805 iterations completed Progress: 806 iterations completed Progress: 807 iterations completed Progress: 808 iterations completed Progress: 809 iterations completed Progress: 810 iterations completed Progress: 811 iterations completed Progress: 812 iterations completed Progress: 813 iterations completed Progress: 814 iterations completed Progress: 815 iterations completed Progress: 816 iterations completed Progress: 817 iterations completed Progress: 818 iterations completed Progress: 819 iterations completed Progress: 820 iterations completed Progress: 821 iterations completed Progress: 822 iterations completed Progress: 823 iterations completed Progress: 824 iterations completed Progress: 825 iterations completed Progress: 826 iterations completed Progress: 827 iterations completed Progress: 828 iterations completed Progress: 829 iterations completed Progress: 830 iterations completed Progress: 831 iterations completed Progress: 832 iterations completed Progress: 833 iterations completed Progress: 834 iterations completed Progress: 835 iterations completed Progress: 836 iterations completed Progress: 837 iterations completed Progress: 838 iterations completed Progress: 839 iterations completed Progress: 840 iterations completed Progress: 841 iterations completed Progress: 842 iterations completed Progress: 843 iterations completed Progress: 844 iterations completed Progress: 845 iterations completed Progress: 846 iterations completed Progress: 847 iterations completed Progress: 848 iterations completed Progress: 849 iterations completed Progress: 850 iterations completed Progress: 851 iterations completed Progress: 852 iterations completed Progress: 853 iterations completed Progress: 854 iterations completed Progress: 855 iterations completed Progress: 856 iterations completed Progress: 857 iterations completed Progress: 858 iterations completed Progress: 859 iterations completed Progress: 860 iterations completed Progress: 861 iterations completed Progress: 862 iterations completed Progress: 863 iterations completed Progress: 864 iterations completed Progress: 865 iterations completed Progress: 866 iterations completed Progress: 867 iterations completed Progress: 868 iterations completed Progress: 869 iterations completed Progress: 870 iterations completed Progress: 871 iterations completed Progress: 872 iterations completed Progress: 873 iterations completed Progress: 874 iterations completed Progress: 875 iterations completed Progress: 876 iterations completed Progress: 877 iterations completed Progress: 878 iterations completed Progress: 879 iterations completed Progress: 880 iterations completed Progress: 881 iterations completed Progress: 882 iterations completed Progress: 883 iterations completed Progress: 884 iterations completed Progress: 885 iterations completed Progress: 886 iterations completed Progress: 887 iterations completed Progress: 888 iterations completed Progress: 889 iterations completed Progress: 890 iterations completed Progress: 891 iterations completed Progress: 892 iterations completed Progress: 893 iterations completed Progress: 894 iterations completed Progress: 895 iterations completed Progress: 896 iterations completed Progress: 897 iterations completed Progress: 898 iterations completed Progress: 899 iterations completed Progress: 900 iterations completed Progress: 901 iterations completed Progress: 902 iterations completed Progress: 903 iterations completed Progress: 904 iterations completed Progress: 905 iterations completed Progress: 906 iterations completed Progress: 907 iterations completed Progress: 908 iterations completed Progress: 909 iterations completed Progress: 910 iterations completed Progress: 911 iterations completed Progress: 912 iterations completed Progress: 913 iterations completed Progress: 914 iterations completed Progress: 915 iterations completed Progress: 916 iterations completed Progress: 917 iterations completed Progress: 918 iterations completed Progress: 919 iterations completed Progress: 920 iterations completed Progress: 921 iterations completed Progress: 922 iterations completed Progress: 923 iterations completed Progress: 924 iterations completed Progress: 925 iterations completed Progress: 926 iterations completed Progress: 927 iterations completed Progress: 928 iterations completed Progress: 929 iterations completed Progress: 930 iterations completed Progress: 931 iterations completed Progress: 932 iterations completed Progress: 933 iterations completed Progress: 934 iterations completed Progress: 935 iterations completed Progress: 936 iterations completed Progress: 937 iterations completed Progress: 938 iterations completed Progress: 939 iterations completed Progress: 940 iterations completed Progress: 941 iterations completed Progress: 942 iterations completed Progress: 943 iterations completed Progress: 944 iterations completed Progress: 945 iterations completed Progress: 946 iterations completed Progress: 947 iterations completed Progress: 948 iterations completed Progress: 949 iterations completed Progress: 950 iterations completed Progress: 951 iterations completed Progress: 952 iterations completed Progress: 953 iterations completed Progress: 954 iterations completed Progress: 955 iterations completed Progress: 956 iterations completed Progress: 957 iterations completed Progress: 958 iterations completed Progress: 959 iterations completed Progress: 960 iterations completed Progress: 961 iterations completed Progress: 962 iterations completed Progress: 963 iterations completed Progress: 964 iterations completed Progress: 965 iterations completed Progress: 966 iterations completed Progress: 967 iterations completed Progress: 968 iterations completed Progress: 969 iterations completed Progress: 970 iterations completed Progress: 971 iterations completed Progress: 972 iterations completed Progress: 973 iterations completed Progress: 974 iterations completed Progress: 975 iterations completed Progress: 976 iterations completed Progress: 977 iterations completed Progress: 978 iterations completed Progress: 979 iterations completed Progress: 980 iterations completed Progress: 981 iterations completed Progress: 982 iterations completed Progress: 983 iterations completed Progress: 984 iterations completed Progress: 985 iterations completed Progress: 986 iterations completed Progress: 987 iterations completed Progress: 988 iterations completed Progress: 989 iterations completed Progress: 990 iterations completed Progress: 991 iterations completed Progress: 992 iterations completed Progress: 993 iterations completed Progress: 994 iterations completed Progress: 995 iterations completed Progress: 996 iterations completed Progress: 997 iterations completed Progress: 998 iterations completed Progress: 999 iterations completed Progress: 1000 iterations completed # Average the correlation matrix over the 1000 iterations avg_cor_matrix &lt;- cor_matrix_sum / 1000 # Print the averaged correlation matrix print(&quot;Averaged Correlation Matrix:&quot;) ## [1] &quot;Averaged Correlation Matrix:&quot; print(avg_cor_matrix) ## lm_pred rf_pred base_gbm_pred tuned_gbm_pred ## lm_pred 1.0000000 0.9006896 0.8882324 0.8921471 ## rf_pred 0.9006896 1.0000000 0.9761753 0.9768151 ## base_gbm_pred 0.8882324 0.9761753 1.0000000 0.9690552 ## tuned_gbm_pred 0.8921471 0.9768151 0.9690552 1.0000000 # Calculate mean RMSPE for the ensemble with all four models mean_rmspe_ensemble_four &lt;- mean(rmspe_ensemble_four) # Print the results print(paste(&quot;Mean RMSPE of Stacked Ensemble with All Four Models:&quot;, mean_rmspe_ensemble_four)) ## [1] &quot;Mean RMSPE of Stacked Ensemble with All Four Models: 0.336746256053834&quot; now what if i use another model that is less correlated to the other models the first knn3 setup to make sure it works It needs to be knnreg… knn3 does not work for regression # Load the required package library(caret) # Sample data (assuming &#39;data&#39; is your dataset and &#39;medv&#39; is the target) ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train &lt;- data[ind, ] test &lt;- data[-ind, ] # Fit the KNN regression model model &lt;- knnreg(medv ~ ., data = train, k = 4) # Predict on the test set predictions &lt;- predict(model, test) # Calculate RMSE rmse &lt;- sqrt(mean((predictions - test$medv)^2)) print(rmse) ## [1] 0.4388689 # Define the grid for k values k_values &lt;- seq(1, 10, by = 1) # Initialize a vector to store the average RMSPE for each k rmspe_k &lt;- numeric(length(k_values)) # Loop over each k value for (j in seq_along(k_values)) { k &lt;- k_values[j] # Initialize a vector to store RMSPE for each bootstrap run rmspe_vals &lt;- numeric(100) for (i in 1:100) { # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train KNN model with current k using knn3 knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = k) #THIS IS HUGE test_pred &lt;- predict(knn_mod, test_data) rmspe_vals[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } # Store the average RMSPE for this k rmspe_k[j] &lt;- mean(rmspe_vals) cat(&quot;\\rProgress:&quot;, j, &quot;/&quot;, length(k_values), &quot;k =&quot;, k, &quot;Average RMSPE:&quot;, rmspe_k[j]) } ## Progress: 1 / 10 k = 1 Average RMSPE: 0.4806884 Progress: 2 / 10 k = 2 Average RMSPE: 0.4280265 Progress: 3 / 10 k = 3 Average RMSPE: 0.4298077 Progress: 4 / 10 k = 4 Average RMSPE: 0.4325119 Progress: 5 / 10 k = 5 Average RMSPE: 0.4472152 Progress: 6 / 10 k = 6 Average RMSPE: 0.4632297 Progress: 7 / 10 k = 7 Average RMSPE: 0.4772773 Progress: 8 / 10 k = 8 Average RMSPE: 0.4799322 Progress: 9 / 10 k = 9 Average RMSPE: 0.4907186 Progress: 10 / 10 k = 10 Average RMSPE: 0.5038339 # Identify the best k with the lowest average RMSPE best_k &lt;- k_values[which.min(rmspe_k)] print(paste(&quot;Best k value is:&quot;, best_k, &quot;with an average RMSPE of:&quot;, min(rmspe_k))) ## [1] &quot;Best k value is: 2 with an average RMSPE of: 0.428026512526112&quot; # Initialize a vector to store RMSPE results for the best k rmspe_best_k &lt;- numeric(100) for (i in 1:100) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train KNN model with the best k using knn3 knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = best_k) # Predict on the test set test_pred &lt;- predict(knn_mod, test_data) # Calculate RMSPE for the best k rmspe_best_k[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } ## Progress: 1 iterations completed Progress: 2 iterations completed Progress: 3 iterations completed Progress: 4 iterations completed Progress: 5 iterations completed Progress: 6 iterations completed Progress: 7 iterations completed Progress: 8 iterations completed Progress: 9 iterations completed Progress: 10 iterations completed Progress: 11 iterations completed Progress: 12 iterations completed Progress: 13 iterations completed Progress: 14 iterations completed Progress: 15 iterations completed Progress: 16 iterations completed Progress: 17 iterations completed Progress: 18 iterations completed Progress: 19 iterations completed Progress: 20 iterations completed Progress: 21 iterations completed Progress: 22 iterations completed Progress: 23 iterations completed Progress: 24 iterations completed Progress: 25 iterations completed Progress: 26 iterations completed Progress: 27 iterations completed Progress: 28 iterations completed Progress: 29 iterations completed Progress: 30 iterations completed Progress: 31 iterations completed Progress: 32 iterations completed Progress: 33 iterations completed Progress: 34 iterations completed Progress: 35 iterations completed Progress: 36 iterations completed Progress: 37 iterations completed Progress: 38 iterations completed Progress: 39 iterations completed Progress: 40 iterations completed Progress: 41 iterations completed Progress: 42 iterations completed Progress: 43 iterations completed Progress: 44 iterations completed Progress: 45 iterations completed Progress: 46 iterations completed Progress: 47 iterations completed Progress: 48 iterations completed Progress: 49 iterations completed Progress: 50 iterations completed Progress: 51 iterations completed Progress: 52 iterations completed Progress: 53 iterations completed Progress: 54 iterations completed Progress: 55 iterations completed Progress: 56 iterations completed Progress: 57 iterations completed Progress: 58 iterations completed Progress: 59 iterations completed Progress: 60 iterations completed Progress: 61 iterations completed Progress: 62 iterations completed Progress: 63 iterations completed Progress: 64 iterations completed Progress: 65 iterations completed Progress: 66 iterations completed Progress: 67 iterations completed Progress: 68 iterations completed Progress: 69 iterations completed Progress: 70 iterations completed Progress: 71 iterations completed Progress: 72 iterations completed Progress: 73 iterations completed Progress: 74 iterations completed Progress: 75 iterations completed Progress: 76 iterations completed Progress: 77 iterations completed Progress: 78 iterations completed Progress: 79 iterations completed Progress: 80 iterations completed Progress: 81 iterations completed Progress: 82 iterations completed Progress: 83 iterations completed Progress: 84 iterations completed Progress: 85 iterations completed Progress: 86 iterations completed Progress: 87 iterations completed Progress: 88 iterations completed Progress: 89 iterations completed Progress: 90 iterations completed Progress: 91 iterations completed Progress: 92 iterations completed Progress: 93 iterations completed Progress: 94 iterations completed Progress: 95 iterations completed Progress: 96 iterations completed Progress: 97 iterations completed Progress: 98 iterations completed Progress: 99 iterations completed Progress: 100 iterations completed # Calculate the mean RMSPE for the best k mean_rmspe_best_k &lt;- mean(rmspe_best_k) # Print the results print(paste(&quot;Mean RMSPE with best k:&quot;, mean_rmspe_best_k)) ## [1] &quot;Mean RMSPE with best k: 0.430163863732055&quot; # Initialize storage for RMSPE results rmspe_ensemble_knn &lt;- numeric(1000) for (i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Train the KNN model with the best k knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = best_k) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) knn_pred &lt;- predict(knn_mod, test_data) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, tuned_gbm_pred, knn_pred) # Train and evaluate the stacked ensemble with KNN replacing base GBM meta_four &lt;- lm(medv ~ ., data = cbind(preds_four, medv = test_data$medv)) meta_four_pred &lt;- predict(meta_four, preds_four) # Calculate RMSPE for the ensemble with KNN replacing base GBM rmspe_ensemble_knn[i] &lt;- sqrt(mean((meta_four_pred - test_data$medv)^2)) } ## Progress: 1 iterations completed Progress: 2 iterations completed Progress: 3 iterations completed Progress: 4 iterations completed Progress: 5 iterations completed Progress: 6 iterations completed Progress: 7 iterations completed Progress: 8 iterations completed Progress: 9 iterations completed Progress: 10 iterations completed Progress: 11 iterations completed Progress: 12 iterations completed Progress: 13 iterations completed Progress: 14 iterations completed Progress: 15 iterations completed Progress: 16 iterations completed Progress: 17 iterations completed Progress: 18 iterations completed Progress: 19 iterations completed Progress: 20 iterations completed Progress: 21 iterations completed Progress: 22 iterations completed Progress: 23 iterations completed Progress: 24 iterations completed Progress: 25 iterations completed Progress: 26 iterations completed Progress: 27 iterations completed Progress: 28 iterations completed Progress: 29 iterations completed Progress: 30 iterations completed Progress: 31 iterations completed Progress: 32 iterations completed Progress: 33 iterations completed Progress: 34 iterations completed Progress: 35 iterations completed Progress: 36 iterations completed Progress: 37 iterations completed Progress: 38 iterations completed Progress: 39 iterations completed Progress: 40 iterations completed Progress: 41 iterations completed Progress: 42 iterations completed Progress: 43 iterations completed Progress: 44 iterations completed Progress: 45 iterations completed Progress: 46 iterations completed Progress: 47 iterations completed Progress: 48 iterations completed Progress: 49 iterations completed Progress: 50 iterations completed Progress: 51 iterations completed Progress: 52 iterations completed Progress: 53 iterations completed Progress: 54 iterations completed Progress: 55 iterations completed Progress: 56 iterations completed Progress: 57 iterations completed Progress: 58 iterations completed Progress: 59 iterations completed Progress: 60 iterations completed Progress: 61 iterations completed Progress: 62 iterations completed Progress: 63 iterations completed Progress: 64 iterations completed Progress: 65 iterations completed Progress: 66 iterations completed Progress: 67 iterations completed Progress: 68 iterations completed Progress: 69 iterations completed Progress: 70 iterations completed Progress: 71 iterations completed Progress: 72 iterations completed Progress: 73 iterations completed Progress: 74 iterations completed Progress: 75 iterations completed Progress: 76 iterations completed Progress: 77 iterations completed Progress: 78 iterations completed Progress: 79 iterations completed Progress: 80 iterations completed Progress: 81 iterations completed Progress: 82 iterations completed Progress: 83 iterations completed Progress: 84 iterations completed Progress: 85 iterations completed Progress: 86 iterations completed Progress: 87 iterations completed Progress: 88 iterations completed Progress: 89 iterations completed Progress: 90 iterations completed Progress: 91 iterations completed Progress: 92 iterations completed Progress: 93 iterations completed Progress: 94 iterations completed Progress: 95 iterations completed Progress: 96 iterations completed Progress: 97 iterations completed Progress: 98 iterations completed Progress: 99 iterations completed Progress: 100 iterations completed Progress: 101 iterations completed Progress: 102 iterations completed Progress: 103 iterations completed Progress: 104 iterations completed Progress: 105 iterations completed Progress: 106 iterations completed Progress: 107 iterations completed Progress: 108 iterations completed Progress: 109 iterations completed Progress: 110 iterations completed Progress: 111 iterations completed Progress: 112 iterations completed Progress: 113 iterations completed Progress: 114 iterations completed Progress: 115 iterations completed Progress: 116 iterations completed Progress: 117 iterations completed Progress: 118 iterations completed Progress: 119 iterations completed Progress: 120 iterations completed Progress: 121 iterations completed Progress: 122 iterations completed Progress: 123 iterations completed Progress: 124 iterations completed Progress: 125 iterations completed Progress: 126 iterations completed Progress: 127 iterations completed Progress: 128 iterations completed Progress: 129 iterations completed Progress: 130 iterations completed Progress: 131 iterations completed Progress: 132 iterations completed Progress: 133 iterations completed Progress: 134 iterations completed Progress: 135 iterations completed Progress: 136 iterations completed Progress: 137 iterations completed Progress: 138 iterations completed Progress: 139 iterations completed Progress: 140 iterations completed Progress: 141 iterations completed Progress: 142 iterations completed Progress: 143 iterations completed Progress: 144 iterations completed Progress: 145 iterations completed Progress: 146 iterations completed Progress: 147 iterations completed Progress: 148 iterations completed Progress: 149 iterations completed Progress: 150 iterations completed Progress: 151 iterations completed Progress: 152 iterations completed Progress: 153 iterations completed Progress: 154 iterations completed Progress: 155 iterations completed Progress: 156 iterations completed Progress: 157 iterations completed Progress: 158 iterations completed Progress: 159 iterations completed Progress: 160 iterations completed Progress: 161 iterations completed Progress: 162 iterations completed Progress: 163 iterations completed Progress: 164 iterations completed Progress: 165 iterations completed Progress: 166 iterations completed Progress: 167 iterations completed Progress: 168 iterations completed Progress: 169 iterations completed Progress: 170 iterations completed Progress: 171 iterations completed Progress: 172 iterations completed Progress: 173 iterations completed Progress: 174 iterations completed Progress: 175 iterations completed Progress: 176 iterations completed Progress: 177 iterations completed Progress: 178 iterations completed Progress: 179 iterations completed Progress: 180 iterations completed Progress: 181 iterations completed Progress: 182 iterations completed Progress: 183 iterations completed Progress: 184 iterations completed Progress: 185 iterations completed Progress: 186 iterations completed Progress: 187 iterations completed Progress: 188 iterations completed Progress: 189 iterations completed Progress: 190 iterations completed Progress: 191 iterations completed Progress: 192 iterations completed Progress: 193 iterations completed Progress: 194 iterations completed Progress: 195 iterations completed Progress: 196 iterations completed Progress: 197 iterations completed Progress: 198 iterations completed Progress: 199 iterations completed Progress: 200 iterations completed Progress: 201 iterations completed Progress: 202 iterations completed Progress: 203 iterations completed Progress: 204 iterations completed Progress: 205 iterations completed Progress: 206 iterations completed Progress: 207 iterations completed Progress: 208 iterations completed Progress: 209 iterations completed Progress: 210 iterations completed Progress: 211 iterations completed Progress: 212 iterations completed Progress: 213 iterations completed Progress: 214 iterations completed Progress: 215 iterations completed Progress: 216 iterations completed Progress: 217 iterations completed Progress: 218 iterations completed Progress: 219 iterations completed Progress: 220 iterations completed Progress: 221 iterations completed Progress: 222 iterations completed Progress: 223 iterations completed Progress: 224 iterations completed Progress: 225 iterations completed Progress: 226 iterations completed Progress: 227 iterations completed Progress: 228 iterations completed Progress: 229 iterations completed Progress: 230 iterations completed Progress: 231 iterations completed Progress: 232 iterations completed Progress: 233 iterations completed Progress: 234 iterations completed Progress: 235 iterations completed Progress: 236 iterations completed Progress: 237 iterations completed Progress: 238 iterations completed Progress: 239 iterations completed Progress: 240 iterations completed Progress: 241 iterations completed Progress: 242 iterations completed Progress: 243 iterations completed Progress: 244 iterations completed Progress: 245 iterations completed Progress: 246 iterations completed Progress: 247 iterations completed Progress: 248 iterations completed Progress: 249 iterations completed Progress: 250 iterations completed Progress: 251 iterations completed Progress: 252 iterations completed Progress: 253 iterations completed Progress: 254 iterations completed Progress: 255 iterations completed Progress: 256 iterations completed Progress: 257 iterations completed Progress: 258 iterations completed Progress: 259 iterations completed Progress: 260 iterations completed Progress: 261 iterations completed Progress: 262 iterations completed Progress: 263 iterations completed Progress: 264 iterations completed Progress: 265 iterations completed Progress: 266 iterations completed Progress: 267 iterations completed Progress: 268 iterations completed Progress: 269 iterations completed Progress: 270 iterations completed Progress: 271 iterations completed Progress: 272 iterations completed Progress: 273 iterations completed Progress: 274 iterations completed Progress: 275 iterations completed Progress: 276 iterations completed Progress: 277 iterations completed Progress: 278 iterations completed Progress: 279 iterations completed Progress: 280 iterations completed Progress: 281 iterations completed Progress: 282 iterations completed Progress: 283 iterations completed Progress: 284 iterations completed Progress: 285 iterations completed Progress: 286 iterations completed Progress: 287 iterations completed Progress: 288 iterations completed Progress: 289 iterations completed Progress: 290 iterations completed Progress: 291 iterations completed Progress: 292 iterations completed Progress: 293 iterations completed Progress: 294 iterations completed Progress: 295 iterations completed Progress: 296 iterations completed Progress: 297 iterations completed Progress: 298 iterations completed Progress: 299 iterations completed Progress: 300 iterations completed Progress: 301 iterations completed Progress: 302 iterations completed Progress: 303 iterations completed Progress: 304 iterations completed Progress: 305 iterations completed Progress: 306 iterations completed Progress: 307 iterations completed Progress: 308 iterations completed Progress: 309 iterations completed Progress: 310 iterations completed Progress: 311 iterations completed Progress: 312 iterations completed Progress: 313 iterations completed Progress: 314 iterations completed Progress: 315 iterations completed Progress: 316 iterations completed Progress: 317 iterations completed Progress: 318 iterations completed Progress: 319 iterations completed Progress: 320 iterations completed Progress: 321 iterations completed Progress: 322 iterations completed Progress: 323 iterations completed Progress: 324 iterations completed Progress: 325 iterations completed Progress: 326 iterations completed Progress: 327 iterations completed Progress: 328 iterations completed Progress: 329 iterations completed Progress: 330 iterations completed Progress: 331 iterations completed Progress: 332 iterations completed Progress: 333 iterations completed Progress: 334 iterations completed Progress: 335 iterations completed Progress: 336 iterations completed Progress: 337 iterations completed Progress: 338 iterations completed Progress: 339 iterations completed Progress: 340 iterations completed Progress: 341 iterations completed Progress: 342 iterations completed Progress: 343 iterations completed Progress: 344 iterations completed Progress: 345 iterations completed Progress: 346 iterations completed Progress: 347 iterations completed Progress: 348 iterations completed Progress: 349 iterations completed Progress: 350 iterations completed Progress: 351 iterations completed Progress: 352 iterations completed Progress: 353 iterations completed Progress: 354 iterations completed Progress: 355 iterations completed Progress: 356 iterations completed Progress: 357 iterations completed Progress: 358 iterations completed Progress: 359 iterations completed Progress: 360 iterations completed Progress: 361 iterations completed Progress: 362 iterations completed Progress: 363 iterations completed Progress: 364 iterations completed Progress: 365 iterations completed Progress: 366 iterations completed Progress: 367 iterations completed Progress: 368 iterations completed Progress: 369 iterations completed Progress: 370 iterations completed Progress: 371 iterations completed Progress: 372 iterations completed Progress: 373 iterations completed Progress: 374 iterations completed Progress: 375 iterations completed Progress: 376 iterations completed Progress: 377 iterations completed Progress: 378 iterations completed Progress: 379 iterations completed Progress: 380 iterations completed Progress: 381 iterations completed Progress: 382 iterations completed Progress: 383 iterations completed Progress: 384 iterations completed Progress: 385 iterations completed Progress: 386 iterations completed Progress: 387 iterations completed Progress: 388 iterations completed Progress: 389 iterations completed Progress: 390 iterations completed Progress: 391 iterations completed Progress: 392 iterations completed Progress: 393 iterations completed Progress: 394 iterations completed Progress: 395 iterations completed Progress: 396 iterations completed Progress: 397 iterations completed Progress: 398 iterations completed Progress: 399 iterations completed Progress: 400 iterations completed Progress: 401 iterations completed Progress: 402 iterations completed Progress: 403 iterations completed Progress: 404 iterations completed Progress: 405 iterations completed Progress: 406 iterations completed Progress: 407 iterations completed Progress: 408 iterations completed Progress: 409 iterations completed Progress: 410 iterations completed Progress: 411 iterations completed Progress: 412 iterations completed Progress: 413 iterations completed Progress: 414 iterations completed Progress: 415 iterations completed Progress: 416 iterations completed Progress: 417 iterations completed Progress: 418 iterations completed Progress: 419 iterations completed Progress: 420 iterations completed Progress: 421 iterations completed Progress: 422 iterations completed Progress: 423 iterations completed Progress: 424 iterations completed Progress: 425 iterations completed Progress: 426 iterations completed Progress: 427 iterations completed Progress: 428 iterations completed Progress: 429 iterations completed Progress: 430 iterations completed Progress: 431 iterations completed Progress: 432 iterations completed Progress: 433 iterations completed Progress: 434 iterations completed Progress: 435 iterations completed Progress: 436 iterations completed Progress: 437 iterations completed Progress: 438 iterations completed Progress: 439 iterations completed Progress: 440 iterations completed Progress: 441 iterations completed Progress: 442 iterations completed Progress: 443 iterations completed Progress: 444 iterations completed Progress: 445 iterations completed Progress: 446 iterations completed Progress: 447 iterations completed Progress: 448 iterations completed Progress: 449 iterations completed Progress: 450 iterations completed Progress: 451 iterations completed Progress: 452 iterations completed Progress: 453 iterations completed Progress: 454 iterations completed Progress: 455 iterations completed Progress: 456 iterations completed Progress: 457 iterations completed Progress: 458 iterations completed Progress: 459 iterations completed Progress: 460 iterations completed Progress: 461 iterations completed Progress: 462 iterations completed Progress: 463 iterations completed Progress: 464 iterations completed Progress: 465 iterations completed Progress: 466 iterations completed Progress: 467 iterations completed Progress: 468 iterations completed Progress: 469 iterations completed Progress: 470 iterations completed Progress: 471 iterations completed Progress: 472 iterations completed Progress: 473 iterations completed Progress: 474 iterations completed Progress: 475 iterations completed Progress: 476 iterations completed Progress: 477 iterations completed Progress: 478 iterations completed Progress: 479 iterations completed Progress: 480 iterations completed Progress: 481 iterations completed Progress: 482 iterations completed Progress: 483 iterations completed Progress: 484 iterations completed Progress: 485 iterations completed Progress: 486 iterations completed Progress: 487 iterations completed Progress: 488 iterations completed Progress: 489 iterations completed Progress: 490 iterations completed Progress: 491 iterations completed Progress: 492 iterations completed Progress: 493 iterations completed Progress: 494 iterations completed Progress: 495 iterations completed Progress: 496 iterations completed Progress: 497 iterations completed Progress: 498 iterations completed Progress: 499 iterations completed Progress: 500 iterations completed Progress: 501 iterations completed Progress: 502 iterations completed Progress: 503 iterations completed Progress: 504 iterations completed Progress: 505 iterations completed Progress: 506 iterations completed Progress: 507 iterations completed Progress: 508 iterations completed Progress: 509 iterations completed Progress: 510 iterations completed Progress: 511 iterations completed Progress: 512 iterations completed Progress: 513 iterations completed Progress: 514 iterations completed Progress: 515 iterations completed Progress: 516 iterations completed Progress: 517 iterations completed Progress: 518 iterations completed Progress: 519 iterations completed Progress: 520 iterations completed Progress: 521 iterations completed Progress: 522 iterations completed Progress: 523 iterations completed Progress: 524 iterations completed Progress: 525 iterations completed Progress: 526 iterations completed Progress: 527 iterations completed Progress: 528 iterations completed Progress: 529 iterations completed Progress: 530 iterations completed Progress: 531 iterations completed Progress: 532 iterations completed Progress: 533 iterations completed Progress: 534 iterations completed Progress: 535 iterations completed Progress: 536 iterations completed Progress: 537 iterations completed Progress: 538 iterations completed Progress: 539 iterations completed Progress: 540 iterations completed Progress: 541 iterations completed Progress: 542 iterations completed Progress: 543 iterations completed Progress: 544 iterations completed Progress: 545 iterations completed Progress: 546 iterations completed Progress: 547 iterations completed Progress: 548 iterations completed Progress: 549 iterations completed Progress: 550 iterations completed Progress: 551 iterations completed Progress: 552 iterations completed Progress: 553 iterations completed Progress: 554 iterations completed Progress: 555 iterations completed Progress: 556 iterations completed Progress: 557 iterations completed Progress: 558 iterations completed Progress: 559 iterations completed Progress: 560 iterations completed Progress: 561 iterations completed Progress: 562 iterations completed Progress: 563 iterations completed Progress: 564 iterations completed Progress: 565 iterations completed Progress: 566 iterations completed Progress: 567 iterations completed Progress: 568 iterations completed Progress: 569 iterations completed Progress: 570 iterations completed Progress: 571 iterations completed Progress: 572 iterations completed Progress: 573 iterations completed Progress: 574 iterations completed Progress: 575 iterations completed Progress: 576 iterations completed Progress: 577 iterations completed Progress: 578 iterations completed Progress: 579 iterations completed Progress: 580 iterations completed Progress: 581 iterations completed Progress: 582 iterations completed Progress: 583 iterations completed Progress: 584 iterations completed Progress: 585 iterations completed Progress: 586 iterations completed Progress: 587 iterations completed Progress: 588 iterations completed Progress: 589 iterations completed Progress: 590 iterations completed Progress: 591 iterations completed Progress: 592 iterations completed Progress: 593 iterations completed Progress: 594 iterations completed Progress: 595 iterations completed Progress: 596 iterations completed Progress: 597 iterations completed Progress: 598 iterations completed Progress: 599 iterations completed Progress: 600 iterations completed Progress: 601 iterations completed Progress: 602 iterations completed Progress: 603 iterations completed Progress: 604 iterations completed Progress: 605 iterations completed Progress: 606 iterations completed Progress: 607 iterations completed Progress: 608 iterations completed Progress: 609 iterations completed Progress: 610 iterations completed Progress: 611 iterations completed Progress: 612 iterations completed Progress: 613 iterations completed Progress: 614 iterations completed Progress: 615 iterations completed Progress: 616 iterations completed Progress: 617 iterations completed Progress: 618 iterations completed Progress: 619 iterations completed Progress: 620 iterations completed Progress: 621 iterations completed Progress: 622 iterations completed Progress: 623 iterations completed Progress: 624 iterations completed Progress: 625 iterations completed Progress: 626 iterations completed Progress: 627 iterations completed Progress: 628 iterations completed Progress: 629 iterations completed Progress: 630 iterations completed Progress: 631 iterations completed Progress: 632 iterations completed Progress: 633 iterations completed Progress: 634 iterations completed Progress: 635 iterations completed Progress: 636 iterations completed Progress: 637 iterations completed Progress: 638 iterations completed Progress: 639 iterations completed Progress: 640 iterations completed Progress: 641 iterations completed Progress: 642 iterations completed Progress: 643 iterations completed Progress: 644 iterations completed Progress: 645 iterations completed Progress: 646 iterations completed Progress: 647 iterations completed Progress: 648 iterations completed Progress: 649 iterations completed Progress: 650 iterations completed Progress: 651 iterations completed Progress: 652 iterations completed Progress: 653 iterations completed Progress: 654 iterations completed Progress: 655 iterations completed Progress: 656 iterations completed Progress: 657 iterations completed Progress: 658 iterations completed Progress: 659 iterations completed Progress: 660 iterations completed Progress: 661 iterations completed Progress: 662 iterations completed Progress: 663 iterations completed Progress: 664 iterations completed Progress: 665 iterations completed Progress: 666 iterations completed Progress: 667 iterations completed Progress: 668 iterations completed Progress: 669 iterations completed Progress: 670 iterations completed Progress: 671 iterations completed Progress: 672 iterations completed Progress: 673 iterations completed Progress: 674 iterations completed Progress: 675 iterations completed Progress: 676 iterations completed Progress: 677 iterations completed Progress: 678 iterations completed Progress: 679 iterations completed Progress: 680 iterations completed Progress: 681 iterations completed Progress: 682 iterations completed Progress: 683 iterations completed Progress: 684 iterations completed Progress: 685 iterations completed Progress: 686 iterations completed Progress: 687 iterations completed Progress: 688 iterations completed Progress: 689 iterations completed Progress: 690 iterations completed Progress: 691 iterations completed Progress: 692 iterations completed Progress: 693 iterations completed Progress: 694 iterations completed Progress: 695 iterations completed Progress: 696 iterations completed Progress: 697 iterations completed Progress: 698 iterations completed Progress: 699 iterations completed Progress: 700 iterations completed Progress: 701 iterations completed Progress: 702 iterations completed Progress: 703 iterations completed Progress: 704 iterations completed Progress: 705 iterations completed Progress: 706 iterations completed Progress: 707 iterations completed Progress: 708 iterations completed Progress: 709 iterations completed Progress: 710 iterations completed Progress: 711 iterations completed Progress: 712 iterations completed Progress: 713 iterations completed Progress: 714 iterations completed Progress: 715 iterations completed Progress: 716 iterations completed Progress: 717 iterations completed Progress: 718 iterations completed Progress: 719 iterations completed Progress: 720 iterations completed Progress: 721 iterations completed Progress: 722 iterations completed Progress: 723 iterations completed Progress: 724 iterations completed Progress: 725 iterations completed Progress: 726 iterations completed Progress: 727 iterations completed Progress: 728 iterations completed Progress: 729 iterations completed Progress: 730 iterations completed Progress: 731 iterations completed Progress: 732 iterations completed Progress: 733 iterations completed Progress: 734 iterations completed Progress: 735 iterations completed Progress: 736 iterations completed Progress: 737 iterations completed Progress: 738 iterations completed Progress: 739 iterations completed Progress: 740 iterations completed Progress: 741 iterations completed Progress: 742 iterations completed Progress: 743 iterations completed Progress: 744 iterations completed Progress: 745 iterations completed Progress: 746 iterations completed Progress: 747 iterations completed Progress: 748 iterations completed Progress: 749 iterations completed Progress: 750 iterations completed Progress: 751 iterations completed Progress: 752 iterations completed Progress: 753 iterations completed Progress: 754 iterations completed Progress: 755 iterations completed Progress: 756 iterations completed Progress: 757 iterations completed Progress: 758 iterations completed Progress: 759 iterations completed Progress: 760 iterations completed Progress: 761 iterations completed Progress: 762 iterations completed Progress: 763 iterations completed Progress: 764 iterations completed Progress: 765 iterations completed Progress: 766 iterations completed Progress: 767 iterations completed Progress: 768 iterations completed Progress: 769 iterations completed Progress: 770 iterations completed Progress: 771 iterations completed Progress: 772 iterations completed Progress: 773 iterations completed Progress: 774 iterations completed Progress: 775 iterations completed Progress: 776 iterations completed Progress: 777 iterations completed Progress: 778 iterations completed Progress: 779 iterations completed Progress: 780 iterations completed Progress: 781 iterations completed Progress: 782 iterations completed Progress: 783 iterations completed Progress: 784 iterations completed Progress: 785 iterations completed Progress: 786 iterations completed Progress: 787 iterations completed Progress: 788 iterations completed Progress: 789 iterations completed Progress: 790 iterations completed Progress: 791 iterations completed Progress: 792 iterations completed Progress: 793 iterations completed Progress: 794 iterations completed Progress: 795 iterations completed Progress: 796 iterations completed Progress: 797 iterations completed Progress: 798 iterations completed Progress: 799 iterations completed Progress: 800 iterations completed Progress: 801 iterations completed Progress: 802 iterations completed Progress: 803 iterations completed Progress: 804 iterations completed Progress: 805 iterations completed Progress: 806 iterations completed Progress: 807 iterations completed Progress: 808 iterations completed Progress: 809 iterations completed Progress: 810 iterations completed Progress: 811 iterations completed Progress: 812 iterations completed Progress: 813 iterations completed Progress: 814 iterations completed Progress: 815 iterations completed Progress: 816 iterations completed Progress: 817 iterations completed Progress: 818 iterations completed Progress: 819 iterations completed Progress: 820 iterations completed Progress: 821 iterations completed Progress: 822 iterations completed Progress: 823 iterations completed Progress: 824 iterations completed Progress: 825 iterations completed Progress: 826 iterations completed Progress: 827 iterations completed Progress: 828 iterations completed Progress: 829 iterations completed Progress: 830 iterations completed Progress: 831 iterations completed Progress: 832 iterations completed Progress: 833 iterations completed Progress: 834 iterations completed Progress: 835 iterations completed Progress: 836 iterations completed Progress: 837 iterations completed Progress: 838 iterations completed Progress: 839 iterations completed Progress: 840 iterations completed Progress: 841 iterations completed Progress: 842 iterations completed Progress: 843 iterations completed Progress: 844 iterations completed Progress: 845 iterations completed Progress: 846 iterations completed Progress: 847 iterations completed Progress: 848 iterations completed Progress: 849 iterations completed Progress: 850 iterations completed Progress: 851 iterations completed Progress: 852 iterations completed Progress: 853 iterations completed Progress: 854 iterations completed Progress: 855 iterations completed Progress: 856 iterations completed Progress: 857 iterations completed Progress: 858 iterations completed Progress: 859 iterations completed Progress: 860 iterations completed Progress: 861 iterations completed Progress: 862 iterations completed Progress: 863 iterations completed Progress: 864 iterations completed Progress: 865 iterations completed Progress: 866 iterations completed Progress: 867 iterations completed Progress: 868 iterations completed Progress: 869 iterations completed Progress: 870 iterations completed Progress: 871 iterations completed Progress: 872 iterations completed Progress: 873 iterations completed Progress: 874 iterations completed Progress: 875 iterations completed Progress: 876 iterations completed Progress: 877 iterations completed Progress: 878 iterations completed Progress: 879 iterations completed Progress: 880 iterations completed Progress: 881 iterations completed Progress: 882 iterations completed Progress: 883 iterations completed Progress: 884 iterations completed Progress: 885 iterations completed Progress: 886 iterations completed Progress: 887 iterations completed Progress: 888 iterations completed Progress: 889 iterations completed Progress: 890 iterations completed Progress: 891 iterations completed Progress: 892 iterations completed Progress: 893 iterations completed Progress: 894 iterations completed Progress: 895 iterations completed Progress: 896 iterations completed Progress: 897 iterations completed Progress: 898 iterations completed Progress: 899 iterations completed Progress: 900 iterations completed Progress: 901 iterations completed Progress: 902 iterations completed Progress: 903 iterations completed Progress: 904 iterations completed Progress: 905 iterations completed Progress: 906 iterations completed Progress: 907 iterations completed Progress: 908 iterations completed Progress: 909 iterations completed Progress: 910 iterations completed Progress: 911 iterations completed Progress: 912 iterations completed Progress: 913 iterations completed Progress: 914 iterations completed Progress: 915 iterations completed Progress: 916 iterations completed Progress: 917 iterations completed Progress: 918 iterations completed Progress: 919 iterations completed Progress: 920 iterations completed Progress: 921 iterations completed Progress: 922 iterations completed Progress: 923 iterations completed Progress: 924 iterations completed Progress: 925 iterations completed Progress: 926 iterations completed Progress: 927 iterations completed Progress: 928 iterations completed Progress: 929 iterations completed Progress: 930 iterations completed Progress: 931 iterations completed Progress: 932 iterations completed Progress: 933 iterations completed Progress: 934 iterations completed Progress: 935 iterations completed Progress: 936 iterations completed Progress: 937 iterations completed Progress: 938 iterations completed Progress: 939 iterations completed Progress: 940 iterations completed Progress: 941 iterations completed Progress: 942 iterations completed Progress: 943 iterations completed Progress: 944 iterations completed Progress: 945 iterations completed Progress: 946 iterations completed Progress: 947 iterations completed Progress: 948 iterations completed Progress: 949 iterations completed Progress: 950 iterations completed Progress: 951 iterations completed Progress: 952 iterations completed Progress: 953 iterations completed Progress: 954 iterations completed Progress: 955 iterations completed Progress: 956 iterations completed Progress: 957 iterations completed Progress: 958 iterations completed Progress: 959 iterations completed Progress: 960 iterations completed Progress: 961 iterations completed Progress: 962 iterations completed Progress: 963 iterations completed Progress: 964 iterations completed Progress: 965 iterations completed Progress: 966 iterations completed Progress: 967 iterations completed Progress: 968 iterations completed Progress: 969 iterations completed Progress: 970 iterations completed Progress: 971 iterations completed Progress: 972 iterations completed Progress: 973 iterations completed Progress: 974 iterations completed Progress: 975 iterations completed Progress: 976 iterations completed Progress: 977 iterations completed Progress: 978 iterations completed Progress: 979 iterations completed Progress: 980 iterations completed Progress: 981 iterations completed Progress: 982 iterations completed Progress: 983 iterations completed Progress: 984 iterations completed Progress: 985 iterations completed Progress: 986 iterations completed Progress: 987 iterations completed Progress: 988 iterations completed Progress: 989 iterations completed Progress: 990 iterations completed Progress: 991 iterations completed Progress: 992 iterations completed Progress: 993 iterations completed Progress: 994 iterations completed Progress: 995 iterations completed Progress: 996 iterations completed Progress: 997 iterations completed Progress: 998 iterations completed Progress: 999 iterations completed Progress: 1000 iterations completed # Calculate mean RMSPE for the ensemble with KNN mean_rmspe_ensemble_knn &lt;- mean(rmspe_ensemble_knn) # Print the results print(paste(&quot;Mean RMSPE of Stacked Ensemble with Tuned KNN Model:&quot;, mean_rmspe_ensemble_knn)) ## [1] &quot;Mean RMSPE of Stacked Ensemble with Tuned KNN Model: 0.33117728762542&quot; # Initialize storage for the sum of correlation matrices cor_matrix_sum &lt;- matrix(0, nrow = 4, ncol = 4) # Assuming 4 base models for (i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Train the KNN model with the best k knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = best_k) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) knn_pred &lt;- predict(knn_mod, test_data) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, tuned_gbm_pred, knn_pred) # Calculate the correlation matrix for the current iteration cor_matrix &lt;- cor(preds_four) # Accumulate the correlation matrix cor_matrix_sum &lt;- cor_matrix_sum + cor_matrix } ## Progress: 1 iterations completed Progress: 2 iterations completed Progress: 3 iterations completed Progress: 4 iterations completed Progress: 5 iterations completed Progress: 6 iterations completed Progress: 7 iterations completed Progress: 8 iterations completed Progress: 9 iterations completed Progress: 10 iterations completed Progress: 11 iterations completed Progress: 12 iterations completed Progress: 13 iterations completed Progress: 14 iterations completed Progress: 15 iterations completed Progress: 16 iterations completed Progress: 17 iterations completed Progress: 18 iterations completed Progress: 19 iterations completed Progress: 20 iterations completed Progress: 21 iterations completed Progress: 22 iterations completed Progress: 23 iterations completed Progress: 24 iterations completed Progress: 25 iterations completed Progress: 26 iterations completed Progress: 27 iterations completed Progress: 28 iterations completed Progress: 29 iterations completed Progress: 30 iterations completed Progress: 31 iterations completed Progress: 32 iterations completed Progress: 33 iterations completed Progress: 34 iterations completed Progress: 35 iterations completed Progress: 36 iterations completed Progress: 37 iterations completed Progress: 38 iterations completed Progress: 39 iterations completed Progress: 40 iterations completed Progress: 41 iterations completed Progress: 42 iterations completed Progress: 43 iterations completed Progress: 44 iterations completed Progress: 45 iterations completed Progress: 46 iterations completed Progress: 47 iterations completed Progress: 48 iterations completed Progress: 49 iterations completed Progress: 50 iterations completed Progress: 51 iterations completed Progress: 52 iterations completed Progress: 53 iterations completed Progress: 54 iterations completed Progress: 55 iterations completed Progress: 56 iterations completed Progress: 57 iterations completed Progress: 58 iterations completed Progress: 59 iterations completed Progress: 60 iterations completed Progress: 61 iterations completed Progress: 62 iterations completed Progress: 63 iterations completed Progress: 64 iterations completed Progress: 65 iterations completed Progress: 66 iterations completed Progress: 67 iterations completed Progress: 68 iterations completed Progress: 69 iterations completed Progress: 70 iterations completed Progress: 71 iterations completed Progress: 72 iterations completed Progress: 73 iterations completed Progress: 74 iterations completed Progress: 75 iterations completed Progress: 76 iterations completed Progress: 77 iterations completed Progress: 78 iterations completed Progress: 79 iterations completed Progress: 80 iterations completed Progress: 81 iterations completed Progress: 82 iterations completed Progress: 83 iterations completed Progress: 84 iterations completed Progress: 85 iterations completed Progress: 86 iterations completed Progress: 87 iterations completed Progress: 88 iterations completed Progress: 89 iterations completed Progress: 90 iterations completed Progress: 91 iterations completed Progress: 92 iterations completed Progress: 93 iterations completed Progress: 94 iterations completed Progress: 95 iterations completed Progress: 96 iterations completed Progress: 97 iterations completed Progress: 98 iterations completed Progress: 99 iterations completed Progress: 100 iterations completed Progress: 101 iterations completed Progress: 102 iterations completed Progress: 103 iterations completed Progress: 104 iterations completed Progress: 105 iterations completed Progress: 106 iterations completed Progress: 107 iterations completed Progress: 108 iterations completed Progress: 109 iterations completed Progress: 110 iterations completed Progress: 111 iterations completed Progress: 112 iterations completed Progress: 113 iterations completed Progress: 114 iterations completed Progress: 115 iterations completed Progress: 116 iterations completed Progress: 117 iterations completed Progress: 118 iterations completed Progress: 119 iterations completed Progress: 120 iterations completed Progress: 121 iterations completed Progress: 122 iterations completed Progress: 123 iterations completed Progress: 124 iterations completed Progress: 125 iterations completed Progress: 126 iterations completed Progress: 127 iterations completed Progress: 128 iterations completed Progress: 129 iterations completed Progress: 130 iterations completed Progress: 131 iterations completed Progress: 132 iterations completed Progress: 133 iterations completed Progress: 134 iterations completed Progress: 135 iterations completed Progress: 136 iterations completed Progress: 137 iterations completed Progress: 138 iterations completed Progress: 139 iterations completed Progress: 140 iterations completed Progress: 141 iterations completed Progress: 142 iterations completed Progress: 143 iterations completed Progress: 144 iterations completed Progress: 145 iterations completed Progress: 146 iterations completed Progress: 147 iterations completed Progress: 148 iterations completed Progress: 149 iterations completed Progress: 150 iterations completed Progress: 151 iterations completed Progress: 152 iterations completed Progress: 153 iterations completed Progress: 154 iterations completed Progress: 155 iterations completed Progress: 156 iterations completed Progress: 157 iterations completed Progress: 158 iterations completed Progress: 159 iterations completed Progress: 160 iterations completed Progress: 161 iterations completed Progress: 162 iterations completed Progress: 163 iterations completed Progress: 164 iterations completed Progress: 165 iterations completed Progress: 166 iterations completed Progress: 167 iterations completed Progress: 168 iterations completed Progress: 169 iterations completed Progress: 170 iterations completed Progress: 171 iterations completed Progress: 172 iterations completed Progress: 173 iterations completed Progress: 174 iterations completed Progress: 175 iterations completed Progress: 176 iterations completed Progress: 177 iterations completed Progress: 178 iterations completed Progress: 179 iterations completed Progress: 180 iterations completed Progress: 181 iterations completed Progress: 182 iterations completed Progress: 183 iterations completed Progress: 184 iterations completed Progress: 185 iterations completed Progress: 186 iterations completed Progress: 187 iterations completed Progress: 188 iterations completed Progress: 189 iterations completed Progress: 190 iterations completed Progress: 191 iterations completed Progress: 192 iterations completed Progress: 193 iterations completed Progress: 194 iterations completed Progress: 195 iterations completed Progress: 196 iterations completed Progress: 197 iterations completed Progress: 198 iterations completed Progress: 199 iterations completed Progress: 200 iterations completed Progress: 201 iterations completed Progress: 202 iterations completed Progress: 203 iterations completed Progress: 204 iterations completed Progress: 205 iterations completed Progress: 206 iterations completed Progress: 207 iterations completed Progress: 208 iterations completed Progress: 209 iterations completed Progress: 210 iterations completed Progress: 211 iterations completed Progress: 212 iterations completed Progress: 213 iterations completed Progress: 214 iterations completed Progress: 215 iterations completed Progress: 216 iterations completed Progress: 217 iterations completed Progress: 218 iterations completed Progress: 219 iterations completed Progress: 220 iterations completed Progress: 221 iterations completed Progress: 222 iterations completed Progress: 223 iterations completed Progress: 224 iterations completed Progress: 225 iterations completed Progress: 226 iterations completed Progress: 227 iterations completed Progress: 228 iterations completed Progress: 229 iterations completed Progress: 230 iterations completed Progress: 231 iterations completed Progress: 232 iterations completed Progress: 233 iterations completed Progress: 234 iterations completed Progress: 235 iterations completed Progress: 236 iterations completed Progress: 237 iterations completed Progress: 238 iterations completed Progress: 239 iterations completed Progress: 240 iterations completed Progress: 241 iterations completed Progress: 242 iterations completed Progress: 243 iterations completed Progress: 244 iterations completed Progress: 245 iterations completed Progress: 246 iterations completed Progress: 247 iterations completed Progress: 248 iterations completed Progress: 249 iterations completed Progress: 250 iterations completed Progress: 251 iterations completed Progress: 252 iterations completed Progress: 253 iterations completed Progress: 254 iterations completed Progress: 255 iterations completed Progress: 256 iterations completed Progress: 257 iterations completed Progress: 258 iterations completed Progress: 259 iterations completed Progress: 260 iterations completed Progress: 261 iterations completed Progress: 262 iterations completed Progress: 263 iterations completed Progress: 264 iterations completed Progress: 265 iterations completed Progress: 266 iterations completed Progress: 267 iterations completed Progress: 268 iterations completed Progress: 269 iterations completed Progress: 270 iterations completed Progress: 271 iterations completed Progress: 272 iterations completed Progress: 273 iterations completed Progress: 274 iterations completed Progress: 275 iterations completed Progress: 276 iterations completed Progress: 277 iterations completed Progress: 278 iterations completed Progress: 279 iterations completed Progress: 280 iterations completed Progress: 281 iterations completed Progress: 282 iterations completed Progress: 283 iterations completed Progress: 284 iterations completed Progress: 285 iterations completed Progress: 286 iterations completed Progress: 287 iterations completed Progress: 288 iterations completed Progress: 289 iterations completed Progress: 290 iterations completed Progress: 291 iterations completed Progress: 292 iterations completed Progress: 293 iterations completed Progress: 294 iterations completed Progress: 295 iterations completed Progress: 296 iterations completed Progress: 297 iterations completed Progress: 298 iterations completed Progress: 299 iterations completed Progress: 300 iterations completed Progress: 301 iterations completed Progress: 302 iterations completed Progress: 303 iterations completed Progress: 304 iterations completed Progress: 305 iterations completed Progress: 306 iterations completed Progress: 307 iterations completed Progress: 308 iterations completed Progress: 309 iterations completed Progress: 310 iterations completed Progress: 311 iterations completed Progress: 312 iterations completed Progress: 313 iterations completed Progress: 314 iterations completed Progress: 315 iterations completed Progress: 316 iterations completed Progress: 317 iterations completed Progress: 318 iterations completed Progress: 319 iterations completed Progress: 320 iterations completed Progress: 321 iterations completed Progress: 322 iterations completed Progress: 323 iterations completed Progress: 324 iterations completed Progress: 325 iterations completed Progress: 326 iterations completed Progress: 327 iterations completed Progress: 328 iterations completed Progress: 329 iterations completed Progress: 330 iterations completed Progress: 331 iterations completed Progress: 332 iterations completed Progress: 333 iterations completed Progress: 334 iterations completed Progress: 335 iterations completed Progress: 336 iterations completed Progress: 337 iterations completed Progress: 338 iterations completed Progress: 339 iterations completed Progress: 340 iterations completed Progress: 341 iterations completed Progress: 342 iterations completed Progress: 343 iterations completed Progress: 344 iterations completed Progress: 345 iterations completed Progress: 346 iterations completed Progress: 347 iterations completed Progress: 348 iterations completed Progress: 349 iterations completed Progress: 350 iterations completed Progress: 351 iterations completed Progress: 352 iterations completed Progress: 353 iterations completed Progress: 354 iterations completed Progress: 355 iterations completed Progress: 356 iterations completed Progress: 357 iterations completed Progress: 358 iterations completed Progress: 359 iterations completed Progress: 360 iterations completed Progress: 361 iterations completed Progress: 362 iterations completed Progress: 363 iterations completed Progress: 364 iterations completed Progress: 365 iterations completed Progress: 366 iterations completed Progress: 367 iterations completed Progress: 368 iterations completed Progress: 369 iterations completed Progress: 370 iterations completed Progress: 371 iterations completed Progress: 372 iterations completed Progress: 373 iterations completed Progress: 374 iterations completed Progress: 375 iterations completed Progress: 376 iterations completed Progress: 377 iterations completed Progress: 378 iterations completed Progress: 379 iterations completed Progress: 380 iterations completed Progress: 381 iterations completed Progress: 382 iterations completed Progress: 383 iterations completed Progress: 384 iterations completed Progress: 385 iterations completed Progress: 386 iterations completed Progress: 387 iterations completed Progress: 388 iterations completed Progress: 389 iterations completed Progress: 390 iterations completed Progress: 391 iterations completed Progress: 392 iterations completed Progress: 393 iterations completed Progress: 394 iterations completed Progress: 395 iterations completed Progress: 396 iterations completed Progress: 397 iterations completed Progress: 398 iterations completed Progress: 399 iterations completed Progress: 400 iterations completed Progress: 401 iterations completed Progress: 402 iterations completed Progress: 403 iterations completed Progress: 404 iterations completed Progress: 405 iterations completed Progress: 406 iterations completed Progress: 407 iterations completed Progress: 408 iterations completed Progress: 409 iterations completed Progress: 410 iterations completed Progress: 411 iterations completed Progress: 412 iterations completed Progress: 413 iterations completed Progress: 414 iterations completed Progress: 415 iterations completed Progress: 416 iterations completed Progress: 417 iterations completed Progress: 418 iterations completed Progress: 419 iterations completed Progress: 420 iterations completed Progress: 421 iterations completed Progress: 422 iterations completed Progress: 423 iterations completed Progress: 424 iterations completed Progress: 425 iterations completed Progress: 426 iterations completed Progress: 427 iterations completed Progress: 428 iterations completed Progress: 429 iterations completed Progress: 430 iterations completed Progress: 431 iterations completed Progress: 432 iterations completed Progress: 433 iterations completed Progress: 434 iterations completed Progress: 435 iterations completed Progress: 436 iterations completed Progress: 437 iterations completed Progress: 438 iterations completed Progress: 439 iterations completed Progress: 440 iterations completed Progress: 441 iterations completed Progress: 442 iterations completed Progress: 443 iterations completed Progress: 444 iterations completed Progress: 445 iterations completed Progress: 446 iterations completed Progress: 447 iterations completed Progress: 448 iterations completed Progress: 449 iterations completed Progress: 450 iterations completed Progress: 451 iterations completed Progress: 452 iterations completed Progress: 453 iterations completed Progress: 454 iterations completed Progress: 455 iterations completed Progress: 456 iterations completed Progress: 457 iterations completed Progress: 458 iterations completed Progress: 459 iterations completed Progress: 460 iterations completed Progress: 461 iterations completed Progress: 462 iterations completed Progress: 463 iterations completed Progress: 464 iterations completed Progress: 465 iterations completed Progress: 466 iterations completed Progress: 467 iterations completed Progress: 468 iterations completed Progress: 469 iterations completed Progress: 470 iterations completed Progress: 471 iterations completed Progress: 472 iterations completed Progress: 473 iterations completed Progress: 474 iterations completed Progress: 475 iterations completed Progress: 476 iterations completed Progress: 477 iterations completed Progress: 478 iterations completed Progress: 479 iterations completed Progress: 480 iterations completed Progress: 481 iterations completed Progress: 482 iterations completed Progress: 483 iterations completed Progress: 484 iterations completed Progress: 485 iterations completed Progress: 486 iterations completed Progress: 487 iterations completed Progress: 488 iterations completed Progress: 489 iterations completed Progress: 490 iterations completed Progress: 491 iterations completed Progress: 492 iterations completed Progress: 493 iterations completed Progress: 494 iterations completed Progress: 495 iterations completed Progress: 496 iterations completed Progress: 497 iterations completed Progress: 498 iterations completed Progress: 499 iterations completed Progress: 500 iterations completed Progress: 501 iterations completed Progress: 502 iterations completed Progress: 503 iterations completed Progress: 504 iterations completed Progress: 505 iterations completed Progress: 506 iterations completed Progress: 507 iterations completed Progress: 508 iterations completed Progress: 509 iterations completed Progress: 510 iterations completed Progress: 511 iterations completed Progress: 512 iterations completed Progress: 513 iterations completed Progress: 514 iterations completed Progress: 515 iterations completed Progress: 516 iterations completed Progress: 517 iterations completed Progress: 518 iterations completed Progress: 519 iterations completed Progress: 520 iterations completed Progress: 521 iterations completed Progress: 522 iterations completed Progress: 523 iterations completed Progress: 524 iterations completed Progress: 525 iterations completed Progress: 526 iterations completed Progress: 527 iterations completed Progress: 528 iterations completed Progress: 529 iterations completed Progress: 530 iterations completed Progress: 531 iterations completed Progress: 532 iterations completed Progress: 533 iterations completed Progress: 534 iterations completed Progress: 535 iterations completed Progress: 536 iterations completed Progress: 537 iterations completed Progress: 538 iterations completed Progress: 539 iterations completed Progress: 540 iterations completed Progress: 541 iterations completed Progress: 542 iterations completed Progress: 543 iterations completed Progress: 544 iterations completed Progress: 545 iterations completed Progress: 546 iterations completed Progress: 547 iterations completed Progress: 548 iterations completed Progress: 549 iterations completed Progress: 550 iterations completed Progress: 551 iterations completed Progress: 552 iterations completed Progress: 553 iterations completed Progress: 554 iterations completed Progress: 555 iterations completed Progress: 556 iterations completed Progress: 557 iterations completed Progress: 558 iterations completed Progress: 559 iterations completed Progress: 560 iterations completed Progress: 561 iterations completed Progress: 562 iterations completed Progress: 563 iterations completed Progress: 564 iterations completed Progress: 565 iterations completed Progress: 566 iterations completed Progress: 567 iterations completed Progress: 568 iterations completed Progress: 569 iterations completed Progress: 570 iterations completed Progress: 571 iterations completed Progress: 572 iterations completed Progress: 573 iterations completed Progress: 574 iterations completed Progress: 575 iterations completed Progress: 576 iterations completed Progress: 577 iterations completed Progress: 578 iterations completed Progress: 579 iterations completed Progress: 580 iterations completed Progress: 581 iterations completed Progress: 582 iterations completed Progress: 583 iterations completed Progress: 584 iterations completed Progress: 585 iterations completed Progress: 586 iterations completed Progress: 587 iterations completed Progress: 588 iterations completed Progress: 589 iterations completed Progress: 590 iterations completed Progress: 591 iterations completed Progress: 592 iterations completed Progress: 593 iterations completed Progress: 594 iterations completed Progress: 595 iterations completed Progress: 596 iterations completed Progress: 597 iterations completed Progress: 598 iterations completed Progress: 599 iterations completed Progress: 600 iterations completed Progress: 601 iterations completed Progress: 602 iterations completed Progress: 603 iterations completed Progress: 604 iterations completed Progress: 605 iterations completed Progress: 606 iterations completed Progress: 607 iterations completed Progress: 608 iterations completed Progress: 609 iterations completed Progress: 610 iterations completed Progress: 611 iterations completed Progress: 612 iterations completed Progress: 613 iterations completed Progress: 614 iterations completed Progress: 615 iterations completed Progress: 616 iterations completed Progress: 617 iterations completed Progress: 618 iterations completed Progress: 619 iterations completed Progress: 620 iterations completed Progress: 621 iterations completed Progress: 622 iterations completed Progress: 623 iterations completed Progress: 624 iterations completed Progress: 625 iterations completed Progress: 626 iterations completed Progress: 627 iterations completed Progress: 628 iterations completed Progress: 629 iterations completed Progress: 630 iterations completed Progress: 631 iterations completed Progress: 632 iterations completed Progress: 633 iterations completed Progress: 634 iterations completed Progress: 635 iterations completed Progress: 636 iterations completed Progress: 637 iterations completed Progress: 638 iterations completed Progress: 639 iterations completed Progress: 640 iterations completed Progress: 641 iterations completed Progress: 642 iterations completed Progress: 643 iterations completed Progress: 644 iterations completed Progress: 645 iterations completed Progress: 646 iterations completed Progress: 647 iterations completed Progress: 648 iterations completed Progress: 649 iterations completed Progress: 650 iterations completed Progress: 651 iterations completed Progress: 652 iterations completed Progress: 653 iterations completed Progress: 654 iterations completed Progress: 655 iterations completed Progress: 656 iterations completed Progress: 657 iterations completed Progress: 658 iterations completed Progress: 659 iterations completed Progress: 660 iterations completed Progress: 661 iterations completed Progress: 662 iterations completed Progress: 663 iterations completed Progress: 664 iterations completed Progress: 665 iterations completed Progress: 666 iterations completed Progress: 667 iterations completed Progress: 668 iterations completed Progress: 669 iterations completed Progress: 670 iterations completed Progress: 671 iterations completed Progress: 672 iterations completed Progress: 673 iterations completed Progress: 674 iterations completed Progress: 675 iterations completed Progress: 676 iterations completed Progress: 677 iterations completed Progress: 678 iterations completed Progress: 679 iterations completed Progress: 680 iterations completed Progress: 681 iterations completed Progress: 682 iterations completed Progress: 683 iterations completed Progress: 684 iterations completed Progress: 685 iterations completed Progress: 686 iterations completed Progress: 687 iterations completed Progress: 688 iterations completed Progress: 689 iterations completed Progress: 690 iterations completed Progress: 691 iterations completed Progress: 692 iterations completed Progress: 693 iterations completed Progress: 694 iterations completed Progress: 695 iterations completed Progress: 696 iterations completed Progress: 697 iterations completed Progress: 698 iterations completed Progress: 699 iterations completed Progress: 700 iterations completed Progress: 701 iterations completed Progress: 702 iterations completed Progress: 703 iterations completed Progress: 704 iterations completed Progress: 705 iterations completed Progress: 706 iterations completed Progress: 707 iterations completed Progress: 708 iterations completed Progress: 709 iterations completed Progress: 710 iterations completed Progress: 711 iterations completed Progress: 712 iterations completed Progress: 713 iterations completed Progress: 714 iterations completed Progress: 715 iterations completed Progress: 716 iterations completed Progress: 717 iterations completed Progress: 718 iterations completed Progress: 719 iterations completed Progress: 720 iterations completed Progress: 721 iterations completed Progress: 722 iterations completed Progress: 723 iterations completed Progress: 724 iterations completed Progress: 725 iterations completed Progress: 726 iterations completed Progress: 727 iterations completed Progress: 728 iterations completed Progress: 729 iterations completed Progress: 730 iterations completed Progress: 731 iterations completed Progress: 732 iterations completed Progress: 733 iterations completed Progress: 734 iterations completed Progress: 735 iterations completed Progress: 736 iterations completed Progress: 737 iterations completed Progress: 738 iterations completed Progress: 739 iterations completed Progress: 740 iterations completed Progress: 741 iterations completed Progress: 742 iterations completed Progress: 743 iterations completed Progress: 744 iterations completed Progress: 745 iterations completed Progress: 746 iterations completed Progress: 747 iterations completed Progress: 748 iterations completed Progress: 749 iterations completed Progress: 750 iterations completed Progress: 751 iterations completed Progress: 752 iterations completed Progress: 753 iterations completed Progress: 754 iterations completed Progress: 755 iterations completed Progress: 756 iterations completed Progress: 757 iterations completed Progress: 758 iterations completed Progress: 759 iterations completed Progress: 760 iterations completed Progress: 761 iterations completed Progress: 762 iterations completed Progress: 763 iterations completed Progress: 764 iterations completed Progress: 765 iterations completed Progress: 766 iterations completed Progress: 767 iterations completed Progress: 768 iterations completed Progress: 769 iterations completed Progress: 770 iterations completed Progress: 771 iterations completed Progress: 772 iterations completed Progress: 773 iterations completed Progress: 774 iterations completed Progress: 775 iterations completed Progress: 776 iterations completed Progress: 777 iterations completed Progress: 778 iterations completed Progress: 779 iterations completed Progress: 780 iterations completed Progress: 781 iterations completed Progress: 782 iterations completed Progress: 783 iterations completed Progress: 784 iterations completed Progress: 785 iterations completed Progress: 786 iterations completed Progress: 787 iterations completed Progress: 788 iterations completed Progress: 789 iterations completed Progress: 790 iterations completed Progress: 791 iterations completed Progress: 792 iterations completed Progress: 793 iterations completed Progress: 794 iterations completed Progress: 795 iterations completed Progress: 796 iterations completed Progress: 797 iterations completed Progress: 798 iterations completed Progress: 799 iterations completed Progress: 800 iterations completed Progress: 801 iterations completed Progress: 802 iterations completed Progress: 803 iterations completed Progress: 804 iterations completed Progress: 805 iterations completed Progress: 806 iterations completed Progress: 807 iterations completed Progress: 808 iterations completed Progress: 809 iterations completed Progress: 810 iterations completed Progress: 811 iterations completed Progress: 812 iterations completed Progress: 813 iterations completed Progress: 814 iterations completed Progress: 815 iterations completed Progress: 816 iterations completed Progress: 817 iterations completed Progress: 818 iterations completed Progress: 819 iterations completed Progress: 820 iterations completed Progress: 821 iterations completed Progress: 822 iterations completed Progress: 823 iterations completed Progress: 824 iterations completed Progress: 825 iterations completed Progress: 826 iterations completed Progress: 827 iterations completed Progress: 828 iterations completed Progress: 829 iterations completed Progress: 830 iterations completed Progress: 831 iterations completed Progress: 832 iterations completed Progress: 833 iterations completed Progress: 834 iterations completed Progress: 835 iterations completed Progress: 836 iterations completed Progress: 837 iterations completed Progress: 838 iterations completed Progress: 839 iterations completed Progress: 840 iterations completed Progress: 841 iterations completed Progress: 842 iterations completed Progress: 843 iterations completed Progress: 844 iterations completed Progress: 845 iterations completed Progress: 846 iterations completed Progress: 847 iterations completed Progress: 848 iterations completed Progress: 849 iterations completed Progress: 850 iterations completed Progress: 851 iterations completed Progress: 852 iterations completed Progress: 853 iterations completed Progress: 854 iterations completed Progress: 855 iterations completed Progress: 856 iterations completed Progress: 857 iterations completed Progress: 858 iterations completed Progress: 859 iterations completed Progress: 860 iterations completed Progress: 861 iterations completed Progress: 862 iterations completed Progress: 863 iterations completed Progress: 864 iterations completed Progress: 865 iterations completed Progress: 866 iterations completed Progress: 867 iterations completed Progress: 868 iterations completed Progress: 869 iterations completed Progress: 870 iterations completed Progress: 871 iterations completed Progress: 872 iterations completed Progress: 873 iterations completed Progress: 874 iterations completed Progress: 875 iterations completed Progress: 876 iterations completed Progress: 877 iterations completed Progress: 878 iterations completed Progress: 879 iterations completed Progress: 880 iterations completed Progress: 881 iterations completed Progress: 882 iterations completed Progress: 883 iterations completed Progress: 884 iterations completed Progress: 885 iterations completed Progress: 886 iterations completed Progress: 887 iterations completed Progress: 888 iterations completed Progress: 889 iterations completed Progress: 890 iterations completed Progress: 891 iterations completed Progress: 892 iterations completed Progress: 893 iterations completed Progress: 894 iterations completed Progress: 895 iterations completed Progress: 896 iterations completed Progress: 897 iterations completed Progress: 898 iterations completed Progress: 899 iterations completed Progress: 900 iterations completed Progress: 901 iterations completed Progress: 902 iterations completed Progress: 903 iterations completed Progress: 904 iterations completed Progress: 905 iterations completed Progress: 906 iterations completed Progress: 907 iterations completed Progress: 908 iterations completed Progress: 909 iterations completed Progress: 910 iterations completed Progress: 911 iterations completed Progress: 912 iterations completed Progress: 913 iterations completed Progress: 914 iterations completed Progress: 915 iterations completed Progress: 916 iterations completed Progress: 917 iterations completed Progress: 918 iterations completed Progress: 919 iterations completed Progress: 920 iterations completed Progress: 921 iterations completed Progress: 922 iterations completed Progress: 923 iterations completed Progress: 924 iterations completed Progress: 925 iterations completed Progress: 926 iterations completed Progress: 927 iterations completed Progress: 928 iterations completed Progress: 929 iterations completed Progress: 930 iterations completed Progress: 931 iterations completed Progress: 932 iterations completed Progress: 933 iterations completed Progress: 934 iterations completed Progress: 935 iterations completed Progress: 936 iterations completed Progress: 937 iterations completed Progress: 938 iterations completed Progress: 939 iterations completed Progress: 940 iterations completed Progress: 941 iterations completed Progress: 942 iterations completed Progress: 943 iterations completed Progress: 944 iterations completed Progress: 945 iterations completed Progress: 946 iterations completed Progress: 947 iterations completed Progress: 948 iterations completed Progress: 949 iterations completed Progress: 950 iterations completed Progress: 951 iterations completed Progress: 952 iterations completed Progress: 953 iterations completed Progress: 954 iterations completed Progress: 955 iterations completed Progress: 956 iterations completed Progress: 957 iterations completed Progress: 958 iterations completed Progress: 959 iterations completed Progress: 960 iterations completed Progress: 961 iterations completed Progress: 962 iterations completed Progress: 963 iterations completed Progress: 964 iterations completed Progress: 965 iterations completed Progress: 966 iterations completed Progress: 967 iterations completed Progress: 968 iterations completed Progress: 969 iterations completed Progress: 970 iterations completed Progress: 971 iterations completed Progress: 972 iterations completed Progress: 973 iterations completed Progress: 974 iterations completed Progress: 975 iterations completed Progress: 976 iterations completed Progress: 977 iterations completed Progress: 978 iterations completed Progress: 979 iterations completed Progress: 980 iterations completed Progress: 981 iterations completed Progress: 982 iterations completed Progress: 983 iterations completed Progress: 984 iterations completed Progress: 985 iterations completed Progress: 986 iterations completed Progress: 987 iterations completed Progress: 988 iterations completed Progress: 989 iterations completed Progress: 990 iterations completed Progress: 991 iterations completed Progress: 992 iterations completed Progress: 993 iterations completed Progress: 994 iterations completed Progress: 995 iterations completed Progress: 996 iterations completed Progress: 997 iterations completed Progress: 998 iterations completed Progress: 999 iterations completed Progress: 1000 iterations completed # Average the correlation matrix over the 1000 iterations avg_cor_matrix &lt;- cor_matrix_sum / 1000 # Print the averaged correlation matrix print(&quot;Averaged Correlation Matrix:&quot;) ## [1] &quot;Averaged Correlation Matrix:&quot; print(avg_cor_matrix) ## lm_pred rf_pred tuned_gbm_pred knn_pred ## lm_pred 1.0000000 0.9014258 0.8926652 0.8656307 ## rf_pred 0.9014258 1.0000000 0.9769452 0.9341390 ## tuned_gbm_pred 0.8926652 0.9769452 1.0000000 0.9154042 ## knn_pred 0.8656307 0.9341390 0.9154042 1.0000000 "],["intro_econ_stats.html", "Chapter 15 14_Intro_Econ_Stats", " Chapter 15 14_Intro_Econ_Stats Work in progess "],["nfl.html", "Chapter 16 NFL 16.1 WBs 16.2 RBs 16.3 QBs 16.4 Plots and Analysis 16.5 Lists to do 16.6 team mates 16.7 Testing", " Chapter 16 NFL 16.1 WBs library(readr) library(randomForest) library(caret) library(tidyr) library(pdp) library(dplyr) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Sort the dataset by player name and season data &lt;- data[order(data$name, data$season), ] names(data)[names(data) == &quot;height_cm&quot;] &lt;- &quot;height_in&quot; data &lt;- data[, !names(data) %in% c(&quot;height_ft&quot;)] # Split the dataset by position data_split &lt;- split(data, data$position) # Access the WR data wr_data &lt;- data_split$WR # Identify zero-variance columns zero_var_cols &lt;- nearZeroVar(wr_data) # Check and remove zero-variance columns, excluding &#39;position&#39; zero_var_colnames &lt;- colnames(wr_data)[zero_var_cols] cols_to_keep &lt;- names(wr_data)[!(names(wr_data) %in% zero_var_colnames) | names(wr_data) == &quot;position&quot;] wr_data &lt;- wr_data[, cols_to_keep] # Filter the data for the 2023 season to save it before we kill it wr_2023 &lt;- subset(wr_data, season == 2023) # Create new columns for the next season&#39;s fantasy points wr_data$next_fantasy_points &lt;- ave(wr_data$fantasy_points, wr_data$name, FUN = function(x) c(x[-1], NA)) wr_data$next_fantasy_points_ppr &lt;- ave(wr_data$fantasy_points_ppr, wr_data$name, FUN = function(x) c(x[-1], NA)) # View the first few rows of the updated data head(data) ## # A tibble: 6 × 75 ## id name height_in position team season season_type completions attempts ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A.J. B… 73 WR TEN 2019 REG 0 0 ## 2 1 A.J. B… 72 WR TEN 2020 REG 0 0 ## 3 1 A.J. B… 73 WR TEN 2021 REG 0 2 ## 4 1 A.J. B… 73 WR PHI 2022 REG 0 0 ## 5 1 A.J. B… 73 WR PHI 2023 REG 0 0 ## 6 2 A.J. D… 77 TE DEN 2016 REG 0 0 ## # ℹ 66 more variables: passing_yards &lt;dbl&gt;, passing_tds &lt;dbl&gt;, ## # interceptions &lt;dbl&gt;, sacks &lt;dbl&gt;, sack_yards &lt;dbl&gt;, sack_fumbles &lt;dbl&gt;, ## # sack_fumbles_lost &lt;dbl&gt;, passing_air_yards &lt;dbl&gt;, ## # passing_yards_after_catch &lt;dbl&gt;, passing_first_downs &lt;dbl&gt;, ## # passing_2pt_conversions &lt;dbl&gt;, carries &lt;dbl&gt;, rushing_yards &lt;dbl&gt;, ## # rushing_tds &lt;dbl&gt;, rushing_fumbles &lt;dbl&gt;, rushing_fumbles_lost &lt;dbl&gt;, ## # rushing_first_downs &lt;dbl&gt;, rushing_2pt_conversions &lt;dbl&gt;, … wr_data &lt;- wr_data[wr_data$rookie_season != 0, ] # Count the number of NA values in each column na_table &lt;- colSums(is.na(wr_data)) print(na_table) ## id name ## 0 0 ## height_in position ## 0 0 ## team season ## 0 0 ## attempts carries ## 0 0 ## rushing_first_downs receptions ## 0 0 ## targets receiving_yards ## 0 0 ## receiving_tds receiving_fumbles ## 0 0 ## receiving_fumbles_lost receiving_air_yards ## 0 0 ## receiving_yards_after_catch receiving_first_downs ## 0 0 ## receiving_2pt_conversions target_share ## 0 0 ## air_yards_share fantasy_points ## 0 0 ## fantasy_points_ppr total_yards ## 0 0 ## games offense_snaps ## 0 0 ## teams_offense_snaps ypc ## 0 0 ## ypr touches ## 0 0 ## rec_td_percentage total_tds ## 0 0 ## td_percentage offense_pct ## 0 77 ## rush_ypg rec_ypg ## 0 0 ## ppg ppr_ppg ## 0 0 ## yps ypg ## 4 0 ## rookie_season round ## 0 0 ## forty bench ## 0 0 ## vertical years_played ## 0 0 ## fp_ps ppr_fp_ps ## 2 2 ## next_fantasy_points next_fantasy_points_ppr ## 675 675 # For now, remove all rows with NAs wr_data &lt;- na.omit(wr_data) # Drop the next_fantasy_points_ppr column wr_data &lt;- wr_data[, !names(wr_data) %in% &quot;next_fantasy_points_ppr&quot;] # Build Random Forest model wrrf_model &lt;- randomForest(next_fantasy_points ~ ., data = wr_data, ntree = 500, importance = TRUE) # Create a simplified dataset for linear regression lm_data &lt;- wr_data[, !names(wr_data) %in% c(&quot;name&quot;, &quot;id&quot;, &quot;position&quot;)] # Initialize a vector to store RMSPE values for each run rmspe &lt;- c() for(i in 1:10) { # Resample the dataset with replacement ind &lt;- unique(sample(nrow(lm_data), nrow(lm_data), replace = TRUE)) train &lt;- lm_data[ind, ] test &lt;- lm_data[-ind, ] # Build Linear Regression model lm_model &lt;- lm(next_fantasy_points ~ ., data = train) # Predict on the test data yhat &lt;- predict(lm_model, newdata = test) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((test$next_fantasy_points - yhat)^2)) } ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases # Calculate the mean RMSPE over the 10 runs mean_rmspe &lt;- mean(rmspe) print(paste(&quot;Mean RMSPE over 10 runs:&quot;, mean_rmspe)) ## [1] &quot;Mean RMSPE over 10 runs: 40.3900949556639&quot; # Optionally, print the RMSPE for each run to see variability print(rmspe) ## [1] 41.51607 38.92039 40.43129 40.48614 39.86521 40.58027 38.78137 41.11945 ## [9] 42.87032 39.33045 # Evaluate feature importance importance_values &lt;- importance(wrrf_model) # Sort and extract the top 10 features by %IncMSE and IncNodePurity top10_mse &lt;- importance_values[order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE), ][1:10, ] top10_purity &lt;- importance_values[order(importance_values[, &quot;IncNodePurity&quot;], decreasing = TRUE), ][1:10, ] # Visualize top 10 features by %IncMSE par(mar = c(5, 12, 4, 2)) # Increase margins for labels barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Mean Decrease in Accuracy)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Visualize top 10 features by IncNodePurity barplot(top10_purity[, &quot;IncNodePurity&quot;], names.arg = rownames(top10_purity), main = &quot;Top 10 Features by IncNodePurity (Mean Decrease in Gini)&quot;, las = 2, col = &quot;red&quot;, horiz = TRUE, cex.names = 0.8) oob_mse &lt;- wrrf_model$mse[wrrf_model$ntree] oobrmse &lt;- sqrt(oob_mse) # Print the OOB MSE print(paste(&quot;OOB rMSE:&quot;, oobrmse)) ## [1] &quot;OOB rMSE: 40.8993809346341&quot; # Extract OOB predictions oob_predictions &lt;- wrrf_model$predicted # Calculate the absolute errors absolute_errors &lt;- abs(oob_predictions - wrrf_model$y) # Calculate the Mean Absolute Error (MAE) oob_mae &lt;- mean(absolute_errors) # Print the OOB MAE print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 31.0312370753633&quot; # Predict 2024 fantasy points for 2023 WRs predicted_2024_fantasy_points &lt;- predict(wrrf_model, newdata = wr_2023) # Create a data frame with the names and the predicted 2024 fantasy points results_wr &lt;- data.frame(name = wr_2023$name, predicted_2024_fantasy_points = predicted_2024_fantasy_points) print(results_wr) ## name predicted_2024_fantasy_points ## 1 A.J. Brown 157.17480 ## 2 A.T. Perry 46.98369 ## 3 Adam Thielen 124.63056 ## 4 Alec Pierce 67.58696 ## 5 Alex Erickson 37.06521 ## 6 Allen Lazard 49.83097 ## 7 Allen Robinson 47.93763 ## 8 Amari Cooper 135.74772 ## 9 Amari Rodgers 25.25656 ## 10 Amon-Ra St. Brown 190.62602 ## 11 Andrei Iosivas 37.23415 ## 12 Antoine Green 25.17406 ## 13 Austin Trammell 18.80340 ## 14 Ben Skowronek 19.88537 ## 15 Bo Melton 59.66029 ## 16 Brandin Cooks 107.68374 ## 17 Brandon Aiyuk 146.37277 ## 18 Brandon Johnson 44.39556 ## 19 Brandon Powell 42.02138 ## 20 Braxton Berrios 39.60880 ## 21 Britain Covey 17.08505 ## 22 Byron Pringle 26.51074 ## 23 Calvin Austin 39.43761 ## 24 Calvin Ridley 130.29700 ## 25 Cedric Tillman 44.89559 ## 26 CeeDee Lamb 187.75591 ## 27 Charlie Jones 24.83876 ## 28 Chase Claypool 12.38027 ## 29 Chris Conley 52.30433 ## 30 Chris Godwin 115.05250 ## 31 Chris Moore 42.57187 ## 32 Chris Olave 140.80677 ## 33 Christian Kirk 97.79042 ## 34 Christian Watson 78.28672 ## 35 Cody Thompson 14.12964 ## 36 Collin Johnson 10.41027 ## 37 Colton Dowell 17.19781 ## 38 Cooper Kupp 108.32921 ## 39 Courtland Sutton 112.63547 ## 40 Curtis Samuel 73.59254 ## 41 D&#39;Wayne Eskridge 16.71575 ## 42 D.J. Chark NA ## 43 D.J. Montgomery 22.14999 ## 44 D.J. Moore 155.89027 ## 45 Darius Slayton 68.79059 ## 46 Darnell Mooney 55.90383 ## 47 Davante Adams 134.72572 ## 48 David Bell 35.11340 ## 49 David Moore 18.23128 ## 50 DeAndre Carter 18.77411 ## 51 DeAndre Hopkins 123.21165 ## 52 Deebo Samuel 127.33945 ## 53 Demarcus Robinson 54.56798 ## 54 Demario Douglas 73.42642 ## 55 Deonte Harty 24.25080 ## 56 Derius Davis 35.31692 ## 57 DeVante Parker 40.49505 ## 58 Deven Thompkins 28.03221 ## 59 Devin Duvernay 18.41180 ## 60 DeVonta Smith 134.73927 ## 61 Diontae Johnson 107.77734 ## 62 DK Metcalf NA ## 63 Donovan Peoples-Jones 13.95773 ## 64 Dontayvion Wicks 69.02956 ## 65 Drake London 94.35400 ## 66 Dyami Brown 34.43046 ## 67 Elijah Cooks 16.03892 ## 68 Elijah Moore 77.19134 ## 69 Equanimeous St. Brown 12.21726 ## 70 Erik Ezukanma 21.20262 ## 71 Gabe Davis NA ## 72 Garrett Wilson 118.87253 ## 73 George Pickens 119.96120 ## 74 Greg Dortch 44.71445 ## 75 Gunner Olszewski 20.97760 ## 76 Hunter Renfrow 40.33806 ## 77 Ihmir Smith-Marsette 26.83299 ## 78 Irvin Charles 17.86954 ## 79 Isaiah Hodgins 42.12548 ## 80 Isaiah McKenzie 25.54656 ## 81 Ja&#39;Marr Chase 137.89821 ## 82 Jahan Dotson 56.38782 ## 83 Jake Bobo 37.67511 ## 84 Jakobi Meyers 113.42291 ## 85 Jalen Brooks 23.39124 ## 86 Jalen Guyton 21.51121 ## 87 Jalen Nailor 19.39478 ## 88 Jalen Reagor 25.13487 ## 89 Jalen Tolbert 42.11227 ## 90 Jalin Hyatt 61.83907 ## 91 Jamal Agnew 32.79430 ## 92 James Proche 30.55856 ## 93 Jameson Williams 49.73451 ## 94 Jamison Crowder 29.39799 ## 95 Jason Brownlee 27.34987 ## 96 Jauan Jennings 36.10466 ## 97 Jaxon Smith-Njigba 82.61150 ## 98 Jayden Reed 129.16471 ## 99 Jaylen Waddle 133.38376 ## 100 Jerry Jeudy 77.73054 ## 101 Jonathan Mingo 77.55108 ## 102 Jordan Addison 122.85956 ## 103 Josh Downs 85.10446 ## 104 Josh Palmer 77.84407 ## 105 Josh Reynolds 54.59260 ## 106 JuJu Smith-Schuster 39.12024 ## 107 Julio Jones 23.73418 ## 108 Justin Jefferson 137.10834 ## 109 Justin Watson 60.80272 ## 110 Justyn Ross 16.38659 ## 111 Juwann Winfree 14.04169 ## 112 K.J. Osborn 47.56511 ## 113 Kadarius Toney 42.00206 ## 114 Kalif Raymond 50.26786 ## 115 Kayshon Boutte 21.53693 ## 116 Keelan Doss 19.73776 ## 117 Keenan Allen 150.16159 ## 118 Keith Kirkwood 13.79941 ## 119 Kendrick Bourne 69.14235 ## 120 KhaDarel Hodge 34.60650 ## 121 Khalil Shakir 65.89405 ## 122 Kwamie Lassiter NA ## 123 Kyle Philips 27.48208 ## 124 Laquon Treadwell 21.12392 ## 125 Laviska Shenault NA ## 126 Lil&#39;Jordan Humphrey 35.20077 ## 127 Lynn Bowden NA ## 128 Mack Hollins 32.42796 ## 129 Malik Heath 31.37605 ## 130 Malik Taylor 11.55329 ## 131 Marquez Valdes-Scantling 48.59948 ## 132 Marquise Brown 75.42577 ## 133 Marquise Goodwin 25.62864 ## 134 Marvin Jones 14.59955 ## 135 Marvin Mims 58.28685 ## 136 Mason Kinsey 16.04117 ## 137 Mecole Hardman 28.05350 ## 138 Michael Gallup 34.03682 ## 139 Michael Pittman NA ## 140 Michael Thomas 49.86298 ## 141 Michael Wilson 74.66154 ## 142 Mike Evans 135.53877 ## 143 Mike Strachan 18.57186 ## 144 Mike Williams 65.83951 ## 145 Miles Boykin 11.39638 ## 146 Nelson Agholor 41.51334 ## 147 Nick Westbrook-Ikhine 42.13301 ## 148 Nico Collins 142.83580 ## 149 Noah Brown 67.31974 ## 150 Olamide Zaccheaus 31.70466 ## 151 Parker Washington 31.81540 ## 152 Parris Campbell 25.62103 ## 153 Phillip Dorsett 24.68863 ## 154 Puka Nacua 181.00701 ## 155 Quentin Johnston 58.12017 ## 156 Quez Watkins 34.27737 ## 157 Rakim Jarrett 20.59234 ## 158 Randall Cobb 25.24437 ## 159 Rashee Rice 105.47835 ## 160 Rashid Shaheed 86.25315 ## 161 Rashod Bateman 57.45422 ## 162 Ray-Ray McCloud 24.92795 ## 163 Richie James 23.33708 ## 164 River Cracraft 16.48914 ## 165 Robbie Chosen 22.14739 ## 166 Robert Woods 52.70001 ## 167 Romeo Doubs 83.70492 ## 168 Rondale Moore 55.50335 ## 169 Ronnie Bell 34.19325 ## 170 Samori Toure 22.00546 ## 171 Scott Miller 29.56553 ## 172 Shedrick Jackson 21.24781 ## 173 Simi Fehoko 19.90172 ## 174 Skyy Moore 34.87768 ## 175 Stefon Diggs 133.00342 ## 176 Sterling Shepard 27.71374 ## 177 Steven Sims 17.06804 ## 178 Tank Dell 128.01543 ## 179 Tee Higgins 88.82843 ## 180 Terrace Marshall NA ## 181 Terry McLaurin 108.61167 ## 182 Tim Jones 34.24164 ## 183 Tre Tucker 58.91104 ## 184 Trent Sherfield 23.04970 ## 185 Trent Taylor 19.23744 ## 186 Trenton Irwin 36.91050 ## 187 Trey Palmer 59.69736 ## 188 Treylon Burks 52.11504 ## 189 Trishton Jackson 14.46942 ## 190 Tutu Atwell 54.53558 ## 191 Ty Montgomery 15.34042 ## 192 Tylan Wallace 13.92826 ## 193 Tyler Boyd 54.53340 ## 194 Tyler Johnson 21.84475 ## 195 Tyler Lockett 86.67339 ## 196 Tyler Scott 36.00726 ## 197 Tyquan Thornton 26.71170 ## 198 Tyreek Hill 200.60328 ## 199 Van Jefferson 44.75561 ## 200 Velus Jones NA ## 201 Wan&#39;Dale Robinson 69.80029 ## 202 Willie Snead 14.29176 ## 203 Xavier Gipson 46.30136 ## 204 Xavier Hutchinson 41.41602 ## 205 Zach Pascal 26.24253 ## 206 Zay Flowers 109.98446 ## 207 Zay Jones 59.75285 mse of specific seasons? we need to factor in the qbs and teamates of the players 16.2 RBs # Access the RB data rb_data &lt;- data_split$RB # Identify zero-variance columns zero_var_cols &lt;- nearZeroVar(rb_data) # Check and remove zero-variance columns, excluding &#39;position&#39; zero_var_colnames &lt;- colnames(rb_data)[zero_var_cols] cols_to_keep &lt;- names(rb_data)[!(names(rb_data) %in% zero_var_colnames) | names(rb_data) == &quot;position&quot;] rb_data &lt;- rb_data[, cols_to_keep] rb_2023 &lt;- subset(rb_data, season == 2023) # Create new columns for the next season&#39;s fantasy points rb_data$next_fantasy_points &lt;- ave(rb_data$fantasy_points, rb_data$name, FUN = function(x) c(x[-1], NA)) rb_data$next_fantasy_points_ppr &lt;- ave(rb_data$fantasy_points_ppr, rb_data$name, FUN = function(x) c(x[-1], NA)) # View the first few rows of the updated data head(rb_data) ## # A tibble: 6 × 55 ## id name height_in position team season carries rushing_yards rushing_tds ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 A.J. … 72 RB GB 2020 46 242 2 ## 2 3 A.J. … 72 RB GB 2021 187 803 5 ## 3 3 A.J. … 72 RB GB 2022 186 770 7 ## 4 10 Aaron… 69 RB GB 2017 81 448 4 ## 5 10 Aaron… 69 RB GB 2018 133 728 8 ## 6 10 Aaron… 69 RB GB 2019 236 1084 16 ## # ℹ 46 more variables: rushing_fumbles &lt;dbl&gt;, rushing_fumbles_lost &lt;dbl&gt;, ## # rushing_first_downs &lt;dbl&gt;, rushing_2pt_conversions &lt;dbl&gt;, receptions &lt;dbl&gt;, ## # targets &lt;dbl&gt;, receiving_yards &lt;dbl&gt;, receiving_tds &lt;dbl&gt;, ## # receiving_fumbles &lt;dbl&gt;, receiving_fumbles_lost &lt;dbl&gt;, ## # receiving_air_yards &lt;dbl&gt;, receiving_yards_after_catch &lt;dbl&gt;, ## # receiving_first_downs &lt;dbl&gt;, target_share &lt;dbl&gt;, air_yards_share &lt;dbl&gt;, ## # fantasy_points &lt;dbl&gt;, fantasy_points_ppr &lt;dbl&gt;, total_yards &lt;dbl&gt;, … rb_data &lt;- rb_data[rb_data$rookie_season != 0, ] # Count the number of NA values in each column na_table &lt;- colSums(is.na(rb_data)) print(na_table) ## id name ## 0 0 ## height_in position ## 0 0 ## team season ## 0 0 ## carries rushing_yards ## 0 0 ## rushing_tds rushing_fumbles ## 0 0 ## rushing_fumbles_lost rushing_first_downs ## 0 0 ## rushing_2pt_conversions receptions ## 0 0 ## targets receiving_yards ## 0 0 ## receiving_tds receiving_fumbles ## 0 0 ## receiving_fumbles_lost receiving_air_yards ## 0 0 ## receiving_yards_after_catch receiving_first_downs ## 0 0 ## target_share air_yards_share ## 0 0 ## fantasy_points fantasy_points_ppr ## 0 0 ## total_yards games ## 0 0 ## offense_snaps teams_offense_snaps ## 0 0 ## ypc ypr ## 0 0 ## touches rush_td_percentage ## 0 0 ## rec_td_percentage total_tds ## 0 0 ## td_percentage offense_pct ## 0 50 ## rush_ypg rec_ypg ## 0 0 ## ppg ppr_ppg ## 0 0 ## yps ypg ## 0 0 ## rookie_season round ## 0 0 ## overall forty ## 0 0 ## bench vertical ## 0 0 ## years_played fp_ps ## 0 0 ## ppr_fp_ps next_fantasy_points ## 0 429 ## next_fantasy_points_ppr ## 429 # For now, remove all rows with NAs rb_data &lt;- na.omit(rb_data) # Drop the next_fantasy_points_ppr column rb_data &lt;- rb_data[, !names(rb_data) %in% &quot;next_fantasy_points_ppr&quot;] # Build Random Forest model rbrf_model &lt;- randomForest(next_fantasy_points ~ ., data = rb_data, ntree = 500, importance = TRUE) # Create a simplified dataset for linear regression lm_data &lt;- rb_data[, !names(rb_data) %in% c(&quot;name&quot;, &quot;id&quot;, &quot;position&quot;)] # Initialize a vector to store RMSPE values for each run rmspe &lt;- c() for(i in 1:10) { # Resample the dataset with replacement ind &lt;- unique(sample(nrow(lm_data), nrow(lm_data), replace = TRUE)) train &lt;- lm_data[ind, ] test &lt;- lm_data[-ind, ] # Build Linear Regression model lm_model &lt;- lm(next_fantasy_points ~ ., data = train) # Predict on the test data yhat &lt;- predict(lm_model, newdata = test) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((test$next_fantasy_points - yhat)^2)) } ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases # Calculate the mean RMSPE over the 10 runs mean_rmspe &lt;- mean(rmspe) print(paste(&quot;Mean RMSPE over 10 runs:&quot;, mean_rmspe)) ## [1] &quot;Mean RMSPE over 10 runs: 59.7769931450521&quot; # Optionally, print the RMSPE for each run to see variability print(rmspe) ## [1] 57.32384 62.04078 55.41853 61.09091 59.29520 64.96443 62.44621 58.12239 ## [9] 59.46607 57.60159 # Evaluate feature importance importance_values &lt;- importance(rbrf_model) # Sort and extract the top 10 features by %IncMSE and IncNodePurity top10_mse &lt;- importance_values[order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE), ][1:10, ] top10_purity &lt;- importance_values[order(importance_values[, &quot;IncNodePurity&quot;], decreasing = TRUE), ][1:10, ] # Visualize top 10 features by %IncMSE par(mar = c(5, 12, 4, 2)) # Increase margins for labels barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Mean Decrease in Accuracy)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Visualize top 10 features by IncNodePurity barplot(top10_purity[, &quot;IncNodePurity&quot;], names.arg = rownames(top10_purity), main = &quot;Top 10 Features by IncNodePurity (Mean Decrease in Gini)&quot;, las = 2, col = &quot;red&quot;, horiz = TRUE, cex.names = 0.8) # Calculate OOB MAE # Extract OOB predictions oob_predictions &lt;- rbrf_model$predicted # Calculate the absolute errors absolute_errors &lt;- abs(oob_predictions - rbrf_model$y) # Calculate the Mean Absolute Error (MAE) oob_mae &lt;- mean(absolute_errors) print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 42.7118053512281&quot; # Predict 2024 fantasy points for 2023 RBs predicted_2024_fantasy_points &lt;- predict(rbrf_model, newdata = rb_2023) # Create a data frame with the names and the predicted 2024 fantasy points results_rb &lt;- data.frame(name = rb_2023$name, predicted_2024_fantasy_points = predicted_2024_fantasy_points) print(results_rb) ## name predicted_2024_fantasy_points ## 1 Aaron Jones 121.52164 ## 2 Alex Armah 22.45872 ## 3 Alexander Mattison 88.98290 ## 4 Alvin Kamara 168.75066 ## 5 Ameer Abdullah 45.42838 ## 6 Anthony McFarland NA ## 7 Antonio Gibson 68.47607 ## 8 Austin Ekeler 157.71401 ## 9 Bijan Robinson 136.19888 ## 10 Boston Scott 19.84746 ## 11 Brandon Bolden 47.05817 ## 12 Breece Hall 158.69111 ## 13 Brian Robinson NA ## 14 Cam Akers 41.44165 ## 15 Chase Brown 78.15063 ## 16 Chase Edmonds 50.74780 ## 17 Chris Evans 45.44282 ## 18 Chris Rodriguez 64.28221 ## 19 Christian McCaffrey 224.10372 ## 20 Chuba Hubbard 117.01363 ## 21 Clyde Edwards-Helaire 46.58028 ## 22 Cordarrelle Patterson 36.71840 ## 23 Craig Reynolds 34.94989 ## 24 D&#39;Andre Swift 115.76660 ## 25 D&#39;Ernest Johnson 33.43723 ## 26 D&#39;Onta Foreman 56.90359 ## 27 Dameon Pierce 86.84517 ## 28 Damien Harris 22.04393 ## 29 Damien Williams 24.27407 ## 30 Dare Ogunbowale 42.36622 ## 31 Darrell Henderson 54.99883 ## 32 Darrynton Evans 43.62251 ## 33 David Montgomery 127.46178 ## 34 DeeJay Dallas 30.55839 ## 35 Deon Jackson 40.77308 ## 36 Derrick Gore 46.33060 ## 37 Derrick Henry 118.73975 ## 38 Deuce Vaughn 27.81279 ## 39 Devin Singletary 116.18461 ## 40 Devine Ozigbo 11.53234 ## 41 Elijah Dotson 26.37480 ## 42 Elijah Mitchell 49.92732 ## 43 Emanuel Wilson 21.89009 ## 44 Emari Demercado 68.05451 ## 45 Eric Gray 29.33684 ## 46 Evan Hull 40.49006 ## 47 Ezekiel Elliott 87.33393 ## 48 Gary Brightwell 20.36802 ## 49 Gus Edwards 102.36925 ## 50 Hunter Luepke 21.60830 ## 51 Isaiah Spiller 31.22534 ## 52 Isiah Pacheco 153.48213 ## 53 Israel Abanikanda 25.03228 ## 54 J.K. Dobbins 46.00932 ## 55 Jahmyr Gibbs 161.34515 ## 56 Jaleel McLaughlin 117.62059 ## 57 Jamaal Williams 47.76322 ## 58 James Conner 139.50478 ## 59 James Cook 125.89687 ## 60 Jashaun Corbin 36.41184 ## 61 Javonte Williams 88.43398 ## 62 Jaylen Warren 117.88615 ## 63 Jerick McKinnon 63.48359 ## 64 Jerome Ford 114.92145 ## 65 Joe Mixon 154.71751 ## 66 Jonathan Taylor 138.01398 ## 67 Jonathan Ward 23.20985 ## 68 Jonathan Williams 47.69419 ## 69 Jordan Mason 50.65410 ## 70 Jordan Mims 47.67516 ## 71 Josh Jacobs 155.74471 ## 72 Joshua Kelley 63.98855 ## 73 Justice Hill 69.29128 ## 74 Kareem Hunt 61.24182 ## 75 Keaton Mitchell 74.06378 ## 76 Kendre Miller 56.04212 ## 77 Kene Nwangwu 16.90155 ## 78 Kenneth Gainwell 63.62875 ## 79 Kenneth Walker NA ## 80 Kenyan Drake 45.96875 ## 81 Kevin Harris 31.43699 ## 82 Khalil Herbert 85.49128 ## 83 Kyren Williams 176.52749 ## 84 La&#39;Mical Perine 37.17253 ## 85 Latavius Murray 58.14279 ## 86 Leonard Fournette 39.26199 ## 87 Matt Breida 40.50366 ## 88 Melvin Gordon 28.27088 ## 89 Michael Carter 40.43726 ## 90 Miles Sanders 61.48906 ## 91 Najee Harris 104.31847 ## 92 Nick Chubb 67.48493 ## 93 Patrick Taylor 41.53899 ## 94 Pierre Strong 71.70650 ## 95 Rachaad White 146.56125 ## 96 Raheem Blackshear 16.50377 ## 97 Raheem Mostert 136.00833 ## 98 Rashaad Penny 27.96484 ## 99 Rhamondre Stevenson 130.06218 ## 100 Rico Dowdle 76.35671 ## 101 Ronnie Rivers 47.70888 ## 102 Roschon Johnson 102.62134 ## 103 Royce Freeman 47.10223 ## 104 Salvon Ahmed 23.81889 ## 105 Samaje Perine 76.38192 ## 106 Saquon Barkley 141.81715 ## 107 Sean Tucker 20.89360 ## 108 Tank Bigsby 74.00957 ## 109 Tony Jones 35.20343 ## 110 Tony Pollard 145.95358 ## 111 Travis Etienne 158.14376 ## 112 Travis Homer 50.87992 ## 113 Trayveon Williams 44.68174 ## 114 Trey Sermon 37.98310 ## 115 Ty Chandler 97.94345 ## 116 Ty Johnson 28.79768 ## 117 Tyjae Spears 119.92003 ## 118 Tyler Allgeier 122.23780 ## 119 Tyler Goodson 29.02777 ## 120 Tyrion Davis-Price 50.26586 ## 121 Zach Charbonnet 113.55740 ## 122 Zach Evans 43.02850 ## 123 Zack Moss 101.57533 ## 124 Zamir White 90.98568 16.3 QBs # Access the QB data qb_data &lt;- data_split$QB # Identify zero-variance columns zero_var_cols &lt;- nearZeroVar(qb_data) # Check and remove zero-variance columns zero_var_colnames &lt;- colnames(qb_data)[zero_var_cols] qb_data &lt;- qb_data[, !names(qb_data) %in% zero_var_colnames] # Filter the data for the 2023 season qb_2023 &lt;- subset(qb_data, season == 2023) # Create new columns for the next season&#39;s fantasy points qb_data$next_fantasy_points &lt;- ave(qb_data$fantasy_points, qb_data$name, FUN = function(x) c(x[-1], NA)) # Remove rows where rookie_season is 0 qb_data &lt;- qb_data[qb_data$rookie_season != 0, ] # Remove rows with NAs qb_data &lt;- na.omit(qb_data) # Build Random Forest model qbrf_model &lt;- randomForest(next_fantasy_points ~ ., data = qb_data, ntree = 500, importance = TRUE) # Evaluate feature importance importance_values &lt;- importance(qbrf_model) # Sort and extract the top 10 features by %IncMSE and IncNodePurity top10_mse &lt;- importance_values[order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE), ][1:10, ] top10_purity &lt;- importance_values[order(importance_values[, &quot;IncNodePurity&quot;], decreasing = TRUE), ][1:10, ] # Visualize top 10 features by %IncMSE par(mar = c(5, 12, 4, 2)) # Increase margins for labels barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Mean Decrease in Accuracy)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Visualize top 10 features by IncNodePurity barplot(top10_purity[, &quot;IncNodePurity&quot;], names.arg = rownames(top10_purity), main = &quot;Top 10 Features by IncNodePurity (Mean Decrease in Gini)&quot;, las = 2, col = &quot;red&quot;, horiz = TRUE, cex.names = 0.8) # Calculate OOB rmse oob_rmse &lt;- sqrt(qbrf_model$mse[qbrf_model$ntree]) # Extract OOB predictions oob_predictions &lt;- qbrf_model$predicted # Calculate the absolute errors absolute_errors &lt;- abs(oob_predictions - qbrf_model$y) # Calculate the Mean Absolute Error (MAE) oob_mae &lt;- mean(absolute_errors) print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 62.1728672351168&quot; oob_rmse ## [1] 82.75093 # Predict 2024 fantasy points for 2023 QBs predicted_2024_fantasy_points &lt;- predict(qbrf_model, newdata = qb_2023) # Create a data frame with the names and the predicted 2024 fantasy points results_qb &lt;- data.frame(name = qb_2023$name, predicted_2024_fantasy_points = predicted_2024_fantasy_points) print(head(results_qb)) ## name predicted_2024_fantasy_points ## 1 Aaron Rodgers 76.52449 ## 2 Aidan O&#39;Connell 124.34640 ## 3 AJ McCarron NA ## 4 Andy Dalton 95.89981 ## 5 Anthony Richardson 153.95453 ## 6 Bailey Zappe 130.72803 16.4 Plots and Analysis # For QB model (qbrf_model) pdp_qb_games &lt;- partial(qbrf_model, pred.var = &quot;games&quot;, plot = TRUE, main = &quot;PDP for Games (QB Random Forest Model)&quot;, xlab = &quot;Games&quot;, ylab = &quot;Partial Dependence&quot;) # For WR model (wrrf_model) pdp_wr_games &lt;- partial(wrrf_model, pred.var = &quot;games&quot;, plot = TRUE, main = &quot;PDP for Games (WR Random Forest Model)&quot;, xlab = &quot;Games&quot;, ylab = &quot;Partial Dependence&quot;) # For RB model (rbrf_model) pdp_rb_games &lt;- partial(rbrf_model, pred.var = &quot;games&quot;, plot = TRUE, main = &quot;PDP for Games (RB Random Forest Model)&quot;, xlab = &quot;Games&quot;, ylab = &quot;Partial Dependence&quot;) pdp_qb_games pdp_wr_games pdp_rb_games Games played is being tracked by other statistics… i dont give a crap about games played i only care about your stats at the end of the season… 16.5 Lists to do need to look at if multiple years back is good idea(i dont think so because then can no use 2nd yr players), and to be able to add teamates. also look if we are good for the seasons that there are 17 games. break the seasons down into quarters. 16.6 team mates library(nflfastR) ## Warning: package &#39;nflfastR&#39; was built under R version 4.3.3 library(dplyr) # Fetch rosters for 2024 rosters &lt;- fast_scraper_roster(season = 2024) # Filter WR and RB positions wr_rosters &lt;- rosters %&gt;% filter(position == &quot;WR&quot;) rb_rosters &lt;- rosters %&gt;% filter(position == &quot;RB&quot;) # Rename columns to match merging requirements colnames(wr_2023)[colnames(wr_2023) == &quot;team.x&quot;] &lt;- &quot;team&quot; colnames(rb_2023)[colnames(rb_2023) == &quot;team.x&quot;] &lt;- &quot;team&quot; # Merge WR data with rosters wr_teams &lt;- merge(wr_2023, wr_rosters, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) colnames(wr_teams)[colnames(wr_teams) == &quot;team.x&quot;] &lt;- &quot;team&quot; # Use team.x for original team colnames(wr_teams)[colnames(wr_teams) == &quot;team.y&quot;] &lt;- &quot;new_team&quot; # Use team.y for updated team wr_teams &lt;- wr_teams[, c(&quot;name&quot;, &quot;team&quot;)] # Merge RB data with rosters rb_teams &lt;- merge(rb_2023, rb_rosters, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) colnames(rb_teams)[colnames(rb_teams) == &quot;team.x&quot;] &lt;- &quot;team&quot; # Use team.x for original team colnames(rb_teams)[colnames(rb_teams) == &quot;team.y&quot;] &lt;- &quot;new_team&quot; # Use team.y for updated team rb_teams &lt;- rb_teams[, c(&quot;name&quot;, &quot;team&quot;)] create wr_2023-2024 that has the 2023 stats that have the players on there correct teams. we will do the same for the rb data # Load the necessary libraries library(readr) library(randomForest) library(caret) library(tidyr) library(pdp) library(dplyr) library(nflfastR) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Sort the dataset by player name and season data &lt;- data[order(data$name, data$season), ] # Adjust columns names(data)[names(data) == &quot;height_cm&quot;] &lt;- &quot;height_in&quot; data &lt;- data[, !names(data) %in% c(&quot;height_ft&quot;)] # Split the dataset by position data_split &lt;- split(data, data$position) # Access the WR and RB data wr_data &lt;- data_split$WR rb_data &lt;- data_split$RB # Identify zero-variance columns and remove them, excluding &#39;position&#39; # For WR data zero_var_cols_wr &lt;- nearZeroVar(wr_data) zero_var_colnames_wr &lt;- colnames(wr_data)[zero_var_cols_wr] cols_to_remove_wr &lt;- zero_var_colnames_wr[zero_var_colnames_wr != &quot;position&quot;] wr_data &lt;- wr_data[, !names(wr_data) %in% cols_to_remove_wr] # For RB data zero_var_cols_rb &lt;- nearZeroVar(rb_data) zero_var_colnames_rb &lt;- colnames(rb_data)[zero_var_cols_rb] cols_to_remove_rb &lt;- zero_var_colnames_rb[zero_var_colnames_rb != &quot;position&quot;] rb_data &lt;- rb_data[, !names(rb_data) %in% cols_to_remove_rb] # Assuming the previous steps up to filtering the 2023 data are done # Filter the data for the 2023 season wr_2023 &lt;- subset(wr_data, season == 2023) rb_2023 &lt;- subset(rb_data, season == 2023) # Fetch current rosters for 2024 rosters &lt;- fast_scraper_roster(season = 2024) wr_rosters &lt;- rosters %&gt;% filter(position == &quot;WR&quot;) rb_rosters &lt;- rosters %&gt;% filter(position == &quot;RB&quot;) # For WR data wr_rosters_temp &lt;- wr_rosters[, c(&quot;full_name&quot;, &quot;team&quot;)] names(wr_rosters_temp)[names(wr_rosters_temp) == &quot;team&quot;] &lt;- &quot;team_roster&quot; wr_2023_2024 &lt;- merge(wr_2023, wr_rosters_temp, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) # Replace NA in team with team_roster if available wr_2023_2024$team &lt;- ifelse(is.na(wr_2023_2024$team_roster), wr_2023_2024$team, wr_2023_2024$team_roster) wr_2023_2024 &lt;- wr_2023_2024[, !names(wr_2023_2024) %in% &quot;team_roster&quot;] # For RB data rb_rosters_temp &lt;- rb_rosters[, c(&quot;full_name&quot;, &quot;team&quot;)] names(rb_rosters_temp)[names(rb_rosters_temp) == &quot;team&quot;] &lt;- &quot;team_roster&quot; rb_2023_2024 &lt;- merge(rb_2023, rb_rosters_temp, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) # Replace NA in team with team_roster if available rb_2023_2024$team &lt;- ifelse(is.na(rb_2023_2024$team_roster), rb_2023_2024$team, rb_2023_2024$team_roster) rb_2023_2024 &lt;- rb_2023_2024[, !names(rb_2023_2024) %in% &quot;team_roster&quot;] # Now, wr_2023_2024 and rb_2023_2024 should have the correct teams for the players in 2023, with fallback to original team if NA. now in wr_data and rb we need to look to include teamate stats. (we will group them for the 2023/2024 one later) we will create collums that have a the teamates states. to determine a teamate use the team and the season… now we obviously cant use every teamate. so for now we wil use the best qb on that team the 2 best wrs(that isnt themselves) and the best rb… now we have to determine what best is. best for qb will be pass yards. best for rbs will be rush yards. and best for wrs will be receving yards we may need to restructure it from the begining a bit because some colloms where dropped but u figure that out # Load the necessary libraries library(readr) library(dplyr) library(tidyr) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Sort the dataset by player name and season data &lt;- data[order(data$name, data$season), ] # Adjust columns names(data)[names(data) == &quot;height_cm&quot;] &lt;- &quot;height_in&quot; # Rename column data &lt;- data[, !names(data) %in% &quot;height_ft&quot;] # Remove height_ft column # Split the dataset by position data_split &lt;- split(data, data$position) # Access the WR, RB, and QB data wr_data &lt;- data_split$WR rb_data &lt;- data_split$RB qb_data &lt;- data_split$QB # Load the necessary libraries library(readr) library(dplyr) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Group by season and team to get a column with the names of all team members get_team_members &lt;- function(data) { data %&gt;% group_by(season, team) %&gt;% summarise(team_members = paste(name, collapse = &quot;, &quot;)) %&gt;% ungroup() } # Apply the function to WR, RB, and QB data wr_team_members &lt;- get_team_members(data_split$WR) ## `summarise()` has grouped output ## by &#39;season&#39;. You can override ## using the `.groups` argument. rb_team_members &lt;- get_team_members(data_split$RB) ## `summarise()` has grouped output ## by &#39;season&#39;. You can override ## using the `.groups` argument. qb_team_members &lt;- get_team_members(data_split$QB) ## `summarise()` has grouped output ## by &#39;season&#39;. You can override ## using the `.groups` argument. # Function to get team members get_team_members &lt;- function(data) { data %&gt;% group_by(season, team) %&gt;% summarise(team_members = paste(name, collapse = &quot;, &quot;)) %&gt;% ungroup() } # Apply the function to existing data wr_team_members &lt;- get_team_members(data_split$WR) ## `summarise()` has grouped output ## by &#39;season&#39;. You can override ## using the `.groups` argument. rb_team_members &lt;- get_team_members(data_split$RB) ## `summarise()` has grouped output ## by &#39;season&#39;. You can override ## using the `.groups` argument. qb_team_members &lt;- get_team_members(data_split$QB) ## `summarise()` has grouped output ## by &#39;season&#39;. You can override ## using the `.groups` argument. # Function to get top WRs by team get_top_wr_team_members &lt;- function(ranked_wr_data) { # Sort data ranked_wr_data &lt;- ranked_wr_data[order(ranked_wr_data$season, ranked_wr_data$team, ranked_wr_data$rank), ] # Function to get nth value safely get_nth &lt;- function(x, n) { if(length(x) &gt;= n) return(x[n]) return(NA) } # Split data by season and team split_data &lt;- split(ranked_wr_data, list(ranked_wr_data$season, ranked_wr_data$team)) # Create result data frame result_list &lt;- lapply(split_data, function(group) { # If the group is empty, return NULL if (nrow(group) == 0) { return(NULL) } data.frame( season = unique(group$season), team = unique(group$team), WR1_name = get_nth(group$name, 1), WR1_yards = get_nth(group$receiving_yards, 1), WR2_name = get_nth(group$name, 2), WR2_yards = get_nth(group$receiving_yards, 2), WR3_name = get_nth(group$name, 3), WR3_yards = get_nth(group$receiving_yards, 3), WR4_name = get_nth(group$name, 4), WR4_yards = get_nth(group$receiving_yards, 4), WR5_name = get_nth(group$name, 5), WR5_yards = get_nth(group$receiving_yards, 5), stringsAsFactors = FALSE ) }) # Filter out NULL entries result_list &lt;- Filter(Negate(is.null), result_list) # Combine all data frames into one result &lt;- do.call(rbind, result_list) # Create team_key result$team_key &lt;- paste(result$team, result$season, sep = &quot;_&quot;) # Reorder columns col_order &lt;- c(&quot;team_key&quot;, &quot;season&quot;, &quot;team&quot;, names(result)[!names(result) %in% c(&quot;team_key&quot;, &quot;season&quot;, &quot;team&quot;)]) result &lt;- result[, col_order] rownames(result) &lt;- NULL return(result) } # Create a function to calculate dense rank dense_rank_desc &lt;- function(x) { # Sort unique values in descending order sorted_unique &lt;- sort(unique(x), decreasing = TRUE) # Create rank mapping rank_map &lt;- seq_along(sorted_unique) names(rank_map) &lt;- sorted_unique # Return ranks rank_map[as.character(x)] } # Split data by season and team splits &lt;- split(wr_data, list(wr_data$season, wr_data$team)) # Function to process each group process_group &lt;- function(group) { if(nrow(group) == 0) return(NULL) # Calculate rank within group group$rank &lt;- dense_rank_desc(group$receiving_yards) # Select only needed columns cols_to_keep &lt;- c(&quot;season&quot;, &quot;team&quot;, &quot;name&quot;, &quot;receiving_yards&quot;, &quot;rank&quot;) group &lt;- group[, cols_to_keep[cols_to_keep %in% names(group)]] return(group) } # Apply processing to each group and combine results ranked_wr_data &lt;- do.call(rbind, lapply(splits, process_group)) # Reset row names rownames(ranked_wr_data) &lt;- NULL # Create WR summary wr_summary &lt;- get_top_wr_team_members(ranked_wr_data) # Check structure str(wr_summary) ## &#39;data.frame&#39;: 326 obs. of 13 variables: ## $ team_key : chr &quot;ARI_2016&quot; &quot;ARI_2017&quot; &quot;ARI_2018&quot; &quot;ARI_2019&quot; ... ## $ season : num 2016 2017 2018 2019 2020 ... ## $ team : chr &quot;ARI&quot; &quot;ARI&quot; &quot;ARI&quot; &quot;ARI&quot; ... ## $ WR1_name : chr &quot;Larry Fitzgerald&quot; &quot;Larry Fitzgerald&quot; &quot;Larry Fitzgerald&quot; &quot;Larry Fitzgerald&quot; ... ## $ WR1_yards: num 1023 1156 734 804 1407 ... ## $ WR2_name : chr &quot;J.J. Nelson&quot; &quot;J.J. Nelson&quot; &quot;Christian Kirk&quot; &quot;Christian Kirk&quot; ... ## $ WR2_yards: num 568 508 590 709 621 848 709 565 711 921 ... ## $ WR3_name : chr &quot;John Brown&quot; &quot;Jaron Brown&quot; &quot;Trent Sherfield&quot; &quot;Damiere Byrd&quot; ... ## $ WR3_yards: num 517 477 210 359 409 572 467 352 580 556 ... ## $ WR4_name : chr &quot;Jaron Brown&quot; &quot;John Brown&quot; &quot;Chad Williams&quot; &quot;Pharoh Cooper&quot; ... ## $ WR4_yards: num 187 299 171 243 224 435 414 280 210 504 ... ## $ WR5_name : chr &quot;Brittan Golden&quot; &quot;Brittan Golden&quot; &quot;J.J. Nelson&quot; &quot;Andy Isabella&quot; ... ## $ WR5_yards: num 82 70 64 189 173 208 236 19 12 102 ... 16.7 Testing # Create team_key and merge with wr_summary wr_data_final &lt;- wr_data wr_data_final$team_key &lt;- paste(wr_data_final$team, wr_data_final$season, sep = &quot;_&quot;) wr_data_final &lt;- merge(wr_data_final, wr_summary, by = &quot;team_key&quot;, all.x = TRUE) # Rename columns and create initial wr_dataf wr_dataf &lt;- wr_data_final names(wr_dataf)[names(wr_dataf) == &quot;season.x&quot;] &lt;- &quot;season&quot; names(wr_dataf)[names(wr_dataf) == &quot;team.x&quot;] &lt;- &quot;team&quot; names(wr_dataf)[names(wr_dataf) == &quot;name&quot;] &lt;- &quot;player_name&quot; # Remove ID column if it exists if(&quot;id&quot; %in% names(wr_dataf)) { wr_dataf &lt;- wr_dataf[, !names(wr_dataf) == &quot;id&quot;] } # Subset 2023 data wr_2023 &lt;- wr_dataf[wr_dataf$season == 2023, ] # Create next season&#39;s fantasy points create_next_points &lt;- function(points, player) { ave(points, player, FUN = function(x) c(x[-1], NA)) } wr_dataf$next_fantasy_points &lt;- create_next_points(wr_dataf$fantasy_points, wr_dataf$player_name) wr_dataf$next_fantasy_points_ppr &lt;- create_next_points(wr_dataf$fantasy_points_ppr, wr_dataf$player_name) # Filter rookie seasons wr_dataf &lt;- wr_dataf[wr_dataf$rookie_season != 0, ] wr_dataf &lt;- wr_dataf[wr_dataf$rookie_season != wr_dataf$season, ] # Remove next_fantasy_points_ppr wr_dataf$next_fantasy_points_ppr &lt;- NULL # Remove zero-variance columns n_distinct &lt;- function(x) length(unique(x)) zero_var_cols &lt;- names(wr_dataf)[sapply(wr_dataf, n_distinct) == 1] wr_dataf &lt;- wr_dataf[, !names(wr_dataf) %in% zero_var_cols] # Remove non-predictive columns columns_to_remove &lt;- c(&quot;player_name&quot;, &quot;team_key&quot;, &quot;WR2_name&quot;, &quot;WR1_name&quot;, &quot;RB3_name&quot;, &quot;RB2_name&quot;, &quot;RB1_name&quot;, &quot;QB_name&quot;, &quot;season&quot;, &quot;team&quot;, &quot;season.y&quot;, &quot;team.y&quot;) wr_dataf &lt;- wr_dataf[, !names(wr_dataf) %in% columns_to_remove] # Train Random Forest model rf_model &lt;- randomForest(next_fantasy_points ~ ., ntree = 1200, data = wr_dataf, importance = TRUE, na.action = na.omit) # Calculate performance metrics oob_predictions &lt;- rf_model$predicted absolute_errors &lt;- abs(oob_predictions - rf_model$y) oob_mae &lt;- mean(absolute_errors) oob_mse &lt;- rf_model$mse[1200] oobrmse &lt;- sqrt(oob_mse) # Print metrics print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 33.312207333587&quot; print(paste(&quot;OOB rMSE:&quot;, oobrmse)) ## [1] &quot;OOB rMSE: 43.6783743687123&quot; # Feature importance importance_values &lt;- importance(rf_model) ordered_importance &lt;- order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE) top10_mse &lt;- importance_values[ordered_importance[1:10], ] # Plot importance par(mar = c(5, 12, 4, 2)) barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Enhanced Model)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Save player names and clean 2023 data nms &lt;- wr_2023$player_name wr_2023 &lt;- wr_2023[, !names(wr_2023) %in% c(zero_var_cols, columns_to_remove)] # Generate predictions predicted_2024_fantasy_points &lt;- predict(rf_model, newdata = wr_2023) # Create and sort results results_wr &lt;- data.frame( name = nms, predicted_2024_fantasy_points = predicted_2024_fantasy_points ) results_wr &lt;- results_wr[order(results_wr$predicted_2024_fantasy_points, decreasing = TRUE), ] # Print predictions print(&quot;2024 WR Predictions (Enhanced Model):&quot;) ## [1] &quot;2024 WR Predictions (Enhanced Model):&quot; print(results_wr) ## name predicted_2024_fantasy_points ## 709 Amon-Ra St. Brown 181.59983 ## 1243 Tyreek Hill 181.14827 ## 571 CeeDee Lamb 179.95472 ## 1104 Puka Nacua 172.72173 ## 1667 A.J. Brown 153.04361 ## 836 Nico Collins 148.01333 ## 1149 Keenan Allen 147.45274 ## 1869 Brandon Aiyuk 139.97278 ## 1298 Justin Jefferson 139.15777 ## 381 D.J. Moore 137.90136 ## 1437 Chris Olave 136.98476 ## 449 Ja&#39;Marr Chase 136.44189 ## 835 Tank Dell 135.39274 ## 1173 Davante Adams 130.17761 ## 1950 Mike Evans 128.77732 ## 1874 Deebo Samuel 125.98903 ## 569 Brandin Cooks 124.51287 ## 1244 Jaylen Waddle 124.43821 ## 503 Amari Cooper 120.31856 ## 1732 George Pickens 119.33197 ## 244 Stefon Diggs 118.97043 ## 1664 DeVonta Smith 115.60759 ## 2023 DeAndre Hopkins 111.29464 ## 984 Calvin Ridley 108.50681 ## 1602 Garrett Wilson 107.87159 ## 1949 Chris Godwin 106.12752 ## 780 Jayden Reed 102.80696 ## 1734 Diontae Johnson 100.35731 ## 2101 Terry McLaurin 100.24320 ## 1370 JuJu Smith-Schuster 98.87110 ## 49 Marquise Brown 98.12850 ## 1296 Jordan Addison 97.20340 ## 1109 Cooper Kupp 95.61552 ## 170 Nelson Agholor 93.99087 ## 986 Christian Kirk 93.93921 ## 169 Zay Flowers 93.74678 ## 640 Courtland Sutton 91.88144 ## 1441 Rashid Shaheed 91.87780 ## 505 Elijah Moore 90.28396 ## 1801 Tyler Lockett 90.13423 ## 456 Tee Higgins 88.55195 ## 786 Christian Watson 87.54887 ## 711 Marvin Jones 87.20233 ## 119 Drake London 87.16003 ## 784 Romeo Doubs 85.63936 ## 1055 Rashee Rice 80.82038 ## 639 Jerry Jeudy 80.71338 ## 834 Robert Woods 77.99893 ## 1150 Josh Palmer 77.78315 ## 907 Josh Downs 76.72685 ## 1371 Demario Douglas 76.49230 ## 1105 Tutu Atwell 76.42632 ## 1247 Chase Claypool 74.70828 ## 2096 Curtis Samuel 74.56587 ## 300 Jonathan Mingo 73.62137 ## 1153 Quentin Johnston 72.85825 ## 785 Bo Melton 71.98097 ## 453 Tyler Boyd 71.59650 ## 1525 Darius Slayton 70.72639 ## 1800 Jaxon Smith-Njigba 70.46430 ## 1524 Wan&#39;Dale Robinson 70.30337 ## 1733 Allen Robinson 70.19659 ## 47 Michael Wilson 70.16334 ## 2100 Jahan Dotson 69.63122 ## 1107 Demarcus Robinson 67.88962 ## 306 Adam Thielen 67.45377 ## 120 Mack Hollins 67.15398 ## 455 Trenton Irwin 67.14472 ## 782 Dontayvion Wicks 65.74947 ## 1175 Jakobi Meyers 64.27733 ## 1603 Randall Cobb 63.51867 ## 905 Alec Pierce 63.02156 ## 1440 Michael Thomas 62.31477 ## 168 Rashod Bateman 61.13400 ## 1665 Julio Jones 60.97879 ## 242 Khalil Shakir 59.78990 ## 1365 DeVante Parker 59.72767 ## 1522 Jalin Hyatt 59.44663 ## 1151 Mike Williams 59.19338 ## 1605 Allen Lazard 57.01559 ## 51 Rondale Moore 56.41350 ## 506 Cedric Tillman 56.39963 ## 507 Marquise Goodwin 55.94416 ## 643 Marvin Mims 54.71009 ## 1052 Richie James 53.44949 ## 1871 Willie Snead 52.39397 ## 1369 Kendrick Bourne 52.14344 ## 1154 Alex Erickson 52.05016 ## 832 Noah Brown 51.82046 ## 378 Darnell Mooney 50.98198 ## 708 Jameson Williams 50.74657 ## 2024 Chris Moore 50.37908 ## 1172 Tre Tucker 49.44492 ## 1299 K.J. Osborn 49.25193 ## 2028 Treylon Burks 48.68337 ## 173 Laquon Treadwell 48.34939 ## 988 Zay Jones 47.78962 ## 1057 Marquez Valdes-Scantling 47.32033 ## 1951 Trey Palmer 47.11448 ## 568 Jalen Tolbert 45.54541 ## 1054 Kadarius Toney 45.41977 ## 1174 Hunter Renfrow 45.05365 ## 1176 DeAndre Carter 44.74819 ## 1301 Brandon Powell 44.67244 ## 1152 Derius Davis 44.59105 ## 1438 A.T. Perry 43.70399 ## 50 Greg Dortch 43.07924 ## 1526 Isaiah Hodgins 42.80481 ## 1248 Erik Ezukanma 42.78455 ## 1873 Jauan Jennings 42.49477 ## 1870 Chris Conley 42.23952 ## 1056 Skyy Moore 42.07177 ## 1368 Jalen Reagor 41.99928 ## 1872 Ronnie Bell 41.87035 ## 379 Tyler Scott 41.03749 ## 1604 Xavier Gipson 40.97803 ## 2026 Nick Westbrook-Ikhine 40.78182 ## 2097 Jamison Crowder 40.30683 ## 638 Brandon Johnson 39.49787 ## 1523 Parris Campbell 38.41738 ## 1663 Quez Watkins 38.36607 ## 710 Josh Reynolds 38.03500 ## 572 Jalen Brooks 37.98076 ## 1730 Calvin Austin 37.35225 ## 48 Zach Pascal 37.24599 ## 171 Devin Duvernay 37.23263 ## 454 Andrei Iosivas 36.84082 ## 1108 Tyler Johnson 36.01099 ## 570 Michael Gallup 35.69286 ## 502 David Bell 34.92553 ## 383 Trent Taylor 34.26550 ## 1607 Jason Brownlee 34.10946 ## 837 Xavier Hutchinson 32.98186 ## 1527 Sterling Shepard 32.97947 ## 1155 Keelan Doss 32.92457 ## 504 James Proche 32.48198 ## 906 Isaiah McKenzie 32.41660 ## 985 Parker Washington 31.84616 ## 1246 Braxton Berrios 31.57056 ## 1053 Justin Watson 31.25025 ## 989 Jamal Agnew 31.09253 ## 240 Trent Sherfield 30.84335 ## 1364 Tyquan Thornton 30.41349 ## 1051 Mecole Hardman 29.89886 ## 305 Ihmir Smith-Marsette 29.49421 ## 1802 Jake Bobo 29.41785 ## 1948 Deven Thompkins 29.08725 ## 117 Van Jefferson 28.10888 ## 783 Malik Heath 28.05905 ## 641 Phillip Dorsett 27.90272 ## 1147 Simi Fehoko 27.61063 ## 2099 Dyami Brown 27.26799 ## 1666 Olamide Zaccheaus 26.88340 ## 2025 Kyle Philips 26.43851 ## 707 Kalif Raymond 25.08213 ## 1300 Trishton Jackson 24.98231 ## 382 Collin Johnson 24.96856 ## 1367 Ty Montgomery 24.81606 ## 1297 Jalen Nailor 24.56069 ## 1731 Miles Boykin 24.49687 ## 712 Antoine Green 24.22513 ## 983 Tim Jones 24.10134 ## 1245 Robbie Chosen 23.58473 ## 1366 Kayshon Boutte 23.07949 ## 1436 Keith Kirkwood 23.00427 ## 380 Equanimeous St. Brown 22.95371 ## 706 Donovan Peoples-Jones 22.72767 ## 1875 Ray-Ray McCloud 22.45901 ## 909 Amari Rodgers 22.31813 ## 172 Tylan Wallace 22.23181 ## 116 KhaDarel Hodge 22.00749 ## 1148 Jalen Guyton 21.98612 ## 642 Lil&#39;Jordan Humphrey 21.72226 ## 1799 Cody Thompson 21.67658 ## 1103 Austin Trammell 21.28101 ## 1803 D&#39;Wayne Eskridge 20.43136 ## 451 Charlie Jones 19.92151 ## 118 Scott Miller 19.70586 ## 904 D.J. Montgomery 18.89337 ## 1606 Irvin Charles 18.56292 ## 1952 Rakim Jarrett 17.96633 ## 1528 Gunner Olszewski 17.76953 ## 1106 Ben Skowronek 17.54110 ## 903 Juwann Winfree 17.24597 ## 450 Shedrick Jackson 16.91313 ## 1242 River Cracraft 16.65992 ## 2098 Byron Pringle 16.63615 ## 1953 David Moore 16.34555 ## 1668 Britain Covey 15.82698 ## 781 Samori Toure 15.61907 ## 1608 Malik Taylor 14.52379 ## 2022 Colton Dowell 13.90308 ## 241 Deonte Harty 13.66192 ## 1058 Justyn Ross 13.10295 ## 303 Mike Strachan 12.17968 ## 2027 Mason Kinsey 12.09344 ## 833 Steven Sims 11.79314 ## 987 Elijah Cooks 10.88795 ## 243 Gabe Davis NA ## 301 Laviska Shenault NA ## 302 D.J. Chark NA ## 304 Terrace Marshall NA ## 377 Velus Jones NA ## 452 Kwamie Lassiter NA ## 908 Michael Pittman NA ## 1439 Lynn Bowden NA ## 1804 DK Metcalf NA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
