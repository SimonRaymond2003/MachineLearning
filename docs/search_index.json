[["index.html", "Machine Learning Applications Chapter 1 About Myself 1.1 Purpose 1.2 Notes for public 1.3 Work in progress", " Machine Learning Applications Simon P. Raymond 2024-12-29 Chapter 1 About Myself My name is Simon Raymond. I am finishing my Fourth year at Saint Mary’s University in Halifax NS. I intend to keep learning Machine Learning techniques outside of a classroom. This is where i will be uploading some of my generic or basic Algorithms and Practices. Contact: simon.raymond@smu.ca 1.1 Purpose I want to display a few things Show some basic skeletons of how to build machine learning algorithms Make some more practical applications Explain key concepts Keep track of my learning goals 1.2 Notes for public The main purpose of this bookdown is not for the public. However all codes are made availible. Due to some issues while knitting this bookdown much of the code was converted to base R code. N/A 1.3 Work in progress Parts/sections Equations Skeletons, Binary Application. Regression Application, Imbalanced data, Speed Testing, Multi Variable Classification Our own models package free, Parrallel proccesing approaches For myself "],["algorithms.html", "Chapter 2 Algorithms 2.1 Data 2.2 Parametric 2.3 Trees 2.4 Boosting 2.5 Other Non-parametric", " Chapter 2 Algorithms These all will be done in classification but can easily be changed and you can look at the regressional examples 2.1 Data suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(ROCR)) suppressPackageStartupMessages(library(rpart)) suppressPackageStartupMessages(library(randomForest)) suppressPackageStartupMessages(library(xgboost)) suppressPackageStartupMessages(library(doParallel)) suppressPackageStartupMessages(library(gbm)) ## Warning: package &#39;gbm&#39; was built under R version 4.3.3 suppressPackageStartupMessages(library(ada)) suppressPackageStartupMessages(library(caret)) suppressPackageStartupMessages(library(nnet)) suppressPackageStartupMessages(library(lightgbm)) library(dslabs) data(&quot;mnist_27&quot;) data &lt;- rbind(mnist_27$train, mnist_27$test) dataf &lt;- as.data.frame(lapply(data, function(x) if(is.numeric(x)) scale(x) else x)) data &lt;- dataf data$y &lt;- ifelse(data$y == &quot;7&quot;, 1, 0) glimpse(data) ## Rows: 1,000 ## Columns: 3 ## $ y &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, … ## $ x_1 &lt;dbl&gt; -1.6192183, -0.2321362, -1.8274064, -0.5171458, 2.3938524, -1.5154… ## $ x_2 &lt;dbl&gt; -1.19265064, -2.30968844, -0.10549778, -0.74534398, 0.94485419, -0… glimpse(dataf) ## Rows: 1,000 ## Columns: 3 ## $ y &lt;fct&gt; 2, 7, 2, 2, 7, 2, 7, 7, 7, 2, 2, 7, 2, 2, 7, 2, 2, 2, 2, 2, 7, 2, … ## $ x_1 &lt;dbl&gt; -1.6192183, -0.2321362, -1.8274064, -0.5171458, 2.3938524, -1.5154… ## $ x_2 &lt;dbl&gt; -1.19265064, -2.30968844, -0.10549778, -0.74534398, 0.94485419, -0… 2.2 Parametric 2.2.1 LM auc_lm &lt;- c() n &lt;- 100 for (i in 1:n){ idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] mdl &lt;- lm(y ~ ., data = trn) phat &lt;- predict(mdl, tst) pred &lt;- prediction(phat, tst$y) auc_lm[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_lm # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3 Trees 2.3.1 CART auc_cart &lt;- c() n &lt;- 100 for (i in 1:n) { idx &lt;- unique(sample(nrow(dataf), size = nrow(dataf), replace = TRUE)) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] # Fit a CART model mdl &lt;- rpart(y ~ ., data = trn, method = &quot;class&quot;) # Predict probabilities. Adjust if your &#39;y&#39; variable is factor with levels other than 0 and 1 phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, tst$y) auc_cart[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_cart # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with CART&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3.2 Bagging auc_bag &lt;- c() n &lt;- 100 B &lt;- 100 num_vars &lt;- ncol(dataf) - 1 for (i in 1:n) { idx &lt;- sample(nrow(dataf), nrow(dataf), replace = TRUE) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] mdl &lt;- randomForest(y ~ ., data = trn, ntree = B, mtry = num_vars) phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, as.numeric(as.character(tst$y))) auc_bag[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_bag # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with Bagging&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.3.3 RF auc_rf &lt;- c() n &lt;- 100 B &lt;- 100 for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(dataf), size = nrow(dataf), replace = TRUE)) trn &lt;- dataf[idx, ] tst &lt;- dataf[-idx, ] # Fit a Random Forest model mdl &lt;- randomForest(y ~ ., data = trn, ntree = B) # Predict probabilities for the positive class phat &lt;- predict(mdl, tst, type = &quot;prob&quot;)[,2] # Calculate AUC pred &lt;- prediction(phat, as.numeric(as.character(tst$y))) auc_rf[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc &lt;- auc_rf # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with RF&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4 Boosting 2.4.1 Adaboost This works now just make it like the others We also can just take gbm and switch it to adaboost ada_model &lt;- ada(y ~ ., data = dataf, iter = 100, nu = 0.1, control = rpart.control(maxdepth = 3)) summary(ada_model) ## Call: ## ada(y ~ ., data = dataf, iter = 100, nu = 0.1, control = rpart.control(maxdepth = 3)) ## ## Loss: exponential Method: discrete Iteration: 100 ## ## Training Results ## ## Accuracy: 0.857 Kappa: 0.713 2.4.2 GBM boost grid &lt;- expand.grid( n.trees = seq(100, 200, by = 100), # Number of trees interaction.depth = seq(1, 2, by = 1), # Max depth of trees shrinkage = seq(0.1, 0.2, by = 0.1) # Learning rate ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] n &lt;- 50 v &lt;- 3 results &lt;- matrix(nrow = n, ncol = 4) for (i in 1:n) { # Bootstrap sampling for training and test sets idx &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) md &lt;- data[idx, ] test_data &lt;- data[-idx, ] auc_vg &lt;- c() for (j in 1:nrow(rgrid)) { auc_v &lt;- c() for (k in 1:v) { v_idx &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE)) train_data &lt;- md[v_idx, ] val_data &lt;- md[-v_idx, ] # Fit GBM model mdl &lt;- gbm(y ~ ., data = train_data, distribution = &quot;bernoulli&quot;, n.trees = rgrid[j, &quot;n.trees&quot;], interaction.depth = rgrid[j, &quot;interaction.depth&quot;], shrinkage = rgrid[j, &quot;shrinkage&quot;], verbose = FALSE) # Predict on validation set and calculate AUC p &lt;- predict(mdl, newdata = val_data, n.trees = rgrid[j, &quot;n.trees&quot;], type = &quot;response&quot;) pred &lt;- prediction(p, val_data$y) auc_v[k] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } auc_vg[j] &lt;- mean(auc_v) } # Identify the best model best_idx &lt;- which.max(auc_vg) best_prm &lt;- rgrid[best_idx, ] # Train final model on the full training data and predict on test set mdl_final &lt;- gbm(y ~ ., data = md, distribution = &quot;bernoulli&quot;, n.trees = best_prm[1, &quot;n.trees&quot;], interaction.depth = best_prm[&quot;interaction.depth&quot;], shrinkage = best_prm[&quot;shrinkage&quot;], verbose = FALSE) p_t &lt;- predict(mdl_final, newdata = test_data, n.trees = best_prm[1, &quot;n.trees&quot;], type = &quot;response&quot;) pred_t &lt;- prediction(p_t, test_data$y) auc_test &lt;- performance(pred_t, &quot;auc&quot;)@y.values[[1]] results[i, 1] &lt;- auc_test results[i, 2] &lt;- best_prm[1, &quot;n.trees&quot;] results[i, 3] &lt;- best_prm[1, &quot;interaction.depth&quot;] results[i, 4] &lt;- best_prm[1, &quot;shrinkage&quot;] } df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, &quot;n.trees&quot;, &quot;interaction.depth&quot;, &quot;shrinkage&quot;) # Plotting plot(df_results$AUC_Test, col = &quot;red&quot;, main = &quot;AUC Test Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(df_results$AUC_Test), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(df_results$AUC_Test) - 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(df_results$AUC_Test) + 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.3 XGBoost grid &lt;- expand.grid( eta = seq(0.1, 0.2, by = 0.05), max_depth = seq(1, 2, by = 1), min_child_weight = seq(1, 1, by = 0), subsample = seq(1, 1, by = 0), colsample_bytree = seq(1, 1, by = 0), lambda = seq(0, 1, by = 1), alpha = seq(0, 1, by = 1), gamma = seq(0, 1, by = 1), nrounds = seq(100, 200, by = 100) ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] xs &lt;- model.matrix(~ . - 1 - y, data = data) y &lt;- data$y nc &lt;- 1 #detectCores - 1 n &lt;- 50 v &lt;- 3 # should be much higher # Adjust the matrix size according to the number of hyperparameters + 1 for AUC results &lt;- matrix(nrow = n, ncol = length(rgrid[1,]) + 1) for (i in 1:n) { idx &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) dx &lt;- xs[idx, ] dy &lt;- y[idx] tx &lt;- xs[-idx, ] ty &lt;- y[-idx] auc_vg &lt;- c() for (j in 1:nrow(rgrid)) { auc_v &lt;- c() for (k in 1:v) { v_idx &lt;- sample(nrow(dx), nrow(dx), replace = TRUE) vx &lt;- dx[v_idx, ] vy &lt;- dy[v_idx] val_x &lt;- dx[-v_idx, ] val_y &lt;- dy[-v_idx] prm &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = rgrid[j, &quot;max_depth&quot;], eta = rgrid[j, &quot;eta&quot;], subsample = rgrid[j, &quot;subsample&quot;], colsample_bytree = rgrid[j, &quot;colsample_bytree&quot;], gamma = rgrid[j, &quot;gamma&quot;], min_child_weight = rgrid[j, &quot;min_child_weight&quot;], alpha = rgrid[j, &quot;alpha&quot;], lambda = rgrid[j, &quot;lambda&quot;], nthread = nc ) dm_train &lt;- xgb.DMatrix(data = vx, label = vy) mdl &lt;- xgb.train(params = prm, data = dm_train, nrounds = rgrid[j, &quot;nrounds&quot;], verbose = FALSE) p &lt;- predict(mdl, xgb.DMatrix(data = val_x)) pred &lt;- prediction(p, val_y) auc_v &lt;- c(auc_v, performance(pred, &quot;auc&quot;)@y.values[[1]]) } auc_vg &lt;- c(auc_vg, mean(auc_v)) } best_idx &lt;- which.max(auc_vg) best_prm &lt;- rgrid[best_idx, ] best_prm_list &lt;- as.list(best_prm[-which(names(best_prm) == &quot;nrounds&quot;)]) best_prm_list$booster &lt;- &quot;gbtree&quot; best_prm_list$objective &lt;- &quot;binary:logistic&quot; best_prm_list$nthread &lt;- nc dm_final &lt;- xgb.DMatrix(data = dx, label = dy) dt_final &lt;- xgb.DMatrix(data = tx, label = ty) mdl_final &lt;- xgb.train(params = best_prm_list, data = dm_final, nrounds = best_prm[ ,&quot;nrounds&quot;], verbose = FALSE) p_t &lt;- predict(mdl_final, dt_final) pred_t &lt;- prediction(p_t, ty) auc_test &lt;- performance(pred_t, &quot;auc&quot;)@y.values[[1]] # Store AUC and hyperparameters in the results matrix results[i, 1] &lt;- auc_test results[i, 2:ncol(results)] &lt;- as.numeric(best_prm) } # Convert results to a dataframe for easy handling df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, names(rgrid[1,])) # Example of plotting, adjust as necessary plot(df_results$AUC_Test, col = &quot;red&quot;, main = &quot;AUC Test Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(df_results$AUC_Test), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(df_results$AUC_Test) - 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(df_results$AUC_Test) + 1.96 * sd(df_results$AUC_Test), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.4 Light GBM boost grid &lt;- expand.grid( learning_rate = seq(0.05, 0.15, by = 0.1), num_leaves = seq(2, 2, by = 0), min_data_in_leaf = seq(1, 1, by = 0), feature_fraction = seq(1, 1, by = 0), bagging_fraction = seq(1, 1, by = 0), bagging_freq = seq(0, 0, by = 0), lambda_l1 = seq(0, 1, by = 1), lambda_l2 = seq(0, 1, by = 1), max_depth = seq(1, 2, by = 1), # -1 for no limit nrounds = seq(100, 200, by = 100) ) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] # Create model matrix for xgboost xs &lt;- model.matrix(~ . -1 -y, data = data) y &lt;- data$y n &lt;- 50 v &lt;- 3 results &lt;- matrix(nrow = n, ncol = length(rgrid[1,]) + 1) for (i in 1:n) { # Bootstrap sampling ind &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] test_y &lt;- y[-ind] auc &lt;- numeric(nrow(rgrid)) for (j in 1:nrow(rgrid)) { auc_100 &lt;- numeric(v) for (k in 1:v) { # Nested validation loop ind2 &lt;- unique(sample(nrow(md_x), nrow(md_x), replace = TRUE)) train_x &lt;- md_x[ind2, ] train_y &lt;- md_y[ind2] val_x &lt;- md_x[-ind2, ] val_y &lt;- md_y[-ind2] # Model training lgb_train &lt;- lgb.Dataset(train_x, label = train_y) lgb_val &lt;- lgb.Dataset(val_x, label = val_y, free_raw_data = FALSE) params &lt;- list( objective = &quot;binary&quot;, metric = &quot;auc&quot;, learning_rate = rgrid[j, &quot;learning_rate&quot;], num_leaves = rgrid[j, &quot;num_leaves&quot;], feature_fraction = rgrid[j, &quot;feature_fraction&quot;], bagging_fraction = rgrid[j, &quot;bagging_fraction&quot;], bagging_freq = rgrid[j, &quot;bagging_freq&quot;], lambda_l1 = rgrid[j, &quot;lambda_l1&quot;], lambda_l2 = rgrid[j, &quot;lambda_l2&quot;], min_data_in_leaf = rgrid[j, &quot;min_data_in_leaf&quot;], max_depth = rgrid[j, &quot;max_depth&quot;] ) lgmb_model &lt;- lgb.train(params, lgb_train, valids = list(val = lgb_val), nrounds = rgrid[j, &quot;nrounds&quot;], verbose = 0) # AUC calculation phat &lt;- predict(lgmb_model, val_x) pred_rocr &lt;- prediction(phat, val_y) auc_100[k] &lt;- performance(pred_rocr, &quot;auc&quot;)@y.values[[1]] } auc[j] &lt;- mean(auc_100) } # Get the best hyperparameters BI &lt;- which.max(auc) params_best &lt;- rgrid[BI, ] # Retrain and test params_final &lt;- list( objective = &quot;binary&quot;, metric = &quot;auc&quot;, learning_rate = params_best[&quot;learning_rate&quot;], num_leaves = params_best[&quot;num_leaves&quot;], feature_fraction = params_best[&quot;feature_fraction&quot;], bagging_fraction = params_best[&quot;bagging_fraction&quot;], bagging_freq = params_best[&quot;bagging_freq&quot;], lambda_l1 = params_best[&quot;lambda_l1&quot;], lambda_l2 = params_best[&quot;lambda_l2&quot;], min_data_in_leaf = params_best[&quot;min_data_in_leaf&quot;], max_depth = params_best[&quot;max_depth&quot;] ) lgb_dm &lt;- lgb.Dataset(md_x, label = md_y) lgmb_model_final &lt;- lgb.train(params_final, lgb_dm, nrounds = params_best[1, &quot;nrounds&quot;], verbose = 0) phat_t &lt;- predict(lgmb_model_final, test_x) pred_rocr_t &lt;- prediction(phat_t, test_y) auc_t &lt;- performance(pred_rocr_t, &quot;auc&quot;)@y.values[[1]] # Store AUC and hyperparameters in the results matrix results[i, 1] &lt;- auc_t results[i, 2] &lt;- params_best[1, 1] results[i, 3] &lt;- params_best[1, 2] results[i, 4] &lt;- params_best[1, 3] results[i, 5] &lt;- params_best[1, 4] results[i, 6] &lt;- params_best[1, 5] results[i, 7] &lt;- params_best[1, 6] results[i, 8] &lt;- params_best[1, 7] results[i, 9] &lt;- params_best[1, 8] results[i, 10] &lt;- params_best[1, 9] results[i, 11] &lt;- params_best[1, 10] } # Convert results to a dataframe for easy handling df_results &lt;- as.data.frame(results) colnames(df_results) &lt;- c(&quot;AUC_Test&quot;, names(rgrid[1,])) auc &lt;- df_results$AUC_Test # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with lgbm&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.4.5 Catboost Somthing to look at in the future 2.5 Other Non-parametric 2.5.1 KNN k &lt;- seq(5, 50, 5) tauc &lt;- c() tuned_k &lt;- c() for (t in 1:50) { ind &lt;- sample(nrow(data), nrow(data)*0.8) mdata &lt;- data[ind, ] test &lt;- data[-ind, ] mauc &lt;- c() for(i in 1:length(k)) { auc &lt;- c() for(j in 1:2) { ind2 &lt;- sample(nrow(mdata), nrow(mdata), replace = TRUE) train &lt;- mdata[ind2, ] val &lt;- mdata[-ind2, ] model &lt;- knn3(y ~ ., data = train, k = k[i]) phat &lt;- predict(model, val, type = &quot;prob&quot;)[,2] pred_rocr &lt;- prediction(phat, val$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[j] &lt;- auc_ROCR@y.values[[1]] } mauc[i] &lt;- mean(auc) } tuned_k &lt;- k[which.max(mauc)] model &lt;- knn3(y ~ ., data = mdata, k = tuned_k) phat &lt;- predict(model, test, type = &quot;prob&quot;)[,2] pred_rocr &lt;- prediction(phat, test$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) tauc[t] &lt;- auc_ROCR@y.values[[1]] } auc &lt;- tauc # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with KNN3&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.5.2 Nueralnet size &lt;- c(seq(2, 3, 1)) decay &lt;- c(seq(0.01, 0.25, 0.05)) maxit &lt;- c(seq(100, 900, 400)) grid &lt;- expand.grid(size, decay, maxit) conf_lev &lt;- .95 num_max &lt;- 5 # Define number around the maximum n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] n &lt;- 100 v &lt;- 5 # store out best values via validation scores opt &lt;- matrix(0, nrow = n, ncol = 5) colnames(opt) &lt;- c(&quot;size&quot;, &quot;decay&quot;, &quot;maxit&quot;, &quot;AUC_val&quot;, &quot;AUC_TEST&quot;) for (j in 1:n){ # put aside data for final test. creat md and test ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) md &lt;- data[ind, ] test &lt;- data[-ind, ] auc_runs &lt;- c() for (i in 1:nrow(rgrid)){ #cat(&quot;loops: &quot;, j, i, &quot;\\r&quot;) auc_tuning &lt;- c() for (p in 1:v){ # bootstrap from md to make a train and val set idx &lt;- unique(sample(nrow(md), nrow(md), replace = TRUE)) train &lt;- md[idx,] val &lt;- md[-idx, ] # model on the train data model &lt;- nnet(y ~ ., data = train, trace = FALSE, act.fct = &quot;logistic&quot;, size = rgrid[i, 1], decay = rgrid[i, 2], maxit = rgrid[i, 3] ) # predict on the val data phat &lt;- predict(model, val) # find the auc pred_rocr &lt;- prediction(phat, val$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_tuning[p] &lt;- auc_ROCR@y.values[[1]] } auc_runs[i] &lt;- mean(auc_tuning) #take the mean of v runs for that one specific hyper parameter } # index the best hyper parameters BI &lt;- which.max(auc_runs) best_AUC &lt;- auc_runs[BI] best_params &lt;- rgrid[BI, ] # store the best hyper parames based on the mean aucs opt[j, 1] &lt;- best_params[1, 1] opt[j, 2] &lt;- best_params[1, 2] opt[j, 3] &lt;- best_params[1, 3] opt[j, 4] &lt;- best_AUC # model with the md data model &lt;- nnet(y ~ ., data = md, trace = FALSE, act.fct = &quot;logistic&quot;, size = opt[j, 1], decay = opt[j, 2], maxit = opt[j, 3] ) # predict the set aside test set phat_t &lt;- predict(model, test) # get the test auc pred_rocr &lt;- prediction(phat_t, test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_test &lt;- auc_ROCR@y.values[[1]] # store the test auc opt[j, 5] &lt;- auc_test } auc &lt;- opt[,5] # Plot AUC values, mean, and confidence intervals plot(auc, col = &quot;red&quot;, main = &quot;AUC Distribution with nnet&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2, lty = 2) abline(h = mean(auc) - 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) abline(h = mean(auc) + 1.96 * sd(auc), col = &quot;green&quot;, lwd = 2, lty = 3) 2.5.3 SVM "],["binary-application.html", "Chapter 3 Binary Application 3.1 Data 3.2 Model Selection 3.3 Comparisions 3.4 Statistic selection", " Chapter 3 Binary Application I will compare results in a different data set that predicts if income is over 50k (1) or not (0) suppressMessages(library(readr)) suppressMessages(library(caret)) suppressMessages(library(ROCR)) suppressMessages(library(xgboost)) suppressMessages(library(foreach)) suppressMessages(library(doParallel)) suppressMessages(library(Matrix)) suppressMessages(library(dplyr)) suppressMessages(library(tidyverse)) suppressMessages(library(forcats)) suppressMessages(library(DataExplorer)) suppressMessages(library(randomForest)) First, we import all necessary libraries for data manipulation, visualization, and machine learning. 3.1 Data 3.1.1 Data Prep data &lt;- read_csv(&quot;adult_train.csv&quot;, col_names = FALSE) ## Rows: 32561 Columns: 15 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (9): X2, X4, X6, X7, X8, X9, X10, X14, X15 ## dbl (6): X1, X3, X5, X11, X12, X13 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. colnames(data) &lt;- c(&quot;age&quot;, &quot;workclass&quot;, &quot;fnlwgt&quot;, &quot;education&quot;, &quot;education_num&quot;, &quot;marital_status&quot;, &quot;occupation&quot;, &quot;relationship&quot;, &quot;race&quot;, &quot;sex&quot;, &quot;capital_gain&quot;, &quot;capital_loss&quot;, &quot;hours_per_week&quot;, &quot;native_country&quot;, &quot;income&quot;) glimpse(data) ## Rows: 32,561 ## Columns: 15 ## $ age &lt;dbl&gt; 39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32,… ## $ workclass &lt;chr&gt; &quot;State-gov&quot;, &quot;Self-emp-not-inc&quot;, &quot;Private&quot;, &quot;Private&quot;, … ## $ fnlwgt &lt;dbl&gt; 77516, 83311, 215646, 234721, 338409, 284582, 160187, 2… ## $ education &lt;chr&gt; &quot;Bachelors&quot;, &quot;Bachelors&quot;, &quot;HS-grad&quot;, &quot;11th&quot;, &quot;Bachelors… ## $ education_num &lt;dbl&gt; 13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11,… ## $ marital_status &lt;chr&gt; &quot;Never-married&quot;, &quot;Married-civ-spouse&quot;, &quot;Divorced&quot;, &quot;Mar… ## $ occupation &lt;chr&gt; &quot;Adm-clerical&quot;, &quot;Exec-managerial&quot;, &quot;Handlers-cleaners&quot;,… ## $ relationship &lt;chr&gt; &quot;Not-in-family&quot;, &quot;Husband&quot;, &quot;Not-in-family&quot;, &quot;Husband&quot;,… ## $ race &lt;chr&gt; &quot;White&quot;, &quot;White&quot;, &quot;White&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;White&quot;, &quot;… ## $ sex &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Fe… ## $ capital_gain &lt;dbl&gt; 2174, 0, 0, 0, 0, 0, 0, 0, 14084, 5178, 0, 0, 0, 0, 0, … ## $ capital_loss &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hours_per_week &lt;dbl&gt; 40, 13, 40, 40, 40, 40, 16, 45, 50, 40, 80, 40, 30, 50,… ## $ native_country &lt;chr&gt; &quot;United-States&quot;, &quot;United-States&quot;, &quot;United-States&quot;, &quot;Uni… ## $ income &lt;chr&gt; &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;&lt;=50K&quot;, &quot;… table(data$income) ## ## &lt;=50K &gt;50K ## 24720 7841 Identify columns containing “?” and count their occurrence. Then, transform character columns to factors and adjust the ‘income’ column for binary classification. question_mark_counts_dplyr &lt;- data %&gt;% summarise(across(everything(), ~sum(. == &quot;?&quot;, na.rm = TRUE))) %&gt;% select(where(~. &gt; 0)) question_mark_counts_dplyr ## # A tibble: 1 × 3 ## workclass occupation native_country ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1836 1843 583 data_transformed &lt;- data %&gt;% mutate(across(where(is.character), as.factor)) %&gt;% mutate(income_binary = if_else(income == &quot;&gt;50K&quot;, 1, 0)) %&gt;% select(-income) %&gt;% select(income_binary, everything()) df &lt;- rename(data_transformed, y = income_binary) df$native_country &lt;- fct_lump(df$native_country, n = 3, other_level = &quot;other&quot;) sapply(df, function(col) { if(is.factor(col)) { sorted_tab &lt;- sort(table(col)) return(sorted_tab) } }) ## $y ## NULL ## ## $age ## NULL ## ## $workclass ## col ## Never-worked Without-pay Federal-gov Self-emp-inc ## 7 14 960 1116 ## State-gov ? Local-gov Self-emp-not-inc ## 1298 1836 2093 2541 ## Private ## 22696 ## ## $fnlwgt ## NULL ## ## $education ## col ## Preschool 1st-4th 5th-6th Doctorate 12th 9th ## 51 168 333 413 433 514 ## Prof-school 7th-8th 10th Assoc-acdm 11th Assoc-voc ## 576 646 933 1067 1175 1382 ## Masters Bachelors Some-college HS-grad ## 1723 5355 7291 10501 ## ## $education_num ## NULL ## ## $marital_status ## col ## Married-AF-spouse Married-spouse-absent Widowed ## 23 418 993 ## Separated Divorced Never-married ## 1025 4443 10683 ## Married-civ-spouse ## 14976 ## ## $occupation ## col ## Armed-Forces Priv-house-serv Protective-serv Tech-support ## 9 149 649 928 ## Farming-fishing Handlers-cleaners Transport-moving ? ## 994 1370 1597 1843 ## Machine-op-inspct Other-service Sales Adm-clerical ## 2002 3295 3650 3770 ## Exec-managerial Craft-repair Prof-specialty ## 4066 4099 4140 ## ## $relationship ## col ## Other-relative Wife Unmarried Own-child Not-in-family ## 981 1568 3446 5068 8305 ## Husband ## 13193 ## ## $race ## col ## Other Amer-Indian-Eskimo Asian-Pac-Islander Black ## 271 311 1039 3124 ## White ## 27816 ## ## $sex ## col ## Female Male ## 10771 21790 ## ## $capital_gain ## NULL ## ## $capital_loss ## NULL ## ## $hours_per_week ## NULL ## ## $native_country ## col ## ? Mexico other United-States ## 583 643 2165 29170 Combine some levels df$education &lt;- as.factor(ifelse(df$education %in% c(&quot;Preschool&quot;, &quot;1st-4th&quot;), &quot;Early Childhood&quot;, as.character(df$education))) df$workclass &lt;- as.factor(ifelse(df$workclass %in% c(&quot;Never-worked&quot;, &quot;Without-pay&quot;), &quot;Unemployed&quot;, as.character(df$workclass))) df$marital_status &lt;- as.factor(ifelse(df$marital_status == &quot;Married-AF-spouse&quot;, &quot;Married-spouse-absent&quot;, as.character(df$marital_status))) df$occupation &lt;- as.factor(ifelse(df$occupation == &quot;Armed-Forces&quot;, &quot;Protective-serv&quot;, as.character(df$occupation))) glimpse(df) ## Rows: 32,561 ## Columns: 15 ## $ y &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0… ## $ age &lt;dbl&gt; 39, 50, 38, 53, 28, 37, 49, 52, 31, 42, 37, 30, 23, 32,… ## $ workclass &lt;fct&gt; State-gov, Self-emp-not-inc, Private, Private, Private,… ## $ fnlwgt &lt;dbl&gt; 77516, 83311, 215646, 234721, 338409, 284582, 160187, 2… ## $ education &lt;fct&gt; Bachelors, Bachelors, HS-grad, 11th, Bachelors, Masters… ## $ education_num &lt;dbl&gt; 13, 13, 9, 7, 13, 14, 5, 9, 14, 13, 10, 13, 13, 12, 11,… ## $ marital_status &lt;fct&gt; Never-married, Married-civ-spouse, Divorced, Married-ci… ## $ occupation &lt;fct&gt; Adm-clerical, Exec-managerial, Handlers-cleaners, Handl… ## $ relationship &lt;fct&gt; Not-in-family, Husband, Not-in-family, Husband, Wife, W… ## $ race &lt;fct&gt; White, White, White, Black, Black, White, Black, White,… ## $ sex &lt;fct&gt; Male, Male, Male, Male, Female, Female, Female, Male, F… ## $ capital_gain &lt;dbl&gt; 2174, 0, 0, 0, 0, 0, 0, 0, 14084, 5178, 0, 0, 0, 0, 0, … ## $ capital_loss &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ hours_per_week &lt;dbl&gt; 40, 13, 40, 40, 40, 40, 16, 45, 50, 40, 80, 40, 30, 50,… ## $ native_country &lt;fct&gt; United-States, United-States, United-States, United-Sta… df_numeric &lt;- df %&gt;% select(where(is.numeric)) df_factor &lt;- df %&gt;% select(where(is.factor)) plot_correlation(df_numeric) plot_correlation(cbind(df_factor, df$y)) numeric_columns &lt;- sapply(df, is.numeric) &amp; names(df) != &quot;y&quot; dfs &lt;- df dfs[numeric_columns] &lt;- scale(df[numeric_columns]) head(dfs) ## # A tibble: 6 × 15 ## y age workclass fnlwgt education education_num marital_status ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 0.0307 State-gov -1.06 Bachelors 1.13 Never-married ## 2 0 0.837 Self-emp-not-inc -1.01 Bachelors 1.13 Married-civ-spo… ## 3 0 -0.0426 Private 0.245 HS-grad -0.420 Divorced ## 4 0 1.06 Private 0.426 11th -1.20 Married-civ-spo… ## 5 0 -0.776 Private 1.41 Bachelors 1.13 Married-civ-spo… ## 6 0 -0.116 Private 0.898 Masters 1.52 Married-civ-spo… ## # ℹ 8 more variables: occupation &lt;fct&gt;, relationship &lt;fct&gt;, race &lt;fct&gt;, ## # sex &lt;fct&gt;, capital_gain &lt;dbl&gt;, capital_loss &lt;dbl&gt;, hours_per_week &lt;dbl&gt;, ## # native_country &lt;fct&gt; table(dfs$y) ## ## 0 1 ## 24720 7841 X &lt;- model.matrix(~ . -1 - y, data = dfs) y &lt;- dfs$y xs &lt;- Matrix(X, sparse = TRUE) 3.2 Model Selection 3.2.1 Functions run_xgb &lt;- function(xs, y, grd, v) { # Setup parallel computing nc &lt;- detectCores() - 1 cl &lt;- makeCluster(nc) registerDoParallel(cl) results &lt;- foreach(g = 1:nrow(grd), .combine=&#39;rbind&#39;, .packages=c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { auc &lt;- numeric(v) # AUC scores for validations for (k in 1:v) { # Bootstrap sampling for validation idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) tr_x &lt;- xs[idx, ] tr_y &lt;- y[idx] vl_x &lt;- xs[-idx, ] vl_y &lt;- y[-idx] # Model training for validation prms &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = grd[g, &quot;max_depth&quot;], eta = grd[g, &quot;eta&quot;], subsample = grd[g, &quot;subsample&quot;], colsample_bytree = grd[g, &quot;colsample_bytree&quot;], gamma = grd[g, &quot;gamma&quot;], min_child_weight = grd[g, &quot;min_child_weight&quot;], alpha = grd[g, &quot;alpha&quot;], lambda = grd[g, &quot;lambda&quot;] ) xgb_tr &lt;- xgb.DMatrix(data = tr_x, label = tr_y) xm &lt;- xgb.train(params = prms, data = xgb_tr, nrounds = grd[g, &quot;nrounds&quot;], verbose = FALSE, nthread = 1) # AUC calculation for validation phat &lt;- predict(xm, xgb.DMatrix(data = vl_x)) pred &lt;- prediction(phat, vl_y) auc[k] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } # AUC mean and params auc_mean &lt;- mean(auc) c(grd[g, ], auc_mean) } # Stop the cluster stopCluster(cl) # Convert results to a tibble and set column names res &lt;- as_tibble(results) names(res) &lt;- c(names(grd), &quot;AUC_Mean&quot;) res &lt;- res %&gt;% mutate(across(everything(), ~unlist(.))) return(res) } test_top_hp &lt;- function(hp, xs, y, t) { # Setup parallel computing nc &lt;- detectCores() - 1 cl &lt;- makeCluster(nc) registerDoParallel(cl) # Testing loop ts_res &lt;- foreach(h = 1:nrow(hp), .combine = &#39;rbind&#39;, .packages = c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { aucs &lt;- numeric(t) for (i in 1:t) { # Bootstrap sampling for testing idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) mdx &lt;- xs[idx, ] mdy &lt;- y[idx] tx &lt;- xs[-idx, ] ty &lt;- y[-idx] # Parameters for the model prms &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = hp[h, &quot;max_depth&quot;], eta = hp[h, &quot;eta&quot;], subsample = hp[h, &quot;subsample&quot;], colsample_bytree = hp[h, &quot;colsample_bytree&quot;], gamma = hp[h, &quot;gamma&quot;], min_child_weight = hp[h, &quot;min_child_weight&quot;], alpha = hp[h, &quot;alpha&quot;], lambda = hp[h, &quot;lambda&quot;], nrounds = hp[h, &quot;nrounds&quot;] ) # Train and test the model dtr &lt;- xgb.DMatrix(data = mdx, label = mdy) dte &lt;- xgb.DMatrix(data = tx, label = ty) mdl &lt;- xgb.train(params = prms, data = dtr, nrounds = prms$nrounds, verbose = 0, nthread = 1) # AUC calculation ph &lt;- predict(mdl, dte) pr &lt;- prediction(ph, ty) aucs[i] &lt;- performance(pr, &quot;auc&quot;)@y.values[[1]] } # Average AUC and combine with hyperparameters c(hp[h, ], mean_auc = mean(aucs)) } # Stop the cluster stopCluster(cl) # Convert to tibble and unlist columns ts_tbl &lt;- as_tibble(ts_res) %&gt;% mutate(across(everything(), ~unlist(.))) return(ts_tbl) } test_auc &lt;- function(xs, y, best_hp, runs) { aucs &lt;- numeric(runs) for (i in 1:runs) { # Bootstrap sampling for testing idx &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) md_x &lt;- xs[idx, ] md_y &lt;- y[idx] test_x &lt;- xs[-idx, ] test_y &lt;- y[-idx] # Set parameters for the model params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, max_depth = best_hp$max_depth, eta = best_hp$eta, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, gamma = best_hp$gamma, min_child_weight = best_hp$min_child_weight, alpha = best_hp$alpha, lambda = best_hp$lambda ) # Train and test the model md &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) model &lt;- xgb.train(params = params, data = md, nrounds = best_hp$nrounds, verbose = 0, nthread = 1) # Calculate AUC phat &lt;- predict(model, dtest) pred &lt;- prediction(phat, test_y) aucs[i] &lt;- performance(pred, &quot;auc&quot;)@y.values[[1]] } return(aucs) } opt_nrd4eta &lt;- function(xs, y, params, o , grd) { # Assuming xgb.train and related functions are already available (via library(xgboost)) library(xgboost) library(ROCR) # Detect the number of cores numCores &lt;- detectCores() - 1 # Initialize an AUC matrix aucm &lt;- matrix(0, nrow = o, ncol = length(grd)) start_time &lt;- Sys.time() for (j in 1:o){ # Creating a bootstrap sample ind &lt;- unique(sample(nrow(xs), nrow(xs), replace = TRUE)) dm &lt;- xgb.DMatrix(data = xs[ind, ], label = y[ind]) dv &lt;- xgb.DMatrix(data = xs[-ind, ], label = y[-ind]) auc &lt;- c() for (i in 1:length(grd)){ # Training the model on the bootstrap sample bsm &lt;- xgb.train(params = params, data = dm, nrounds = grd[i], verbose = FALSE, nthread = numCores ) # Predict on the validation set and calculate AUC phat &lt;- predict(bsm, dv, type = &quot;prob&quot;) # Calculating the AUC pred_rocr &lt;- prediction(phat, y[-ind]) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] } aucm[j, ] &lt;- auc } evalauc &lt;- colMeans(aucm) # Plotting plot(grd, evalauc, type = &quot;b&quot;, col = &quot;blue&quot;, xlab = &quot;Number of Rounds&quot;, ylab = &quot;AUC&quot;, main = &quot;AUC over o rounds vs Number of Rounds in XGBoost&quot;) best_nrounds &lt;- grd[which.max(evalauc)] max_auc &lt;- max(evalauc) end_time &lt;- Sys.time() elapsed_time &lt;- end_time - start_time return(list(best_nrounds = best_nrounds, max_auc = max_auc, elapsed_time = elapsed_time)) } 3.2.2 XGB # Define XGBoost parameters params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.05 ) grd &lt;- seq(1, 500, by = 50) o &lt;- 5 rs &lt;- opt_nrd4eta(xs = xs, y = y, params = params, o, grd) This is a very small grid and cuts out alot of tuning power grid &lt;- expand.grid( eta = seq(0.03, 0.05, by = 0.02), max_depth = seq(4, 6, by = 2), min_child_weight = seq(1, 1, by = 0), subsample = seq(1, 1, by = 0), colsample_bytree = seq(1, 1, by = 0), lambda = seq(1, 3, by = 1), alpha = seq(0, 1, by = 1), gamma = seq(0, 1, by = 1), nrounds = seq(350, 450, by = 50) ) conf_lev &lt;- .95 num_max &lt;- 5 n &lt;- log(1-conf_lev)/log(1-num_max/nrow(grid)) ind &lt;- sample(nrow(grid), nrow(grid)*(n/nrow(grid)), replace = FALSE) rgrid &lt;- grid[ind, ] v &lt;- 5 #This number should be much higher vr &lt;- run_xgb(xs, y, rgrid, v) print(head(vr), width = Inf) ## # A tibble: 6 × 10 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 4 1 1 1 3 0 0 ## 2 0.05 6 1 1 1 1 0 1 ## 3 0.05 6 1 1 1 2 0 0 ## 4 0.03 4 1 1 1 2 1 0 ## 5 0.03 4 1 1 1 3 1 1 ## 6 0.03 4 1 1 1 1 0 1 ## nrounds AUC_Mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 350 0.927 ## 2 350 0.928 ## 3 400 0.929 ## 4 400 0.926 ## 5 450 0.925 ## 6 350 0.924 We will take the top 10% and do a more thorough search # Sort vr by AUC_Mean and select the top 25% top_hp &lt;- vr %&gt;% arrange(desc(AUC_Mean)) %&gt;% slice_head(prop = 0.05) t &lt;- 25 ts_res &lt;- test_top_hp(top_hp, xs, y, t) print(head(ts_res), width = Inf) ## # A tibble: 4 × 11 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 4 1 1 1 3 0 1 ## 2 0.03 6 1 1 1 1 0 1 ## 3 0.05 4 1 1 1 2 0 0 ## 4 0.05 6 1 1 1 2 0 0 ## nrounds AUC_Mean mean_auc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 400 0.929 0.927 ## 2 400 0.929 0.927 ## 3 450 0.929 0.927 ## 4 400 0.929 0.928 Final AUC pres best_hp &lt;- ts_res %&gt;% dplyr::arrange(desc(mean_auc)) %&gt;% dplyr::slice(1) print(best_hp, width = Inf) ## # A tibble: 1 × 11 ## eta max_depth min_child_weight subsample colsample_bytree lambda alpha gamma ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.05 6 1 1 1 2 0 0 ## nrounds AUC_Mean mean_auc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 400 0.929 0.928 r &lt;- 100 auc &lt;- test_auc(xs, y, best_hp, r) mauc &lt;- mean(auc) mauc ## [1] 0.928137 sd(auc) ## [1] 0.001976774 plot(auc, col=&quot;red&quot;) abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) 3.3 Comparisions Random Forest rfd &lt;- dfs rfd$y &lt;- as.factor(rfd$y) B &lt;- 120 n &lt;- 100 obs &lt;- nrow(data) numCores &lt;- detectCores()-1 cl &lt;- makeCluster(numCores) registerDoParallel(cl) lst &lt;- foreach(i=1:n, .packages = c(&quot;randomForest&quot;, &quot;ROCR&quot;)) %dopar% { tryCatch({ idx &lt;- unique(sample(obs, obs, replace = TRUE)) train &lt;- rfd[idx,] test &lt;- rfd[-idx, ] model &lt;- randomForest(y ~ ., ntree = B, data = train) phat &lt;- predict(model, test, type = &quot;prob&quot;) pred_rocr &lt;- prediction(phat[,2], test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc_ROCR@y.values[[1]] }, error = function(e) { NA # Return NA on error }) } stopCluster(cl) # combine the results auc &lt;- unlist(lst) auc &lt;- na.omit(auc) # plot auc and mean plot(auc, col=&quot;red&quot;) abline(a = mean(auc), b = 0, col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) LPM data_lm &lt;- rfd data_lm$y &lt;- as.numeric(data_lm$y) data_lm$y &lt;- data_lm$y - 1 # bring it back to 1 and 0s n &lt;- 100 obs &lt;- nrow(data_lm) numCores &lt;- detectCores() - 1 cl &lt;- makeCluster(numCores) registerDoParallel(cl) auc_list &lt;- foreach(i = 1:n, .packages = &quot;ROCR&quot;) %dopar% { tryCatch({ idx &lt;- unique(sample(obs, obs, replace = TRUE)) trn &lt;- data_lm[idx, ] # Training data tst &lt;- data_lm[-idx, ] # Test data mdl &lt;- lm(y ~ ., data = trn) phat &lt;- predict(mdl, tst) pred &lt;- prediction(phat, tst$y) performance(pred, &quot;auc&quot;)@y.values[[1]] }, error = function(e) { NA # Return NA on error }) } stopCluster(cl) # Process the AUC results auc_values &lt;- unlist(auc_list) auc_values &lt;- na.omit(auc_values) mean_auc &lt;- mean(auc_values) sd_auc &lt;- sd(auc_values) # Plot AUC values, mean, and confidence intervals plot(auc_values, col = &quot;red&quot;, main = &quot;AUC Distribution&quot;, xlab = &quot;Iteration&quot;, ylab = &quot;AUC&quot;) abline(h = mean_auc, col = &quot;blue&quot;, lwd = 2, lty = 2) # Mean line abline(h = mean_auc - 1.96 * sd_auc, col = &quot;green&quot;, lwd = 2, lty = 3) # Lower CI abline(h = mean_auc + 1.96 * sd_auc, col = &quot;green&quot;, lwd = 2, lty = 3) # Upper CI 3.4 Statistic selection Times we run everything here r &lt;- 10 3.4.1 0.5 # Initialize storage for the confusion matrix conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Train the model with predefined parameters xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions using a fixed threshold of 0.5 final_phat &lt;- predict(xgbmdl_final, dtest) final_predicted_classes &lt;- ifelse(final_phat &gt; 0.5, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test_y, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_matrix / r print(avg_conf_matrix) ## Actual ## Predicted 1 0 ## 1 189.2 55.7 ## 0 98.0 851.5 3.4.2 Youden’s J Statistic best_thresholds &lt;- c() j_stats &lt;- c() for(i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Extract parameters for the model from best_hp params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = best_hp$eta, max_depth = best_hp$max_depth, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, min_child_weight = best_hp$min_child_weight, gamma = best_hp$gamma, alpha = best_hp$alpha, lambda = best_hp$lambda ) # Train the model xgbmdl &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions phat &lt;- predict(xgbmdl, dtest) # ROCR predictions object pred &lt;- prediction(phat, test_y) # Calculate performance measures perf &lt;- performance(pred, measure = &quot;sens&quot;, x.measure = &quot;spec&quot;) sensitivity &lt;- slot(perf, &quot;y.values&quot;)[[1]] specificity &lt;- slot(perf, &quot;x.values&quot;)[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] j_stat &lt;- sensitivity + specificity - 1 # Find the best threshold (maximizing J statistic) best_idx &lt;- which.max(j_stat) best_thresholds[i] &lt;- thresholds[best_idx] j_stats[i] &lt;- j_stat[best_idx] } # Calculate and print the average of the best thresholds avg_best_youden_threshold &lt;- mean(best_thresholds) cat(&quot;Average Best Threshold:&quot;, avg_best_youden_threshold, &quot;\\n&quot;) ## Average Best Threshold: 0.2283687 # Plot the distribution of max J statistics for each iteration hist(j_stats, col = &quot;blue&quot;, xlab = &quot;Max J Statistic&quot;, ylab = &quot;Frequency&quot;, main = &quot;Distribution of Max J Statistics Across Bootstrap Iterations&quot;) abline(v = mean(j_stats), col = &quot;red&quot;, lwd = 2) # Mean J Stat line # Initialize storage for aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) # Initialize lists to store TPR and FPR values for each bootstrap iteration all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:r) { ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) final_phat &lt;- predict(xgbmdl_final, dtest) final_predicted_classes &lt;- ifelse(final_phat &gt; avg_best_youden_threshold, 1, 0) # Using F1 threshold conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test_y, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) final_pred_rocr &lt;- prediction(final_phat, test_y) final_perf_rocr &lt;- performance(final_pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) # Store TPR and FPR values for this iteration all_tpr[[i]] &lt;- final_perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- final_perf_rocr@x.values[[1]] } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_mat_totals / r print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 2501.0 1694.4 ## Predicted_0 354.7 7435.1 3.4.3 Fscore library(xgboost) library(ROCR) best_thresholds &lt;- numeric(r) # To store best thresholds for F1 score f1_stats &lt;- numeric(r) # To store the F1 scores at the best thresholds for(i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] # Create DMatrix objects dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) # Train the model with parameters extracted from a hypothetical &#39;best_hp&#39; object xgbmdl &lt;- xgb.train(params = list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = best_hp$eta, max_depth = best_hp$max_depth, subsample = best_hp$subsample, colsample_bytree = best_hp$colsample_bytree, min_child_weight = best_hp$min_child_weight, gamma = best_hp$gamma, alpha = best_hp$alpha, lambda = best_hp$lambda ), data = dtrain, nrounds = best_hp$nrounds, verbose = 0) # Predictions phat &lt;- predict(xgbmdl, dtest) # Calculate F1 score for each threshold pred &lt;- prediction(phat, test_y) perf &lt;- performance(pred, measure = &quot;prec&quot;, x.measure = &quot;rec&quot;) precision &lt;- slot(perf, &quot;y.values&quot;)[[1]] recall &lt;- slot(perf, &quot;x.values&quot;)[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] f1_score &lt;- (2 * precision * recall) / (precision + recall) # Find the best threshold (maximizing F1 score) best_idx &lt;- which.max(f1_score) best_thresholds[i] &lt;- thresholds[best_idx] f1_stats[i] &lt;- f1_score[best_idx] } # Calculate and print the average of the best thresholds avg_best_f1_threshold &lt;- mean(best_thresholds) cat(&quot;Average Best F1 Threshold:&quot;, avg_best_f1_threshold, &quot;\\n&quot;) ## Average Best F1 Threshold: 0.3737744 # Plot the distribution of max F1 scores for each iteration hist(f1_stats, col = &quot;blue&quot;, main = &quot;Distribution of Max F1 Scores Across Bootstrap Iterations&quot;) abline(v = mean(f1_stats), col = &quot;red&quot;, lwd = 2) # Mean F1 score line # Initialize storage for aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2) colnames(conf_mat_totals) &lt;- c(&quot;Actual_1&quot;, &quot;Actual_0&quot;) rownames(conf_mat_totals) &lt;- c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;) # Initialize lists to store TPR and FPR values for each bootstrap iteration all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:r) { ind &lt;- sample(nrow(xs), size = nrow(xs), replace = TRUE) train_xs &lt;- xs[ind, ] test_xs &lt;- xs[-ind, ] train_y &lt;- y[ind] test_y &lt;- y[-ind] dtrain &lt;- xgb.DMatrix(data = train_xs, label = train_y) dtest &lt;- xgb.DMatrix(data = test_xs, label = test_y) xgbmdl_final &lt;- xgb.train(params = params, data = dtrain, nrounds = best_hp$nrounds, verbose = 0) final_phat &lt;- predict(xgbmdl_final, dtest) # Using the F1 threshold we calculated earlier final_predicted_classes &lt;- ifelse(final_phat &gt; avg_best_f1_threshold, 1, 0) # No need to transform test_y as it&#39;s already binary conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test_y, levels = c(1, 0))) conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) final_pred_rocr &lt;- prediction(final_phat, test_y) final_perf_rocr &lt;- performance(final_pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) # Store TPR and FPR values for this iteration all_tpr[[i]] &lt;- final_perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- final_perf_rocr@x.values[[1]] } # Calculate the average confusion matrix avg_conf_matrix &lt;- conf_mat_totals / r print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 2201.0 968.0 ## Predicted_0 688.4 8151.7 3.4.4 ROC curve # First, create common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vector to hold averaged TPR values averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Calculate average best F1 threshold from previous results avg_best_f1_threshold &lt;- mean(best_thresholds) # Find the indices in the ROC curve closest to these thresholds f1_index &lt;- which.min(abs(common_fpr_thresholds - avg_best_f1_threshold)) youden_index &lt;- which.min(abs(common_fpr_thresholds - avg_best_youden_threshold)) fixed_threshold_index &lt;- which.min(abs(common_fpr_thresholds - 0.5)) # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Add points for the thresholds points(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], col = &quot;green&quot;, pch = 19, cex = 1.5) text(common_fpr_thresholds[f1_index], averaged_tpr[f1_index], &quot; Avg Best F1&quot;, pos = 4) points(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], col = &quot;orange&quot;, pch = 19, cex = 1.5) text(common_fpr_thresholds[youden_index], averaged_tpr[youden_index], &quot; Avg Best Youden&quot;, pos = 4) points(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], col = &quot;purple&quot;, pch = 19, cex = 1.5) text(common_fpr_thresholds[fixed_threshold_index], averaged_tpr[fixed_threshold_index], &quot; Fixed 0.5 Threshold&quot;, pos = 4, col = &quot;purple&quot;) # Add legend legend(&quot;bottomright&quot;, legend = c(&quot;Avg Best F1&quot;, &quot;Avg Best Youden&quot;, &quot;Fixed 0.5 Threshold&quot;), col = c(&quot;green&quot;, &quot;orange&quot;, &quot;purple&quot;), pch = 19, bty = &quot;n&quot;) "],["regressional-application.html", "Chapter 4 Regressional Application 4.1 Data 4.2 Model Selection", " Chapter 4 Regressional Application 4.1 Data suppressPackageStartupMessages(library(MASS)) suppressPackageStartupMessages(library(dplyr)) suppressPackageStartupMessages(library(DataExplorer)) suppressPackageStartupMessages(library(randomForest)) data(&quot;Boston&quot;) df &lt;- Boston crim: Per capita crime rate by town. zn: Proportion of residential land zoned for lots over 25,000 sq.ft. indus: Proportion of non-retail business acres per town. chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise). nox: Nitric oxides concentration (parts per 10 million). rm: Average number of rooms per dwelling. age: Proportion of owner-occupied units built prior to 1940. dis: Weighted distances to five Boston employment centres. rad: Index of accessibility to radial highways. tax: Full-value property-tax rate per $10,000. ptratio: Pupil-teacher ratio by town. black: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town. lstat: Lower status of the population (percent). medv: Median value of owner-occupied homes in $1000s. glimpse(df) ## Rows: 506 ## Columns: 14 ## $ crim &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ zn &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ indus &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ chas &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ nox &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ rm &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ age &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ dis &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ rad &lt;int&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ tax &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ black &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ lstat &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ medv &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… 4.1.1 Data Prep data &lt;- df data$chas &lt;- as.factor(data$chas) data$rad &lt;- as.factor(data$rad) names(data)[names(data) == &quot;medv&quot;] &lt;- &quot;y&quot; glimpse(data) ## Rows: 506 ## Columns: 14 ## $ crim &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,… ## $ zn &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1… ## $ indus &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.… ## $ chas &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ nox &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,… ## $ rm &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,… ## $ age &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9… ## $ dis &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505… ## $ rad &lt;fct&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,… ## $ tax &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31… ## $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15… ## $ black &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90… ## $ lstat &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10… ## $ y &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15… data &lt;- data.frame(Map(function(x, name) { if(is.numeric(x) &amp;&amp; name != &quot;y&quot;) scale(x) else x }, data, names(data))) 4.1.2 Data Explorations plot_correlation(df) 4.2 Model Selection 4.2.1 RF B &lt;- 1200 n &lt;- 100 rmspe &lt;- c() for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] # Fit a Random Forest model mdl &lt;- randomForest(y ~ ., data = trn, ntree = B) yhat &lt;- predict(mdl, tst) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((tst$y - yhat)^2)) } mean_rmspe &lt;- mean(rmspe, na.rm = TRUE) mean_rmspe ## [1] 3.432052 4.2.2 LM n &lt;- 100 rmspe &lt;- c() for (i in 1:n) { # Ensure unique indices for training data to avoid empty test set idx &lt;- unique(sample(nrow(data), size = nrow(data), replace = TRUE)) trn &lt;- data[idx, ] tst &lt;- data[-idx, ] mdl &lt;-lm(y ~ ., data = trn) yhat &lt;- predict(mdl, tst) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((tst$y - yhat)^2)) } mean_rmspe &lt;- mean(rmspe, na.rm = TRUE) mean_rmspe ## [1] 4.907648 We will add others later but it is the same basic idea as classification algorithms only we use rmspe instead of auc. "],["imbalanced-application.html", "Chapter 5 Imbalanced Application 5.1 Data 5.2 Approaches 5.3 Statistic selection", " Chapter 5 Imbalanced Application 5.1 Data library(readr) library(dplyr) library(randomForest) library(ROCR) library(smotefamily) data &lt;- read_csv(&quot;HTRU_2.csv&quot;, col_names = FALSE) ## Rows: 17898 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (9): X1, X2, X3, X4, X5, X6, X7, X8, X9 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Renaming X9 to y and moving it to the first position df &lt;- data.frame(y = data$X9, data[, names(data) != &quot;X9&quot;]) # Converting y to a factor with levels 1 and 0 df$y &lt;- factor(df$y, levels = c(&quot;1&quot;, &quot;0&quot;)) glimpse(df) ## Rows: 17,898 ## Columns: 9 ## $ y &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0… ## $ X1 &lt;dbl&gt; 140.56250, 102.50781, 103.01562, 136.75000, 88.72656, 93.57031, 119… ## $ X2 &lt;dbl&gt; 55.68378, 58.88243, 39.34165, 57.17845, 40.67223, 46.69811, 48.7650… ## $ X3 &lt;dbl&gt; -0.23457141, 0.46531815, 0.32332837, -0.06841464, 0.60086608, 0.531… ## $ X4 &lt;dbl&gt; -0.69964840, -0.51508791, 1.05116443, -0.63623837, 1.12349169, 0.41… ## $ X5 &lt;dbl&gt; 3.1998328, 1.6772575, 3.1212375, 3.6429766, 1.1789298, 1.6362876, 0… ## $ X6 &lt;dbl&gt; 19.110426, 14.860146, 21.744669, 20.959280, 11.468720, 14.545074, 9… ## $ X7 &lt;dbl&gt; 7.975532, 10.576487, 7.735822, 6.896499, 14.269573, 10.621748, 19.2… ## $ X8 &lt;dbl&gt; 74.24222, 127.39358, 63.17191, 53.59366, 252.56731, 131.39400, 479.… table(df$y) ## ## 1 0 ## 1639 16259 df_y1 &lt;- subset(df, y == 1) df_yNot1 &lt;- subset(df, y != 1) df_y1_sampled &lt;- df_y1[sample(nrow(df_y1), size = ceiling(0.1 * nrow(df_y1))), ] df &lt;- rbind(df_yNot1, df_y1_sampled) table(df$y) ## ## 1 0 ## 164 16259 5.2 Approaches 5.2.1 No changes library(randomForest) library(ROCR) B &lt;- 100 n &lt;- 100 # Initialize storage for AUC values and ROC curve data auc &lt;- numeric(n) all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { # Bootstrap sampling idx &lt;- sample(nrow(df), size = nrow(df), replace = TRUE) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Training the RandomForest model model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predicting probabilities phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculating AUC pred_rocr &lt;- prediction(predictions = phat[,1], labels = test$y) # Assuming class 1 probabilities are in the first column auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Constructing ROC curves perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.2 Undersample library(randomForest) library(ROCR) library(dplyr) B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) # Store AUC values all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Downsampling the majority class d1 &lt;- subset(train, y == 1) d0 &lt;- subset(train, y == 0) d0s &lt;- d0[sample(nrow(d0), nrow(d1), replace = TRUE), ] train &lt;- rbind(d1, d0s) %&gt;% sample_n(nrow(d1) * 2) # RandomForest model training model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predicting probabilities phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculating AUC pred_rocr &lt;- prediction(phat[,1], test$y) # Ensure correct column for class probabilities auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Calculating TPR and FPR for ROC perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.3 Oversampling B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) # To store AUC values all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx, ] test &lt;- df[-idx, ] # Manual oversampling of the minority class d1 &lt;- subset(train, y == 1) d0 &lt;- subset(train, y == 0) d1s &lt;- d1[sample(nrow(d1), nrow(d0), replace = TRUE), ] train &lt;- rbind(d1s, d0) # Combined oversampled positives with all negatives train &lt;- train[sample(nrow(train)), ] # Shuffle the rows # Model training model &lt;- randomForest(y ~ ., data = train, ntree = B) phat &lt;- predict(model, test, type = &quot;prob&quot;) # AUC calculation pred_rocr &lt;- prediction(phat[,1], test$y) auc_ROCR &lt;- performance(pred_rocr, &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # ROC Curve Calculation perf_rocr &lt;- performance(pred_rocr, &quot;tpr&quot;, &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.2.4 Smoteing Smote has to be in the loop on the train data B &lt;- 100 n &lt;- 100 auc &lt;- numeric(n) all_tpr &lt;- list() all_fpr &lt;- list() for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx,] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model model &lt;- randomForest(y ~ ., data = train, ntree = B) # Predict probabilities on the test set phat &lt;- predict(model, newdata = test, type = &quot;prob&quot;) # Calculate AUC pred_rocr &lt;- prediction(phat[,2], test$y) auc_ROCR &lt;- performance(pred_rocr, measure = &quot;auc&quot;) auc[i] &lt;- auc_ROCR@y.values[[1]] # Calculate TPR and FPR for ROC perf_rocr &lt;- performance(pred_rocr, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;) all_tpr[[i]] &lt;- perf_rocr@y.values[[1]] all_fpr[[i]] &lt;- perf_rocr@x.values[[1]] } # Plot AUC values plot(auc, col=&quot;red&quot;, main=&quot;AUC Values per Bootstrap Sample&quot;) abline(h = mean(auc), col = &quot;blue&quot;, lwd = 2) abline(a = mean(auc)-1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) abline(a = mean(auc)+1.96*sd(auc), b = 0, col = &quot;green&quot;, lwd = 3) legend(&quot;topright&quot;, legend = c(&quot;AUC Values&quot;, &quot;Mean AUC&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = 1, cex = 0.8) # Determine common FPR thresholds for interpolation common_fpr_thresholds &lt;- seq(0, 1, length.out = 100) # Initialize vectors to hold averaged TPR values for these common thresholds averaged_tpr &lt;- numeric(length(common_fpr_thresholds)) # Interpolate TPR values at common FPR thresholds for each bootstrap iteration and average them for (i in seq_along(common_fpr_thresholds)) { tpr_values_at_threshold &lt;- sapply(seq_along(all_tpr), function(j) { approx(all_fpr[[j]], all_tpr[[j]], xout = common_fpr_thresholds[i])$y }) averaged_tpr[i] &lt;- mean(tpr_values_at_threshold, na.rm = TRUE) } # Plot the averaged ROC curve plot(common_fpr_thresholds, averaged_tpr, type = &#39;l&#39;, col = &#39;blue&#39;, xlab = &#39;False Positive Rate&#39;, ylab = &#39;True Positive Rate&#39;, main = &#39;Averaged ROC Curve across Bootstrap Samples&#39;) abline(a = 0, b = 1, lty = 2, col = &#39;red&#39;) # Reference line 5.3 Statistic selection Obviously our data isnt balenced so we cannot use ACC or J stat for our threashold selection n &lt;- 100 # Number of iterations b &lt;- 100 # Number of trees in the RandomForest model # Placeholder for the aggregated confusion matrix totals conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2, dimnames = list(c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;), c(&quot;Actual_1&quot;, &quot;Actual_0&quot;))) bt &lt;- c() for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx, ] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model rf_model &lt;- randomForest(y ~ ., data = train, ntree = b) # Predict probabilities on the test set phat &lt;- predict(rf_model, newdata = test, type = &quot;prob&quot;)[,2] # Calculate ROC curve pred_rocr &lt;- prediction(predictions = phat, labels = test$y) # Calculate performance measures for Youden&#39;s J statistic perf &lt;- performance(pred_rocr, measure = &quot;sens&quot;, x.measure = &quot;spec&quot;) sensitivity &lt;- perf@y.values[[1]] specificity &lt;- perf@x.values[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] j_values &lt;- sensitivity + specificity - 1 # Find the best threshold best_threshold_index &lt;- which.max(j_values) best_threshold &lt;- thresholds[best_threshold_index] bt[i] &lt;- best_threshold # Make predictions based on the best threshold final_predicted_classes &lt;- ifelse(phat &gt; best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test$y, levels = c(1, 0))) # Update the confusion matrix conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average confusion matrix after all iterations avg_conf_matrix &lt;- conf_mat_totals / n print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 53.07 106.13 ## Predicted_0 7.60 5876.23 mean(bt) ## [1] 0.1142 That has issues We will use F1 for our threshold n &lt;- 100 # Number of iterations b &lt;- 100 # Number of trees in the RandomForest model best_f1_scores &lt;- c() bt &lt;- c() conf_mat_totals &lt;- matrix(0, nrow = 2, ncol = 2, dimnames = list(c(&quot;Predicted_1&quot;, &quot;Predicted_0&quot;), c(&quot;Actual_1&quot;, &quot;Actual_0&quot;))) for(i in 1:n) { # Bootstrap sampling with replacement idx &lt;- unique(sample(nrow(df), nrow(df), replace = TRUE)) train &lt;- df[idx,] test &lt;- df[-idx, ] # Apply SMOTE to the training set df_smote &lt;- SMOTE(X = train[, -1], target = train$y, K = 10, dup_size = 5) dfs &lt;- df_smote$data %&gt;% rename(y = class) %&gt;% mutate(y = as.factor(y)) train &lt;- dfs[c(&quot;y&quot;, setdiff(names(dfs), &quot;y&quot;))] # Train the RandomForest model rf_model &lt;- randomForest(y ~ ., data = train, ntree = b) # Predict probabilities on the test set phat &lt;- predict(rf_model, newdata = test, type = &quot;prob&quot;)[,2] # Calculate ROC curve pred_rocr &lt;- prediction(predictions = phat, labels = test$y) perf &lt;- performance(pred_rocr, measure = &quot;prec&quot;, x.measure = &quot;rec&quot;) precision &lt;- perf@y.values[[1]] recall &lt;- perf@x.values[[1]] thresholds &lt;- slot(perf, &quot;alpha.values&quot;)[[1]] # Calculate F1 scores for each threshold f1_scores &lt;- 2 * (precision * recall) / (precision + recall) # Find the best threshold based on F1 score best_f1_index &lt;- which.max(f1_scores) best_f1 &lt;- f1_scores[best_f1_index] best_threshold &lt;- thresholds[best_f1_index] bt[i] &lt;- best_threshold # Store the best F1 score for this iteration best_f1_scores[i] &lt;- best_f1 # Make predictions based on the best threshold final_predicted_classes &lt;- ifelse(phat &gt; best_threshold, 1, 0) conf_matrix &lt;- table(Predicted = factor(final_predicted_classes, levels = c(1, 0)), Actual = factor(test$y, levels = c(1, 0))) # Update the confusion matrix totals with the current confusion matrix conf_mat_totals &lt;- conf_mat_totals + as.matrix(conf_matrix) } # Calculate the average of the best F1 scores after all iterations avg_best_f1_score &lt;- mean(best_f1_scores) print(paste(&quot;Average Best F1 Score:&quot;, avg_best_f1_score)) ## [1] &quot;Average Best F1 Score: 0.769625119395329&quot; # Calculate the average confusion matrix after all iterations avg_conf_matrix &lt;- conf_mat_totals / n print(&quot;Average Confusion Matrix:&quot;) ## [1] &quot;Average Confusion Matrix:&quot; print(avg_conf_matrix) ## Actual_1 Actual_0 ## Predicted_1 44.60 12.57 ## Predicted_0 15.75 5964.69 mean(bt) ## [1] 0.6124 We could do more analysis but this sets a good understanding and basline of what we were trying to accomplish As a note for roc curves and confusion tables these are averages not CIs which we should do aswell. "],["speeds.html", "Chapter 6 Speeds 6.1 Functions to run the tests 6.2 Data 6.3 10,000 rows 6.4 500,000 rows 6.5 Presentations 6.6 Analysis", " Chapter 6 Speeds library(xgboost) library(ROCR) library(foreach) library(doParallel) library(Matrix) library(readr) library(dplyr) library(ggplot2) library(tidyr) 6.1 Functions to run the tests I will make 3 functions. One with my parallel processing, one with xgboost pp and then one with no pp at all so n thread = 1. I have included the code so that you can see it but it is not important just know that we are looping r times and that the loops use bootstrapping for splits. xgb1 &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer auc &lt;- numeric(r) # Pre-allocate a numeric vector for AUC values for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] test_y &lt;- y[-ind] # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = 1) predictions &lt;- predict(model, dtest) pred &lt;- ROCR::prediction(predictions, test_y) perf &lt;- ROCR::performance(pred, &quot;auc&quot;) auc[i] &lt;- perf@y.values[[1]] } end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(auc = auc, time_taken = time_taken) } xgb2 &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer auc &lt;- numeric(r) # Initialize the AUC vector to store AUC values for each iteration for (i in 1:r) { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] # Potential indexing issue test_y &lt;- y[-ind] # Potential indexing issue # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = detectCores() - 1) # Predicting and calculating AUC predictions &lt;- predict(model, dtest) pred &lt;- ROCR::prediction(predictions, test_y) perf &lt;- ROCR::performance(pred, &quot;auc&quot;) auc[i] &lt;- perf@y.values[[1]] } end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(mean_auc = mean(auc), time_taken = time_taken) } xgbpar &lt;- function(r, xs, y, params, nrounds) { start_time &lt;- Sys.time() # Start timer cl &lt;- makeCluster(detectCores() - 1) # Use one less than the total number of cores registerDoParallel(cl) # Parallel processing using foreach results &lt;- foreach(i = 1:r, .combine = &#39;c&#39;, .packages = c(&#39;xgboost&#39;, &#39;ROCR&#39;)) %dopar% { # Bootstrap sampling ind &lt;- sample(nrow(xs), nrow(xs) * 0.8) md_x &lt;- xs[ind, ] md_y &lt;- y[ind] test_x &lt;- xs[-ind, ] # Potential indexing issue test_y &lt;- y[-ind] # Potential indexing issue # onvert to DMatrix dtrain &lt;- xgb.DMatrix(data = md_x, label = md_y) dtest &lt;- xgb.DMatrix(data = test_x, label = test_y) # Train the model params &lt;- list(objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;) model &lt;- xgb.train(params = params, data = dtrain, nrounds = nrounds, nthread = 1) predictions &lt;- predict(model, dtest) pred &lt;- prediction(predictions, test_y) perf &lt;- performance(pred, &quot;auc&quot;) perf@y.values[[1]] } # Stop the cluster stopCluster(cl) end_time &lt;- Sys.time() # End timer time_taken &lt;- as.numeric(end_time - start_time, units = &quot;secs&quot;) list(mean_auc = mean(results), time_taken = time_taken) } 6.2 Data generate_dataset &lt;- function(n_rows) { set.seed(123) # Ensure reproducibility df &lt;- data.frame(y = sample(c(0, 1), n_rows, replace = TRUE)) # Add 10 numeric columns for(i in 1:10) { df &lt;- df %&gt;% mutate(!!paste0(&quot;num&quot;, i) := runif(n_rows)) } # Add 20 factor columns for(i in 1:20) { df &lt;- df %&gt;% mutate(!!paste0(&quot;fac&quot;, i) := factor(sample(c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), n_rows, replace = TRUE))) } return(df) } # Generate datasets of different sizes df&lt;- generate_dataset(1000) dfb &lt;- generate_dataset(500000) 6.3 10,000 rows Create the data. as a note this data has no relations so it wont be predictable 20 factor collums 10 numeric 6.3.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = df) y &lt;- df$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1sf &lt;- x1$time_taken t2sf &lt;- x2$time_taken tpsf &lt;- xp$time_taken 6.3.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- df$y xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1ssf &lt;- x1$time_taken t2ssf &lt;- x2$time_taken tpssf &lt;- xp$time_taken 6.4 500,000 rows 6.4.1 With a normal Matrix xs &lt;- model.matrix(~ . -1 - y, data = dfb) y &lt;- dfb$y params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t150 &lt;- x1$time_taken t250 &lt;- x2$time_taken tp50 &lt;- xp$time_taken 6.4.2 With a sparse matrix For a sparse matrix. The data must be one hot coded then turned into a dataframe. Then turned into a sparse matrix. y &lt;- dfb$y xs &lt;- model.matrix(~ . -1 - y, data = dfb) xs &lt;- sparse.model.matrix(~ ., data = as.data.frame(xs)) params &lt;- list( booster = &quot;gbtree&quot;, objective = &quot;binary:logistic&quot;, eta = 0.1, gamma = 0, max_depth = 6, min_child_weight = 1, subsample = 1, colsample_bytree = 1, lambda = 1, alpha = 0 ) nrounds &lt;- 100 r &lt;- detectCores() - 1 # Execute the functions x1 &lt;- xgb1(r, xs, y, params, nrounds) x2 &lt;- xgb2(r, xs, y, params, nrounds) xp &lt;- xgbpar(r, xs, y, params, nrounds) # Extract execution times t1s50 &lt;- x1$time_taken t2s50 &lt;- x2$time_taken tps50 &lt;- xp$time_taken 6.5 Presentations Print them out nrounds is 100 for all 10ks matrix t1sf ## [1] 2.435772 t2sf ## [1] 4.363386 tpsf ## [1] 4.455129 sparse t1ssf ## [1] 1.903891 t2ssf ## [1] 4.151235 tpssf ## [1] 4.104647 500ks matrix t150 ## [1] 1104.106 t250 ## [1] 378.959 tp50 ## [1] 707.4114 sparse t1s50 ## [1] 754.743 t2s50 ## [1] 274.7935 tps50 ## [1] 545.2745 6.6 Analysis 6.6.1 Graphs # Data preparation results &lt;- data.frame( dataset_size = c(rep(&quot;10k&quot;, 6), rep(&quot;500k&quot;, 6)), matrix_type = rep(c(&quot;Normal&quot;, &quot;Sparse&quot;), each = 3, times = 2), method = rep(c(&quot;No PP&quot;, &quot;XGBoost PP&quot;, &quot;Own PP&quot;), times = 4), execution_time = c(t1sf, t2sf, tpsf, t1ssf, t2ssf, tpssf, t150, t250, tp50, t1s50, t2s50, tps50) ) # Graph for comparing methods on 10k rows dataset ggplot(subset(results, dataset_size == &quot;10k&quot;), aes(x = method, y = execution_time, fill = matrix_type)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time Comparison for 10k Rows Dataset&quot;, x = &quot;Method&quot;, y = &quot;Execution Time (seconds)&quot;, fill = &quot;Matrix Type&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Graph for comparing methods on 500k rows dataset ggplot(subset(results, dataset_size == &quot;500k&quot;), aes(x = method, y = execution_time, fill = matrix_type)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Execution Time Comparison for 500k Rows Dataset&quot;, x = &quot;Method&quot;, y = &quot;Execution Time (seconds)&quot;, fill = &quot;Matrix Type&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) # Update the results data frame to include execution time per row results$execution_time_per_row &lt;- with(results, execution_time / ifelse(dataset_size == &quot;10k&quot;, 10000, 500000)) # Re-define the plotting function to use execution_time_per_row plot_dataset_size_effect_per_row &lt;- function(data, method_name) { ggplot(subset(data, method == method_name), aes(x = dataset_size, y = execution_time_per_row, fill = dataset_size)) + geom_bar(stat = &quot;identity&quot;) + labs(title = paste(&quot;Effect of Dataset Size on&quot;, method_name, &quot;Execution Time Per Row&quot;), x = &quot;Dataset Size&quot;, y = &quot;Execution Time Per Row (seconds)&quot;, fill = &quot;Dataset Size&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) } # Separate results for normal and sparse matrices normal_results_per_row &lt;- subset(results, matrix_type == &quot;Normal&quot;) sparse_results_per_row &lt;- subset(results, matrix_type == &quot;Sparse&quot;) # Plotting for each method, now on a per-row basis plot_dataset_size_effect_per_row(normal_results_per_row, &quot;No PP&quot;) plot_dataset_size_effect_per_row(normal_results_per_row, &quot;XGBoost PP&quot;) plot_dataset_size_effect_per_row(normal_results_per_row, &quot;Own PP&quot;) # Plotting for No PP, XGBoost PP, and Own PP methods for Sparse Matrix plot_dataset_size_effect_per_row(sparse_results_per_row, &quot;No PP&quot;) + ggtitle(&quot;No Parallel Processing (Sparse Matrix)&quot;) plot_dataset_size_effect_per_row(sparse_results_per_row, &quot;XGBoost PP&quot;) + ggtitle(&quot;XGBoost Parallel Processing (Sparse Matrix)&quot;) plot_dataset_size_effect_per_row(sparse_results_per_row, &quot;Own PP&quot;) + ggtitle(&quot;Own Parallel Processing (Sparse Matrix)&quot;) 6.6.2 Takaways With factor collums sparse seems to be better. With numeric collums though it would be worse. XGB is better with internal processing when nrows is huge. When nrows is smaller though we should probably be using gbm anyways. Also times include how many loops we are doing which is related to numcores of the machine being used if you look at our codes 6.6.3 Things to change Test light gbm and gbm with these. test if only numerical data kills a sparse matrix. What about doparrallel with numcores set inside the xgboost funtions. What about purrr or other ways of parrallel processing. Do correlations in the data(the data actually being predictable have a effect). Does scaling help with speed? Test purrr/furrr Test these vs h2o "],["multi-variable-classification.html", "Chapter 7 Multi Variable Classification 7.1 Data 7.2 AUCs 7.3 Confusion Table", " Chapter 7 Multi Variable Classification 7.1 Data library(randomForest) library(ROCR) library(dplyr) library(caret) # Load the Wine Quality dataset url &lt;- &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv&quot; df &lt;- read.csv(url, sep=&quot;;&quot;) names(df)[names(df) == &quot;quality&quot;] &lt;- &quot;y&quot; df &lt;- df[c(&quot;y&quot;, setdiff(names(df), &quot;y&quot;))] df$y &lt;- as.factor(df$y) glimpse(df) ## Rows: 1,599 ## Columns: 12 ## $ y &lt;fct&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 7… ## $ fixed.acidity &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5… ## $ volatile.acidity &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, … ## $ citric.acid &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0… ## $ residual.sugar &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1,… ## $ chlorides &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, … ## $ free.sulfur.dioxide &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16… ## $ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102,… ## $ density &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0… ## $ pH &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3… ## $ sulphates &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0… ## $ alcohol &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.… table(df$y) ## ## 3 4 5 6 7 8 ## 10 53 681 638 199 18 we need to drop 3 and 8 df&lt;- df[!df$y %in% c(3, 8), ] df$y &lt;- droplevels(df$y) 7.2 AUCs n &lt;- nrow(df) ind &lt;- unique(sample(n, n, T)) train &lt;- df[ind, ] test &lt;- df[-ind, ] rf &lt;- randomForest(y ~ ., data = train, ntree = 500) phat &lt;- predict(rf, test, type = &quot;prob&quot;) auc_list &lt;- list() for (level in levels(test$y)) { # Making the current level the positive class actual &lt;- ifelse(test$y == level, 1, 0) pred &lt;- prediction(phat[, level], actual) perf &lt;- performance(pred, measure = &quot;auc&quot;) auc_list[[level]] &lt;- as.numeric(perf@y.values) } auc_list ## $`4` ## [1] 0.7856884 ## ## $`5` ## [1] 0.8840401 ## ## $`6` ## [1] 0.8115992 ## ## $`7` ## [1] 0.9240204 # Prepare plotting par(mfrow = c(2, ceiling(length(levels(test$y)) / 2))) # Adjust layout based on number of levels # Loop through each level to plot ROC curve for (level in levels(test$y)) { actual &lt;- ifelse(test$y == level, 1, 0) pred &lt;- prediction(phat[, level], actual) perf &lt;- performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) # Plotting the ROC curve plot(perf, col = &quot;red&quot;, main = paste(&quot;ROC Curve for&quot;, level)) abline(a = 0, b = 1, lty = 2) # Adding a diagonal line } 7.3 Confusion Table r &lt;- 100 n &lt;- nrow(df) num_classes &lt;- length(levels(df$y)) auc_matrix &lt;- matrix(0, nrow = r, ncol = num_classes) colnames(auc_matrix) &lt;- levels(df$y) cm_list &lt;- vector(&quot;list&quot;, r) for (i in 1:r) { ind &lt;- sample(n, n, replace = TRUE) tr &lt;- df[ind, ] ts &lt;- df[-ind, ] rf &lt;- randomForest(y ~ ., data = tr, ntree = 500) ph &lt;- predict(rf, ts, type = &quot;prob&quot;) pc &lt;- apply(ph, 1, which.max) pc &lt;- levels(ts$y)[pc] cm &lt;- confusionMatrix(factor(pc), factor(ts$y)) cm_list[[i]] &lt;- as.matrix(cm$table) } # Calculate the average confusion matrix cm_avg &lt;- Reduce(&quot;+&quot;, cm_list) / length(cm_list) cm_avg ## Reference ## Prediction 4 5 6 7 ## 4 0.14 0.38 0.28 0.00 ## 5 12.76 195.09 56.80 3.98 ## 6 6.40 51.50 163.34 35.13 ## 7 0.31 1.92 13.71 34.02 We see huge imbalances in data in the target variable but i just wanted to leave this here as a basic idea. in truth we would drop some categories and deal with it better. "],["h20.html", "Chapter 8 h20 8.1 Set-Up 8.2 EDA 8.3 Models h20", " Chapter 8 h20 8.1 Set-Up library(readr) library(dplyr) library(tidyr) library(ggplot2) library(DataExplorer) library(caret) library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 library(tidyr) library(readr) data &lt;- read_csv(&quot;machine failure.csv&quot;) ## Rows: 10000 Columns: 14 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Product ID, Type ## dbl (12): UDI, Air temperature [K], Process temperature [K], Rotational spee... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(data) ## Rows: 10,000 ## Columns: 14 ## $ UDI &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 1… ## $ `Product ID` &lt;chr&gt; &quot;M14860&quot;, &quot;L47181&quot;, &quot;L47182&quot;, &quot;L47183&quot;, &quot;L47… ## $ Type &lt;chr&gt; &quot;M&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;L&quot;, &quot;M&quot;, &quot;L&quot;, &quot;L&quot;, &quot;M&quot;,… ## $ `Air temperature [K]` &lt;dbl&gt; 298.1, 298.2, 298.1, 298.2, 298.2, 298.1, 29… ## $ `Process temperature [K]` &lt;dbl&gt; 308.6, 308.7, 308.5, 308.6, 308.7, 308.6, 30… ## $ `Rotational speed [rpm]` &lt;dbl&gt; 1551, 1408, 1498, 1433, 1408, 1425, 1558, 15… ## $ `Torque [Nm]` &lt;dbl&gt; 42.8, 46.3, 49.4, 39.5, 40.0, 41.9, 42.4, 40… ## $ `Tool wear [min]` &lt;dbl&gt; 0, 3, 5, 7, 9, 11, 14, 16, 18, 21, 24, 29, 3… ## $ `Machine failure` &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ TWF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ HDF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ PWF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OSF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ RNF &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ID: A unique identifier for each observation in the dataset. Product Id: A combined identifier that starts with the machine type followed by a numeric identifier. Type: Categorizes the type of machine, which can affect its failure rates and operational characteristics. Air Temperature [K]: The ambient air temperature around the machine, measured in Kelvin, which might influence machine performance. Process Temperature [K]: The operational temperature of the machine during the process, also measured in Kelvin. Rotational Speed [rpm]: Indicates how fast the machine operates, measured in rotations per minute. Torque [Nm]: Measures the twisting force that causes rotation, in Newton-meters. High torque may indicate higher operational stress. Tool Wear [min]: Tracks the amount of wear on the machine’s tools, suggesting when maintenance or replacement might be necessary. Machine Failure: The target variable indicating if a failure occurred (1) or not (0). Additional columns related to specific failure types include: TWF (Tool Wear Failure): Indicates failures due to the tool wearing out. HDF (Heat Dissipation Failure): Relates to failures caused by inadequate heat dissipation. PWF (Power Failure): Associated with failures due to power issues. OSF (Overstrain Failure): Indicates failures from overstressing the machine. RNF (Random Failure): Captures failures that are random or do not fit into other specified categories. library(janitor) ## Warning: package &#39;janitor&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test data &lt;- data %&gt;% clean_names() data &lt;- data %&gt;% mutate(across(c(type, twf, hdf, pwf, osf, rnf, machine_failure), as.factor)) data &lt;- data[,-(1:2)] # kill the two ids data &lt;- data %&gt;% rename(y = machine_failure) %&gt;% relocate(y, .before = 1) The data is ready for gerneral usage introduce(data) ## # A tibble: 1 × 9 ## rows columns discrete_columns continuous_columns all_missing_columns ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 10000 12 7 5 0 ## # ℹ 4 more variables: total_missing_values &lt;int&gt;, complete_rows &lt;int&gt;, ## # total_observations &lt;int&gt;, memory_usage &lt;dbl&gt; plot_missing(data) 8.2 EDA # Histograms for numerical variables plot_histogram(data) # For factor variables, use bar plots plot_bar(data) # Summary statistics for numerical data data %&gt;% summarise(across(where(is.numeric), list(mean = mean, sd = sd, median = median, IQR = IQR, min = min, max = max))) ## # A tibble: 1 × 30 ## air_temperature_k_mean air_temperature_k_sd air_temperature_k_median ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 300. 2.00 300. ## # ℹ 27 more variables: air_temperature_k_IQR &lt;dbl&gt;, ## # air_temperature_k_min &lt;dbl&gt;, air_temperature_k_max &lt;dbl&gt;, ## # process_temperature_k_mean &lt;dbl&gt;, process_temperature_k_sd &lt;dbl&gt;, ## # process_temperature_k_median &lt;dbl&gt;, process_temperature_k_IQR &lt;dbl&gt;, ## # process_temperature_k_min &lt;dbl&gt;, process_temperature_k_max &lt;dbl&gt;, ## # rotational_speed_rpm_mean &lt;dbl&gt;, rotational_speed_rpm_sd &lt;dbl&gt;, ## # rotational_speed_rpm_median &lt;dbl&gt;, rotational_speed_rpm_IQR &lt;dbl&gt;, … # Identify zero variance features zero_var_indices &lt;- nearZeroVar(data, saveMetrics = TRUE) # View the metrics to determine which variables have zero or near-zero variance print(zero_var_indices) ## freqRatio percentUnique zeroVar nzv ## y 28.498525 0.02 FALSE TRUE ## type 2.002002 0.03 FALSE FALSE ## air_temperature_k 1.207792 0.93 FALSE FALSE ## process_temperature_k 1.161172 0.82 FALSE FALSE ## rotational_speed_rpm 1.116279 9.41 FALSE FALSE ## torque_nm 1.040000 5.77 FALSE FALSE ## tool_wear_min 1.739130 2.46 FALSE FALSE ## twf 216.391304 0.02 FALSE TRUE ## hdf 85.956522 0.02 FALSE TRUE ## pwf 104.263158 0.02 FALSE TRUE ## osf 101.040816 0.02 FALSE TRUE ## rnf 525.315789 0.02 FALSE TRUE # Optionally, print the names of columns with zero variance zero_var_columns &lt;- names(data)[zero_var_indices$nzv] print(zero_var_columns) ## [1] &quot;y&quot; &quot;twf&quot; &quot;hdf&quot; &quot;pwf&quot; &quot;osf&quot; &quot;rnf&quot; for now we wont care about the type of failure we just will care about whether it failed or not # right now i am forced to use the numerical positions of the collumns due to issues knitting into a book. in practice we want to use select and name the columns data &lt;- data[, -(8:12)] 8.3 Models h20 library(h2o) ## Warning: package &#39;h2o&#39; was built under R version 4.3.3 ## ## ---------------------------------------------------------------------- ## ## Your next step is to start H2O: ## &gt; h2o.init() ## ## For H2O package documentation, ask for help: ## &gt; ??h2o ## ## After starting H2O, you can use the Web UI at http://localhost:54321 ## For more information visit https://docs.h2o.ai ## ## ---------------------------------------------------------------------- ## ## Attaching package: &#39;h2o&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## day, hour, month, week, year ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, sd, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, %in%, &amp;&amp;, ||, apply, as.factor, as.numeric, colnames, ## colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log, ## log10, log1p, log2, round, signif, trunc #dir.create(&quot;C:/Users/simon/Dropbox/github_ML/Codes&amp;Processes/h2o_logs&quot;) h2o.init(max_mem_size = &quot;12g&quot;, log_dir = &quot;C:/Users/simon/Dropbox/github_ML/Codes&amp;Processes/h2o_logs&quot;) ## ## H2O is not running yet, starting it now... ## ## Note: In case of errors look at the following log files: ## C:\\Users\\simon\\AppData\\Local\\Temp\\RtmpYLIvSo\\file948eb65276/h2o_simon_started_from_r.out ## C:\\Users\\simon\\AppData\\Local\\Temp\\RtmpYLIvSo\\file94823196abc/h2o_simon_started_from_r.err ## ## ## Starting H2O JVM and connecting: Connection successful! ## ## R is connected to the H2O cluster: ## H2O cluster uptime: 4 seconds 120 milliseconds ## H2O cluster timezone: America/Halifax ## H2O data parsing timezone: UTC ## H2O cluster version: 3.44.0.3 ## H2O cluster version age: 1 year and 8 days ## H2O cluster name: H2O_started_from_R_simon_dsf270 ## H2O cluster total nodes: 1 ## H2O cluster total memory: 10.66 GB ## H2O cluster total cores: 12 ## H2O cluster allowed cores: 12 ## H2O cluster healthy: TRUE ## H2O Connection ip: localhost ## H2O Connection port: 54321 ## H2O Connection proxy: NA ## H2O Internal Security: FALSE ## R Version: R version 4.3.2 (2023-10-31 ucrt) ## Warning in h2o.clusterInfo(): ## Your H2O cluster version is (1 year and 8 days) old. There may be a newer version available. ## Please download and install the latest version from: https://h2o-release.s3.amazonaws.com/h2o/latest_stable.html data_h2o &lt;- as.h2o(data) ## | | | 0% | |======================================================================| 100% data_h2o$y &lt;- as.factor(data_h2o$y) 8.3.1 Random Forest auc_values &lt;- c() for (i in 1:100) { # Split the data into train and test sets splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8) train &lt;- splits[[1]] test &lt;- splits[[2]] # Train a Random Forest model rf_model &lt;- h2o.randomForest( x = names(train)[-which(names(train) == &quot;y&quot;)], y = &quot;y&quot;, training_frame = train, ntrees = 1200 ) # Perform predictions predictions &lt;- h2o.predict(rf_model, test) # Get performance object perf &lt;- h2o.performance(rf_model, newdata = test) # Calculate AUC auc_values[i] &lt;- h2o.auc(perf) } ## | | | 0% | |=== | 4% | |=============== | 21% | |================== | 26% | |========================================== | 60% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======== | 11% | |=================== | 28% | |============================================ | 63% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |================================ | 45% | |=================================================== | 72% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |=========== | 15% | |==================== | 28% | |=============================== | 44% | |================================================== | 72% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |========= | 13% | |============= | 18% | |==================== | 29% | |=============================== | 44% | |========================================== | 60% | |======================================================= | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 2% | |======== | 12% | |============== | 20% | |=================== | 28% | |=============================== | 44% | |=================================================== | 74% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |============ | 17% | |===================================== | 52% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |============= | 19% | |=============================== | 44% | |==================================================== | 74% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 6% | |========== | 14% | |=================== | 27% | |=============================== | 45% | |================================================== | 71% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 9% | |============= | 19% | |=================================== | 50% | |=============================================== | 68% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |========== | 14% | |==================== | 29% | |======================== | 34% | |========================================== | 59% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |========== | 14% | |=============== | 21% | |============================== | 42% | |===================================================== | 75% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 11% | |================= | 25% | |====================== | 31% | |=================================== | 50% | |======================================= | 56% | |====================================================== | 77% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======== | 11% | |============= | 19% | |======================= | 33% | |======================================= | 56% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==================== | 28% | |========================================= | 58% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |=============== | 22% | |=========================== | 39% | |=================================================== | 73% | |======================================================================| 99% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |======= | 10% | |================= | 24% | |================================= | 47% | |===================================================== | 75% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |============ | 16% | |================================= | 48% | |====================================================== | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |=========== | 15% | |================== | 25% | |===================================== | 52% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========== | 14% | |========================= | 36% | |=============================================== | 67% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========= | 13% | |============= | 18% | |======================== | 34% | |=============================== | 44% | |=================================================== | 73% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |============= | 19% | |=================================== | 50% | |========================================================= | 81% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |====== | 8% | |================== | 26% | |================================== | 49% | |========================================================= | 82% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |=========== | 15% | |====================== | 32% | |============================================ | 63% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 12% | |============ | 17% | |====================== | 32% | |================================= | 48% | |=============================================== | 67% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |=========== | 16% | |========================== | 37% | |============================================== | 66% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========== | 14% | |============= | 18% | |===================== | 30% | |================================= | 46% | |==================================== | 52% | |============================================= | 64% | |====================================================== | 78% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========== | 14% | |===================== | 29% | |========================= | 35% | |==================================== | 51% | |================================================== | 72% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========== | 14% | |================= | 24% | |=============================== | 44% | |======================================================= | 78% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========= | 13% | |=================== | 27% | |================================ | 46% | |================================================ | 69% | |=========================================================== | 84% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |=================== | 27% | |================================= | 47% | |================================================= | 71% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |============ | 17% | |========================= | 36% | |================================== | 49% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |=========== | 16% | |=============== | 22% | |========================== | 36% | |=============================================== | 68% | |================================================================= | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======== | 12% | |============== | 20% | |======================= | 33% | |================================ | 46% | |========================================= | 58% | |================================================== | 72% | |======================================================= | 78% | |=============================================================== | 90% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 2% | |===== | 7% | |============= | 19% | |==================== | 29% | |============================== | 43% | |========================================= | 59% | |======================================================= | 79% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 12% | |============== | 20% | |=========================== | 38% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 2% | |====== | 9% | |========== | 14% | |============ | 17% | |================ | 23% | |========================= | 35% | |================================== | 49% | |============================================ | 63% | |======================================================= | 78% | |========================================================== | 82% | |============================================================= | 87% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |====== | 9% | |============= | 18% | |=============== | 22% | |====================== | 32% | |====================================== | 54% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |===== | 7% | |============ | 17% | |===================== | 30% | |================================= | 47% | |============================================ | 64% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |============== | 20% | |==================================== | 51% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |=========== | 16% | |============= | 19% | |================= | 24% | |=========================== | 39% | |=========================================== | 61% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======== | 12% | |================ | 23% | |=================================== | 50% | |==================================================== | 75% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |===== | 7% | |============= | 19% | |=================== | 28% | |============================ | 40% | |================================== | 48% | |===================================== | 52% | |============================================== | 65% | |========================================================== | 83% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |====== | 9% | |============== | 20% | |================== | 25% | |====================== | 31% | |================================ | 45% | |=========================================== | 62% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |===== | 6% | |============== | 20% | |======================= | 33% | |================================ | 46% | |==================================================== | 74% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |====== | 8% | |========== | 15% | |============= | 19% | |==================== | 28% | |============================ | 40% | |=================================== | 50% | |============================================ | 63% | |==================================================== | 75% | |======================================================= | 78% | |=========================================================== | 85% | |============================================================== | 88% | |================================================================ | 91% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |============ | 18% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========= | 12% | |============ | 18% | |============================ | 40% | |=============================================== | 67% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |================= | 24% | |====================================== | 55% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========= | 12% | |======================= | 33% | |============================== | 44% | |======================================== | 57% | |================================================= | 70% | |============================================================ | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |================= | 24% | |============================= | 41% | |============================================= | 64% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 9% | |========= | 14% | |==================== | 28% | |============================ | 40% | |==================================== | 51% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======== | 12% | |========== | 15% | |=========================== | 38% | |================================================ | 68% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |=================== | 27% | |======================================= | 56% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |======== | 12% | |============ | 18% | |===================== | 30% | |================================ | 45% | |====================================== | 54% | |================================================== | 71% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |============ | 17% | |============================== | 43% | |=================================================== | 73% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |====== | 9% | |========= | 13% | |============ | 17% | |=============== | 21% | |================== | 25% | |==================== | 29% | |============================== | 42% | |=================================== | 50% | |========================================= | 59% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |============== | 20% | |============================= | 42% | |======================================= | 56% | |================================================= | 70% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |========= | 12% | |============= | 19% | |===================== | 30% | |================================== | 49% | |============================================ | 63% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |========== | 14% | |============= | 18% | |===================== | 30% | |============================ | 40% | |===================================== | 53% | |=================================================== | 73% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |=========== | 15% | |============================== | 43% | |==================================================== | 75% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |======= | 11% | |========== | 14% | |=============== | 21% | |========================== | 37% | |==================================== | 51% | |========================================== | 60% | |============================================== | 65% | |====================================================== | 77% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |=========== | 15% | |============== | 20% | |=========================== | 38% | |================================================ | 69% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |===== | 7% | |========= | 13% | |============ | 17% | |================= | 24% | |========================== | 36% | |========================== | 37% | |================================== | 48% | |======================================== | 57% | |========================================== | 60% | |================================================ | 69% | |======================================================== | 80% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |========= | 13% | |================== | 26% | |===================== | 30% | |================================ | 46% | |================================================= | 70% | |=========================================================== | 85% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |======== | 11% | |============== | 21% | |======================= | 33% | |================================ | 46% | |========================================= | 59% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |======== | 12% | |=========== | 15% | |===================== | 30% | |========================== | 37% | |======================================== | 57% | |======================================================== | 80% | |========================================================= | 82% | |============================================================== | 88% | |================================================================ | 92% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |===== | 7% | |========== | 15% | |=============== | 22% | |================== | 25% | |===================== | 29% | |========================= | 36% | |================================ | 46% | |==================================== | 51% | |============================================ | 63% | |=============================================== | 67% | |====================================================== | 77% | |============================================================ | 85% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========= | 13% | |============ | 17% | |=============== | 22% | |======================== | 34% | |================================ | 45% | |========================================= | 58% | |================================================== | 71% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========== | 15% | |================ | 22% | |======================== | 34% | |=============================== | 45% | |======================================== | 57% | |================================================== | 71% | |===================================================== | 75% | |========================================================== | 82% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |==== | 6% | |========== | 15% | |============== | 20% | |================ | 23% | |================== | 26% | |====================== | 32% | |======================== | 35% | |=========================== | 38% | |=============================== | 44% | |==================================== | 51% | |====================================== | 54% | |======================================== | 57% | |============================================== | 66% | |===================================================== | 76% | |======================================================== | 79% | |=============================================================== | 91% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======= | 10% | |=========== | 16% | |====================== | 32% | |=========================== | 39% | |===================================== | 54% | |==================================================== | 75% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 1% | |=== | 4% | |===== | 8% | |======== | 12% | |=========== | 15% | |============= | 19% | |=============== | 21% | |================= | 24% | |=================== | 28% | |===================== | 31% | |======================== | 34% | |============================ | 40% | |=============================== | 44% | |======================================= | 56% | |================================================= | 71% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======= | 10% | |============= | 18% | |===================== | 30% | |=============================== | 44% | |======================================== | 57% | |================================================== | 72% | |============================================================ | 86% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |==== | 6% | |======= | 10% | |========== | 15% | |============ | 17% | |=============== | 21% | |=================== | 27% | |========================== | 36% | |==================================== | 52% | |=========================================== | 62% | |======================================================= | 78% | |============================================================ | 86% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |===== | 7% | |============== | 20% | |================== | 25% | |============================ | 40% | |================================= | 47% | |======================================== | 57% | |================================================ | 68% | |================================================== | 71% | |===================================================== | 75% | |============================================================== | 88% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |==== | 6% | |========= | 13% | |================ | 22% | |========================== | 37% | |=================================== | 50% | |============================================== | 66% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======= | 10% | |========== | 14% | |============ | 18% | |================ | 23% | |====================== | 31% | |=============================== | 44% | |======================================= | 56% | |============================================ | 63% | |=============================================== | 66% | |================================================= | 70% | |==================================================== | 74% | |====================================================== | 78% | |============================================================== | 88% | |================================================================ | 92% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |===== | 7% | |========= | 13% | |================ | 22% | |========================== | 37% | |================================ | 46% | |========================================= | 58% | |=========================================== | 62% | |============================================== | 66% | |================================================= | 70% | |========================================================== | 83% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |====== | 9% | |============== | 20% | |======================== | 35% | |============================ | 40% | |============================== | 43% | |==================================== | 52% | |========================================= | 59% | |=============================================== | 68% | |========================================================== | 83% | |================================================================ | 92% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |==== | 5% | |======== | 11% | |========== | 14% | |============= | 18% | |=============== | 21% | |================ | 24% | |================== | 26% | |======================= | 32% | |========================= | 36% | |=========================== | 39% | |================================ | 45% | |====================================== | 55% | |================================================ | 69% | |================================================== | 72% | |========================================================= | 82% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |==== | 6% | |================ | 23% | |========================== | 37% | |==================================== | 52% | |============================================= | 64% | |====================================================== | 77% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========= | 13% | |=========== | 16% | |================== | 25% | |============================ | 40% | |===================================== | 53% | |=============================================== | 67% | |========================================================== | 82% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |====== | 8% | |============= | 19% | |==================== | 29% | |============================== | 43% | |=================================== | 49% | |======================================== | 57% | |================================================== | 71% | |====================================================== | 77% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=================================================== | 73% | |======================================================= | 78% | |========================================================= | 82% | |============================================================ | 85% | |============================================================== | 89% | |================================================================ | 92% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |==== | 6% | |====== | 9% | |========= | 13% | |=========== | 16% | |============== | 20% | |================ | 23% | |================== | 26% | |====================== | 31% | |======================== | 34% | |========================= | 36% | |============================== | 42% | |======================================= | 56% | |================================================= | 69% | |=========================================================== | 84% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |==== | 6% | |====== | 9% | |========== | 14% | |============ | 17% | |============== | 20% | |================ | 23% | |=================== | 27% | |========================== | 38% | |=================================== | 50% | |=========================================== | 62% | |==================================================== | 74% | |============================================================ | 85% | |============================================================== | 89% | |================================================================= | 92% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |===== | 7% | |========= | 13% | |============= | 18% | |================= | 24% | |===================== | 30% | |============================ | 40% | |=================================== | 49% | |============================================== | 65% | |================================================== | 71% | |===================================================== | 76% | |============================================================== | 89% | |================================================================ | 92% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |===== | 7% | |======== | 12% | |=========== | 16% | |============== | 19% | |================ | 22% | |================== | 26% | |====================== | 32% | |============================== | 42% | |======================================= | 56% | |========================================== | 60% | |============================================= | 64% | |=============================================== | 68% | |=================================================== | 72% | |====================================================== | 76% | |======================================================= | 79% | |=========================================================== | 84% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |===== | 8% | |========= | 13% | |=============== | 21% | |========================= | 36% | |============================================ | 63% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 11% | |============== | 20% | |======================= | 33% | |========================= | 35% | |================================== | 49% | |========================================= | 58% | |======================================================== | 80% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 4% | |======== | 11% | |================ | 22% | |========================= | 35% | |================================ | 46% | |===================================== | 53% | |======================================== | 57% | |========================================== | 60% | |============================================= | 65% | |===================================================== | 75% | |============================================================ | 86% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |========= | 13% | |============ | 17% | |============== | 20% | |================== | 25% | |===================== | 29% | |======================= | 33% | |=========================== | 39% | |================================== | 49% | |===================================== | 53% | |============================================== | 66% | |================================================= | 70% | |=================================================== | 73% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |= | 2% | |====== | 9% | |============== | 20% | |======================= | 32% | |================================ | 45% | |=================================================== | 72% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 2% | |=== | 5% | |========= | 14% | |================ | 23% | |========================= | 36% | |================================== | 48% | |======================================= | 56% | |========================================== | 60% | |============================================ | 63% | |=============================================== | 67% | |================================================= | 70% | |==================================================== | 74% | |====================================================== | 77% | |============================================================= | 87% | |================================================================= | 93% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |==== | 6% | |======== | 11% | |========= | 12% | |========== | 14% | |============= | 18% | |============== | 20% | |================= | 24% | |==================== | 28% | |======================= | 32% | |========================== | 37% | |================================= | 48% | |==================================== | 51% | |========================================= | 59% | |=============================================== | 66% | |======================================================== | 79% | |=========================================================== | 84% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 3% | |======= | 10% | |============== | 20% | |====================== | 32% | |============================= | 41% | |================================= | 47% | |=================================== | 50% | |===================================== | 53% | |======================================== | 57% | |========================================== | 60% | |============================================ | 63% | |============================================== | 65% | |================================================== | 71% | |==================================================== | 75% | |====================================================== | 78% | |========================================================= | 81% | |========================================================== | 83% | |============================================================= | 87% | |================================================================ | 92% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |==== | 6% | |======== | 12% | |========== | 15% | |============ | 18% | |============== | 21% | |================ | 23% | |==================== | 28% | |====================== | 31% | |======================== | 34% | |============================= | 41% | |================================ | 45% | |====================================== | 54% | |============================================== | 65% | |======================================================= | 78% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |==== | 6% | |======= | 10% | |=========== | 16% | |============= | 19% | |=============== | 22% | |==================== | 29% | |=============================== | 44% | |========================================= | 59% | |============================================== | 65% | |================================================ | 69% | |======================================================= | 79% | |=============================================================== | 89% | |=================================================================== | 96% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |== | 2% | |===== | 7% | |======= | 10% | |========= | 14% | |============ | 17% | |============== | 20% | |================= | 24% | |==================== | 29% | |========================== | 37% | |=================================== | 50% | |=========================================== | 61% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% # Calculate the mean AUC value mean(auc_values) ## [1] 0.9728363 8.3.2 Gradient Boosting Machine note that cartesian uses all the grid values. the random version just selects random values from the grid which is very valid # Define the hyperparameter grid with added ntrees and learn_rate_annealing options hyper_params &lt;- list( max_depth = seq(3, 6, by = 3), min_rows = c(10), learn_rate = seq(0.01, 0.2, by = 0.04), learn_rate_annealing = seq(0.99, 1, length.out = 2), col_sample_rate_per_tree = seq(0.75, 1, length.out = 2), col_sample_rate = seq(1, 1, length.out = 1), ntrees = c(100, 250, 500, 750), min_split_improvement = c(1e-5) ) search_criteria &lt;- list(strategy = &quot;Cartesian&quot;) How is column sampling implemented for GBM? For an example model using: 100-column dataset col_sample_rate_per_tree=0.754 col_sample_rate=0.8 (Refers to available columns after per-tree sampling) For each tree, the floor is used to determine the number - in this example, (0.754 * 100)=75 out of the 100 - of columns that are randomly picked, and then the floor is used to determine the number - in this case, (0.754 * 0.8 * 100)=60 - of columns that are then randomly chosen for each split decision (out of the 75). # Grid search setup with added parameters grid_result &lt;- h2o.grid( algorithm = &quot;gbm&quot;, grid_id = &quot;gbm_grid_search_extended&quot;, x = setdiff(names(data_h2o), &quot;y&quot;), y = &quot;y&quot;, training_frame = data_h2o, hyper_params = hyper_params, search_criteria = search_criteria, nfolds = 5 ) ## | | | 0% | |======================================================================| 100% # Retrieve the grid results, sorted by AUC full_grid &lt;- h2o.getGrid(grid_id = &quot;gbm_grid_search_extended&quot;, sort_by = &quot;auc&quot;, decreasing = TRUE) # Extract the grid results into a data frame full_summary_df &lt;- as.data.frame(full_grid@summary_table) # Print the structure of the data frame print(str(full_summary_df)) ## &#39;data.frame&#39;: 160 obs. of 10 variables: ## $ col_sample_rate : num 1 1 1 1 1 1 1 1 1 1 ... ## $ col_sample_rate_per_tree: num 1 1 1 1 1 1 0.75 1 0.75 1 ... ## $ learn_rate : num 0.09 0.05 0.13 0.05 0.13 0.17 0.13 0.17 0.01 0.05 ... ## $ learn_rate_annealing : num 0.99 1 0.99 0.99 1 0.99 1 0.99 1 1 ... ## $ max_depth : num 6 6 6 6 6 6 6 6 6 6 ... ## $ min_rows : num 10 10 10 10 10 10 10 10 10 10 ... ## $ min_split_improvement : num 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 ... ## $ ntrees : num 500 250 250 500 100 750 100 100 750 100 ... ## $ model_ids : chr &quot;gbm_grid_search_extended_model_106&quot; &quot;gbm_grid_search_extended_model_74&quot; &quot;gbm_grid_search_extended_model_68&quot; &quot;gbm_grid_search_extended_model_104&quot; ... ## $ auc : num 0.979 0.978 0.977 0.977 0.976 ... ## - attr(*, &quot;header&quot;)= chr &quot;Hyper-Parameter Search Summary&quot; ## - attr(*, &quot;formats&quot;)= chr [1:10] &quot;%.5f&quot; &quot;%.5f&quot; &quot;%.5f&quot; &quot;%.5f&quot; ... ## - attr(*, &quot;description&quot;)= chr &quot;ordered by decreasing auc&quot; ## NULL splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8) train &lt;- splits[[1]] test &lt;- splits[[2]] best_model &lt;- h2o.getModel(full_grid@model_ids[[1]]) predictions &lt;- h2o.predict(best_model, test) ## | | | 0% | |======================================================================| 100% actual &lt;- factor(as.vector(test$y), levels = c(&quot;1&quot;, &quot;0&quot;)) predicted &lt;- factor(as.vector(predictions$predict), levels = c(&quot;1&quot;, &quot;0&quot;)) # maxed f1 score confusion_matrix &lt;- table(Actual = actual, Predicted = predicted) print(confusion_matrix) ## Predicted ## Actual 1 0 ## 1 66 2 ## 0 0 1848 # Access the best model from the grid search best_model_id &lt;- grid_result@model_ids[[1]] best_model &lt;- h2o.getModel(best_model_id) # Retrieve the panel with all parameters used by the best model best_params &lt;- best_model@allparameters # Display winning hyperparameters best_params &lt;- best_model@allparameters cat(&quot;Winning Hyperparameters:\\n&quot;) ## Winning Hyperparameters: cat(sprintf(&quot;Max Depth: %d\\n&quot;, best_params$max_depth)) ## Max Depth: 6 cat(sprintf(&quot;Min Rows: %d\\n&quot;, best_params$min_rows)) ## Min Rows: 10 cat(sprintf(&quot;Learn Rate: %.2f\\n&quot;, best_params$learn_rate)) ## Learn Rate: 0.09 cat(sprintf(&quot;Learn Rate Annealing: %.2f\\n&quot;, best_params$learn_rate_annealing)) ## Learn Rate Annealing: 0.99 cat(sprintf(&quot;Sample Rate: %.2f\\n&quot;, best_params$col_sample_rate_per_tree)) ## Sample Rate: 1.00 cat(sprintf(&quot;Column Sample Rate: %.2f\\n&quot;, best_params$col_sample_rate)) ## Column Sample Rate: 1.00 cat(sprintf(&quot;Number of Trees (ntrees): %d\\n&quot;, best_params$ntrees)) ## Number of Trees (ntrees): 500 cat(sprintf(&quot;Minimum Split Improvement: %.1e\\n&quot;, best_params$min_split_improvement)) ## Minimum Split Improvement: 1.0e-05 # Retrieve the parameters used by the best model # Loop to perform repeated train-test split and model evaluation r &lt;- 100 auc_scores &lt;- numeric(r) for (i in 1:r) { splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8, seed = i) train &lt;- splits[[1]] test &lt;- splits[[2]] # Train the model with the best parameters from the grid search model &lt;- h2o.gbm( x = setdiff(names(train), &quot;y&quot;), y = &quot;y&quot;, training_frame = train, validation_frame = test, learn_rate = best_params$learn_rate, max_depth = best_params$max_depth, col_sample_rate_per_tree = best_params$col_sample_rate_per_tree, col_sample_rate = best_params$col_sample_rate, ntrees = best_params$ntrees, learn_rate_annealing = best_params$learn_rate_annealing, min_split_improvement = best_params$min_split_improvement ) perf &lt;- h2o.performance(model, newdata = test) auc_scores[i] &lt;- h2o.auc(perf) } ## | | | 0% | |== | 3% | |====== | 9% | |================================= | 47% | |===================================== | 53% | |======================================== | 57% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======= | 9% | |======================== | 35% | |=================================== | 50% | |================================================ | 68% | |====================================================== | 77% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 12% | |====================== | 32% | |================================ | 45% | |========================================= | 59% | |================================================== | 71% | |============================================================ | 86% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 10% | |================ | 23% | |=============================== | 45% | |======================================== | 57% | |================================================ | 69% | |=========================================================== | 84% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========= | 13% | |==================== | 29% | |============================== | 42% | |======================================== | 57% | |======================================================= | 79% | |=============================================================== | 91% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |============ | 17% | |============================ | 40% | |================================== | 48% | |============================================ | 63% | |===================================================== | 76% | |============================================================= | 88% | |======================================================================| 99% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |========== | 14% | |======================= | 32% | |================================ | 45% | |======================================= | 56% | |================================================ | 69% | |======================================================== | 79% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 12% | |================ | 23% | |======================= | 33% | |================================= | 47% | |========================================= | 58% | |================================================== | 71% | |========================================================== | 82% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 11% | |================= | 24% | |================================== | 49% | |========================================= | 59% | |================================================== | 71% | |=========================================================== | 85% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======== | 12% | |==================== | 28% | |============================== | 43% | |======================================== | 58% | |================================================= | 70% | |============================================================ | 86% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |= | 2% | |===== | 7% | |================= | 25% | |========================= | 35% | |============================= | 41% | |======================================= | 56% | |================================================== | 72% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 12% | |======================= | 32% | |============================ | 40% | |================================= | 47% | |=========================================== | 62% | |================================================== | 72% | |============================================================ | 86% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |=========== | 15% | |=================== | 27% | |========================== | 37% | |================================== | 49% | |============================================ | 62% | |================================================= | 69% | |========================================================= | 81% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========== | 14% | |================== | 26% | |============================== | 43% | |======================================= | 56% | |================================================ | 68% | |======================================================== | 80% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |====== | 9% | |============== | 20% | |====================== | 31% | |============================== | 43% | |======================================= | 55% | |============================================ | 63% | |===================================================== | 76% | |========================================================== | 83% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 12% | |======================== | 35% | |================================= | 48% | |======================================== | 58% | |================================================== | 71% | |========================================================= | 82% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========== | 14% | |================== | 26% | |============================ | 41% | |=================================== | 50% | |============================================ | 62% | |================================================= | 71% | |========================================================== | 83% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========= | 13% | |======================= | 33% | |================================ | 46% | |========================================= | 59% | |=============================================== | 67% | |======================================================= | 78% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======= | 9% | |=========== | 16% | |================= | 24% | |======================== | 34% | |================================= | 47% | |======================================== | 57% | |============================================= | 64% | |===================================================== | 75% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======= | 10% | |=============== | 21% | |===================== | 31% | |============================= | 41% | |=================================== | 50% | |======================================== | 57% | |=============================================== | 67% | |========================================================= | 82% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======= | 10% | |============= | 18% | |====================== | 32% | |====================================== | 54% | |========================================== | 60% | |=================================================== | 72% | |========================================================== | 82% | |================================================================= | 92% | |======================================================================| 100% ## | | | 0% | |= | 1% | |=== | 4% | |========== | 15% | |================== | 25% | |========================== | 38% | |================================== | 48% | |======================================= | 56% | |=============================================== | 67% | |====================================================== | 77% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |== | 3% | |====== | 8% | |=============== | 21% | |===================== | 31% | |============================= | 41% | |==================================== | 51% | |========================================= | 58% | |================================================ | 69% | |========================================================== | 82% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |======= | 10% | |============== | 20% | |======================= | 33% | |================================ | 46% | |======================================== | 57% | |==================================================== | 74% | |========================================================== | 82% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |======== | 11% | |=============== | 21% | |======================== | 35% | |================================ | 45% | |=========================================== | 62% | |================================================ | 68% | |======================================================= | 79% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======= | 10% | |================ | 22% | |======================= | 32% | |============================== | 43% | |=================================== | 51% | |=========================================== | 61% | |================================================= | 70% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |======= | 11% | |================ | 22% | |========================= | 36% | |================================= | 47% | |============================================ | 63% | |=================================================== | 72% | |========================================================== | 83% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |===== | 7% | |=========== | 16% | |==================== | 29% | |============================ | 40% | |=================================== | 50% | |======================================== | 58% | |================================================ | 68% | |============================================================ | 86% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 13% | |================== | 26% | |=========================== | 39% | |==================================== | 51% | |============================================ | 63% | |================================================== | 71% | |========================================================== | 83% | |============================================================ | 86% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======== | 11% | |========== | 15% | |================== | 26% | |=========================== | 38% | |============================== | 43% | |======================================== | 57% | |============================================== | 66% | |===================================================== | 76% | |======================================================================| 100% ## | | | 0% | |===================================== | 53% | |============================================ | 63% | |==================================================== | 74% | |====================================================== | 77% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |===== | 7% | |============= | 19% | |====================== | 31% | |=============================== | 44% | |====================================== | 55% | |================================================ | 69% | |======================================================= | 79% | |=========================================================== | 85% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |====== | 8% | |====== | 9% | |================= | 24% | |========================== | 38% | |=================================== | 50% | |=================================================== | 73% | |========================================================= | 82% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |================================================== | 71% | |============================================================ | 85% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======== | 11% | |==================== | 28% | |============================= | 41% | |====================================== | 54% | |=============================================== | 67% | |============================================================ | 86% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========== | 15% | |====================== | 31% | |========================================= | 58% | |=============================================== | 67% | |========================================================== | 82% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 13% | |====================== | 31% | |=============================== | 45% | |======================================== | 58% | |================================================ | 69% | |=========================================================== | 84% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 11% | |================= | 24% | |========================= | 35% | |================================== | 48% | |========================================== | 60% | |=========================================================== | 85% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========= | 13% | |===================== | 29% | |================================= | 47% | |========================================= | 58% | |=============================================== | 67% | |======================================================= | 78% | |=========================================================== | 85% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |====== | 9% | |=================== | 27% | |============================== | 43% | |================================== | 48% | |============================================ | 63% | |=============================================== | 68% | |================================================ | 69% | |======================================================== | 80% | |========================================================= | 81% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 11% | |================ | 23% | |======================== | 34% | |================================ | 45% | |======================================= | 55% | |=============================================== | 67% | |======================================================= | 79% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======= | 10% | |================ | 23% | |========================= | 36% | |================================ | 45% | |========================================= | 58% | |==================================================== | 74% | |========================================================== | 83% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |======= | 11% | |====================== | 31% | |================================== | 48% | |===================================== | 53% | |================================================= | 70% | |========================================================= | 82% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |========= | 13% | |===================== | 30% | |================================ | 46% | |======================================== | 57% | |================================================= | 70% | |============================================================ | 86% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 10% | |================ | 23% | |========================= | 36% | |=============================== | 45% | |============================================ | 62% | |=================================================== | 73% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 12% | |============== | 21% | |======================= | 33% | |============================== | 43% | |========================================== | 59% | |===================================================== | 75% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |========== | 14% | |===================== | 29% | |============================= | 41% | |================================== | 49% | |=========================================== | 62% | |==================================================== | 74% | |============================================================== | 89% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========== | 14% | |=================== | 28% | |=========================== | 39% | |================================= | 47% | |======================================== | 57% | |================================================== | 71% | |=========================================================== | 85% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======== | 12% | |================= | 24% | |========================= | 35% | |=================================== | 50% | |=========================================== | 62% | |===================================================== | 75% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========= | 12% | |=============== | 21% | |======================== | 34% | |================================ | 46% | |======================================== | 57% | |===================================================== | 76% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |====== | 8% | |================ | 23% | |========================= | 35% | |============================== | 42% | |===================================== | 53% | |============================================== | 66% | |====================================================== | 77% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========= | 12% | |================ | 23% | |======================== | 35% | |============================== | 43% | |======================================= | 56% | |=============================================== | 67% | |======================================================= | 79% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 12% | |============= | 18% | |======================= | 33% | |================================= | 48% | |========================================== | 60% | |==================================================== | 74% | |============================================================ | 86% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 11% | |=============== | 21% | |======================= | 33% | |=============================== | 44% | |======================================== | 57% | |=============================================== | 68% | |======================================================== | 80% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |========= | 13% | |=================== | 27% | |============================= | 41% | |======================================= | 55% | |================================================ | 68% | |========================================================== | 82% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |====== | 9% | |============ | 17% | |==================== | 28% | |========================= | 36% | |======================================== | 58% | |================================================== | 71% | |========================================================== | 83% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |==== | 6% | |================== | 26% | |=========================== | 39% | |==================================== | 51% | |============================================ | 63% | |===================================================== | 76% | |=============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |======== | 11% | |========= | 13% | |=============== | 21% | |=================== | 27% | |========================= | 36% | |================================= | 48% | |========================================= | 59% | |======================================================== | 80% | |========================================================= | 81% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |=========== | 15% | |=================== | 27% | |========================= | 36% | |================================= | 47% | |============================================== | 65% | |==================================================== | 74% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 12% | |================= | 24% | |============================= | 41% | |===================================== | 53% | |=========================================== | 62% | |==================================================== | 74% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |========== | 14% | |====================== | 32% | |============================== | 43% | |======================================== | 57% | |=============================================== | 68% | |======================================================== | 81% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |======== | 12% | |================== | 26% | |=========================== | 38% | |==================================== | 52% | |=========================================== | 61% | |===================================================== | 76% | |============================================================ | 85% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========= | 13% | |=========== | 16% | |=============== | 21% | |============================ | 40% | |===================================== | 53% | |======================================= | 56% | |================================================== | 71% | |===================================================== | 76% | |=============================================================== | 90% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |============================================= | 64% | |================================================== | 71% | |=========================================================== | 84% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 11% | |================== | 26% | |=========================== | 38% | |===================================== | 53% | |=============================================== | 67% | |========================================================= | 81% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |====== | 9% | |========= | 13% | |===================== | 29% | |==================================== | 51% | |=============================================== | 67% | |========================================================= | 81% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========== | 14% | |=================== | 28% | |============================ | 40% | |==================================== | 51% | |==================================================== | 75% | |============================================================ | 85% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========= | 13% | |================== | 25% | |========================== | 37% | |===================================== | 53% | |=============================================== | 67% | |========================================================== | 83% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======== | 12% | |===================== | 31% | |============================= | 42% | |======================================= | 55% | |================================================= | 69% | |======================================================== | 80% | |================================================================= | 93% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========== | 14% | |=================== | 27% | |============================== | 43% | |======================================= | 56% | |================================================ | 68% | |======================================================== | 81% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 10% | |================== | 26% | |=========================== | 38% | |====================================== | 54% | |================================================= | 71% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |======== | 12% | |======================= | 32% | |============================== | 43% | |====================================== | 54% | |============================================== | 66% | |======================================================= | 79% | |============================================================== | 88% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |========== | 14% | |===================== | 29% | |=================================== | 49% | |============================================= | 64% | |===================================================== | 75% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |=========== | 15% | |=================== | 27% | |============================= | 41% | |==================================== | 52% | |============================================= | 65% | |====================================================== | 78% | |============================================================= | 87% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |======= | 10% | |================== | 25% | |============================= | 41% | |======================================= | 56% | |============================================ | 63% | |===================================================== | 76% | |============================================================== | 89% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |== | 2% | |====== | 9% | |======= | 11% | |=================== | 27% | |====================================== | 54% | |================================================ | 69% | |================================================================= | 92% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |========== | 15% | |===================== | 30% | |============================= | 41% | |============================================= | 64% | |==================================================== | 74% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |========= | 13% | |==================== | 29% | |============================== | 43% | |======================================= | 56% | |================================================ | 69% | |======================================================= | 78% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======= | 10% | |==================== | 29% | |============================== | 42% | |======================================= | 56% | |================================================ | 68% | |========================================================== | 83% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |=== | 4% | |======== | 11% | |================= | 24% | |======================== | 35% | |=============================== | 45% | |========================================= | 58% | |==================================================== | 74% | |========================================================== | 83% | |================================================================== | 94% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |========= | 13% | |==================== | 29% | |=============================== | 44% | |============================================ | 63% | |==================================================== | 74% | |============================================================= | 88% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |========= | 12% | |===================== | 30% | |================================ | 45% | |========================================== | 60% | |==================================================== | 75% | |============================================================ | 86% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======= | 9% | |=============== | 21% | |========================= | 36% | |================================== | 48% | |============================================ | 63% | |==================================================== | 75% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |== | 3% | |======== | 11% | |================ | 23% | |========================= | 36% | |================================== | 49% | |============================================== | 66% | |======================================================= | 79% | |============================================================== | 88% | |===================================================================== | 99% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |============ | 17% | |======================== | 34% | |============================== | 43% | |====================================== | 54% | |=============================================== | 67% | |===================================================== | 76% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |======== | 11% | |================= | 24% | |========================== | 37% | |==================================== | 52% | |============================================ | 63% | |======================================================== | 79% | |============================================================ | 86% | |==================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |========== | 15% | |=========================== | 38% | |==================================== | 52% | |============================================== | 66% | |======================================================== | 80% | |============================================================= | 87% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 9% | |================== | 26% | |============================ | 40% | |========================================= | 58% | |===================================================== | 76% | |============================================================== | 89% | |======================================================================| 100% ## | | | 0% | |==== | 6% | |======= | 10% | |==================== | 29% | |============================= | 42% | |============================================= | 64% | |========================================================= | 82% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |====== | 8% | |======== | 12% | |======================== | 34% | |================================= | 47% | |======================================== | 57% | |=================================================== | 72% | |=========================================================== | 84% | |======================================================================| 100% ## | | | 0% | |===== | 8% | |========== | 14% | |==================== | 29% | |=============================== | 44% | |========================================= | 59% | |================================================= | 70% | |========================================================== | 83% | |================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |=== | 5% | |======== | 11% | |================ | 23% | |========================== | 37% | |==================================== | 51% | |============================================= | 64% | |====================================================== | 78% | |============================================================ | 86% | |=================================================================== | 96% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 12% | |=================== | 28% | |============================== | 42% | |====================================== | 54% | |================================================ | 68% | |======================================================== | 80% | |=================================================================== | 95% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 10% | |===================== | 30% | |============================== | 43% | |======================================== | 57% | |================================================= | 70% | |================================================================= | 92% | |======================================================================| 100% ## | | | 0% | |======= | 10% | |=========== | 15% | |============================== | 42% | |======================================= | 56% | |================================================= | 70% | |========================================================= | 81% | |================================================================ | 91% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 10% | |============== | 20% | |======================= | 33% | |================================ | 46% | |========================================== | 60% | |================================================== | 72% | |============================================================= | 88% | |===================================================================== | 98% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |========= | 13% | |================ | 23% | |========================== | 38% | |==================================== | 51% | |=========================================== | 62% | |==================================================== | 75% | |======================================================= | 79% | |================================================================ | 92% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |======= | 11% | |========================= | 35% | |================================ | 46% | |======================================= | 56% | |================================================ | 68% | |===================================================== | 76% | |=============================================================== | 90% | |======================================================================| 100% ## | | | 0% | |==== | 5% | |====== | 8% | |================ | 23% | |========================== | 37% | |====================================== | 55% | |=============================================== | 67% | |======================================================== | 80% | |============================================================ | 86% | |==================================================================== | 97% | |======================================================================| 100% ## | | | 0% | |===== | 7% | |========= | 12% | |======================== | 34% | |================================== | 48% | |======================================== | 57% | |================================================= | 70% | |======================================================= | 79% | |================================================================== | 95% | |======================================================================| 100% mean_auc &lt;- mean(auc_scores) print(paste(&quot;Average AUC over&quot;, r, &quot;trials:&quot;, mean_auc)) ## [1] &quot;Average AUC over 100 trials: 0.974888800717688&quot; 8.3.3 AutoML # Step 1: Define your target variable y &lt;- &quot;y&quot; # Step 2: Define predictors x &lt;- setdiff(names(data_h2o), y) # Step 3: Run AutoML aml &lt;- h2o.automl( x = x, y = y, training_frame = data_h2o, max_models = 20, max_runtime_secs = 3600*5 ) ## | | | 0% | |= | 2% ## 05:18:59.467: AutoML: XGBoost is not available; skipping it. | |== | 3% | |==== | 5% | |==== | 6% | |===== | 7% | |====== | 8% | |====== | 9% | |======== | 11% | |======== | 12% | |============ | 17% | |============= | 18% | |============== | 20% | |============== | 21% | |=============== | 21% | |================ | 23% | |================ | 24% | |================== | 26% | |=================== | 26% | |=============================== | 44% | |===================================== | 53% | |=========================================== | 62% | |====================================================== | 76% | |======================================================== | 79% | |======================================================================| 100% # Step 4: View the AutoML Leaderboard lb &lt;- h2o.get_leaderboard(aml, extra_columns = &quot;ALL&quot;) print(lb) ## model_id auc logloss ## 1 StackedEnsemble_AllModels_1_AutoML_1_20241229_51859 0.9772958 0.04725423 ## 2 GBM_3_AutoML_1_20241229_51859 0.9768935 0.05192053 ## 3 StackedEnsemble_BestOfFamily_1_AutoML_1_20241229_51859 0.9758105 0.04888945 ## 4 GBM_grid_1_AutoML_1_20241229_51859_model_1 0.9744953 0.05889433 ## 5 GBM_2_AutoML_1_20241229_51859 0.9734788 0.05307717 ## 6 GBM_4_AutoML_1_20241229_51859 0.9733428 0.05647616 ## aucpr mean_per_class_error rmse mse training_time_ms ## 1 0.8239358 0.1456038 0.1092502 0.01193562 26824 ## 2 0.7956109 0.1475445 0.1139472 0.01298396 3705 ## 3 0.8109416 0.1333128 0.1113926 0.01240830 30222 ## 4 0.7262399 0.2077319 0.1279477 0.01637062 5786 ## 5 0.7864638 0.1514777 0.1159855 0.01345263 1241 ## 6 0.7679228 0.1723078 0.1199868 0.01439684 5492 ## predict_time_per_row_ms algo ## 1 0.071819 StackedEnsemble ## 2 0.064883 GBM ## 3 0.080489 StackedEnsemble ## 4 0.092447 GBM ## 5 0.086089 GBM ## 6 0.084048 GBM ## ## [22 rows x 10 columns] # Step 5: Get the best model best_model &lt;- aml@leader h2o.performance(best_model, data_h2o) ## H2OBinomialMetrics: stackedensemble ## ## MSE: 0.00340925 ## RMSE: 0.05838878 ## LogLoss: 0.01656 ## Mean Per-Class Error: 0.02991256 ## AUC: 0.9997015 ## AUCPR: 0.9923206 ## Gini: 0.9994031 ## ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold: ## 0 1 Error Rate ## 0 9653 8 0.000828 =8/9661 ## 1 20 319 0.058997 =20/339 ## Totals 9673 327 0.002800 =28/10000 ## ## Maximum Metrics: Maximum metrics at their respective thresholds ## metric threshold value idx ## 1 max f1 0.379277 0.957958 133 ## 2 max f2 0.249510 0.957998 168 ## 3 max f0point5 0.436426 0.974922 122 ## 4 max accuracy 0.379277 0.997200 133 ## 5 max precision 0.999748 1.000000 0 ## 6 max recall 0.178150 1.000000 193 ## 7 max specificity 0.999748 1.000000 0 ## 8 max absolute_mcc 0.379277 0.956676 133 ## 9 max min_per_class_accuracy 0.194902 0.992030 187 ## 10 max mean_per_class_accuracy 0.178150 0.995394 193 ## 11 max tns 0.999748 9661.000000 0 ## 12 max fns 0.999748 306.000000 0 ## 13 max fps 0.000089 9661.000000 399 ## 14 max tps 0.178150 339.000000 193 ## 15 max tnr 0.999748 1.000000 0 ## 16 max fnr 0.999748 0.902655 0 ## 17 max fpr 0.000089 1.000000 399 ## 18 max tpr 0.178150 1.000000 193 ## ## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)` # Retrieve the best model from AutoML best_model &lt;- aml@leader # Initialize a vector to store AUC scores or any other performance metric performance_scores &lt;- numeric(100) # Loop to perform the train-test split, predict, and calculate performance 100 times for (i in 1:100) { # Split the data into 80% training and 20% testing splits &lt;- h2o.splitFrame(data = data_h2o, ratios = 0.8) train &lt;- splits[[1]] test &lt;- splits[[2]] pred &lt;- h2o.predict(best_model, test) # Evaluate performance perf &lt;- h2o.performance(best_model, newdata = test) auc_score &lt;- h2o.auc(perf) performance_scores[i] &lt;- auc_score } ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% ## | | | 0% | |======================================================================| 100% # Calculate average performance across all splits average_performance &lt;- mean(performance_scores) print(paste(&quot;Average AUC over 100 trials: &quot;, average_performance)) ## [1] &quot;Average AUC over 100 trials: 0.999699865299846&quot; now we get the info about the winning model best_model@model_id ## [1] &quot;StackedEnsemble_AllModels_1_AutoML_1_20241229_51859&quot; best_model@parameters ## $model_id ## [1] &quot;StackedEnsemble_AllModels_1_AutoML_1_20241229_51859&quot; ## ## $training_frame ## [1] &quot;AutoML_1_20241229_51859_training_RTMP_sid_b5df_5&quot; ## ## $base_models ## $base_models[[1]] ## $base_models[[1]]$`__meta` ## $base_models[[1]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[1]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[1]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[1]]$name ## [1] &quot;GBM_3_AutoML_1_20241229_51859&quot; ## ## $base_models[[1]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[1]]$URL ## NULL ## ## ## $base_models[[2]] ## $base_models[[2]]$`__meta` ## $base_models[[2]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[2]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[2]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[2]]$name ## [1] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_1&quot; ## ## $base_models[[2]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[2]]$URL ## NULL ## ## ## $base_models[[3]] ## $base_models[[3]]$`__meta` ## $base_models[[3]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[3]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[3]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[3]]$name ## [1] &quot;GBM_2_AutoML_1_20241229_51859&quot; ## ## $base_models[[3]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[3]]$URL ## NULL ## ## ## $base_models[[4]] ## $base_models[[4]]$`__meta` ## $base_models[[4]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[4]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[4]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[4]]$name ## [1] &quot;GBM_4_AutoML_1_20241229_51859&quot; ## ## $base_models[[4]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[4]]$URL ## NULL ## ## ## $base_models[[5]] ## $base_models[[5]]$`__meta` ## $base_models[[5]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[5]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[5]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[5]]$name ## [1] &quot;GBM_1_AutoML_1_20241229_51859&quot; ## ## $base_models[[5]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[5]]$URL ## NULL ## ## ## $base_models[[6]] ## $base_models[[6]]$`__meta` ## $base_models[[6]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[6]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[6]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[6]]$name ## [1] &quot;GBM_5_AutoML_1_20241229_51859&quot; ## ## $base_models[[6]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[6]]$URL ## NULL ## ## ## $base_models[[7]] ## $base_models[[7]]$`__meta` ## $base_models[[7]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[7]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[7]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[7]]$name ## [1] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_5&quot; ## ## $base_models[[7]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[7]]$URL ## NULL ## ## ## $base_models[[8]] ## $base_models[[8]]$`__meta` ## $base_models[[8]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[8]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[8]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[8]]$name ## [1] &quot;XRT_1_AutoML_1_20241229_51859&quot; ## ## $base_models[[8]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[8]]$URL ## NULL ## ## ## $base_models[[9]] ## $base_models[[9]]$`__meta` ## $base_models[[9]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[9]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[9]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[9]]$name ## [1] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_2&quot; ## ## $base_models[[9]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[9]]$URL ## NULL ## ## ## $base_models[[10]] ## $base_models[[10]]$`__meta` ## $base_models[[10]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[10]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[10]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[10]]$name ## [1] &quot;DRF_1_AutoML_1_20241229_51859&quot; ## ## $base_models[[10]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[10]]$URL ## NULL ## ## ## $base_models[[11]] ## $base_models[[11]]$`__meta` ## $base_models[[11]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[11]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[11]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[11]]$name ## [1] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_4&quot; ## ## $base_models[[11]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[11]]$URL ## NULL ## ## ## $base_models[[12]] ## $base_models[[12]]$`__meta` ## $base_models[[12]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[12]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[12]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[12]]$name ## [1] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_3&quot; ## ## $base_models[[12]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[12]]$URL ## NULL ## ## ## $base_models[[13]] ## $base_models[[13]]$`__meta` ## $base_models[[13]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[13]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[13]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[13]]$name ## [1] &quot;DeepLearning_grid_3_AutoML_1_20241229_51859_model_1&quot; ## ## $base_models[[13]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[13]]$URL ## NULL ## ## ## $base_models[[14]] ## $base_models[[14]]$`__meta` ## $base_models[[14]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[14]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[14]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[14]]$name ## [1] &quot;DeepLearning_grid_3_AutoML_1_20241229_51859_model_2&quot; ## ## $base_models[[14]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[14]]$URL ## NULL ## ## ## $base_models[[15]] ## $base_models[[15]]$`__meta` ## $base_models[[15]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[15]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[15]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[15]]$name ## [1] &quot;DeepLearning_grid_1_AutoML_1_20241229_51859_model_1&quot; ## ## $base_models[[15]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[15]]$URL ## NULL ## ## ## $base_models[[16]] ## $base_models[[16]]$`__meta` ## $base_models[[16]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[16]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[16]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[16]]$name ## [1] &quot;DeepLearning_grid_2_AutoML_1_20241229_51859_model_2&quot; ## ## $base_models[[16]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[16]]$URL ## NULL ## ## ## $base_models[[17]] ## $base_models[[17]]$`__meta` ## $base_models[[17]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[17]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[17]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[17]]$name ## [1] &quot;DeepLearning_grid_1_AutoML_1_20241229_51859_model_2&quot; ## ## $base_models[[17]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[17]]$URL ## NULL ## ## ## $base_models[[18]] ## $base_models[[18]]$`__meta` ## $base_models[[18]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[18]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[18]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[18]]$name ## [1] &quot;DeepLearning_1_AutoML_1_20241229_51859&quot; ## ## $base_models[[18]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[18]]$URL ## NULL ## ## ## $base_models[[19]] ## $base_models[[19]]$`__meta` ## $base_models[[19]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[19]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[19]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[19]]$name ## [1] &quot;DeepLearning_grid_2_AutoML_1_20241229_51859_model_1&quot; ## ## $base_models[[19]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[19]]$URL ## NULL ## ## ## $base_models[[20]] ## $base_models[[20]]$`__meta` ## $base_models[[20]]$`__meta`$schema_version ## [1] 3 ## ## $base_models[[20]]$`__meta`$schema_name ## [1] &quot;KeyV3&quot; ## ## $base_models[[20]]$`__meta`$schema_type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## ## $base_models[[20]]$name ## [1] &quot;GLM_1_AutoML_1_20241229_51859&quot; ## ## $base_models[[20]]$type ## [1] &quot;Key&lt;Keyed&gt;&quot; ## ## $base_models[[20]]$URL ## NULL ## ## ## ## $metalearner_algorithm ## [1] &quot;glm&quot; ## ## $metalearner_nfolds ## [1] 5 ## ## $metalearner_transform ## [1] &quot;Logit&quot; ## ## $seed ## [1] &quot;3501269604985256214&quot; ## ## $keep_levelone_frame ## [1] TRUE ## ## $x ## [1] &quot;type&quot; &quot;air_temperature_k&quot; &quot;process_temperature_k&quot; ## [4] &quot;rotational_speed_rpm&quot; &quot;torque_nm&quot; &quot;tool_wear_min&quot; ## ## $y ## [1] &quot;y&quot; its stacked ensambled so we need to get the models that are in the stack lets get the base models base_models &lt;- best_model@model$base_models base_models ## [1] &quot;GBM_3_AutoML_1_20241229_51859&quot; ## [2] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_1&quot; ## [3] &quot;GBM_2_AutoML_1_20241229_51859&quot; ## [4] &quot;GBM_4_AutoML_1_20241229_51859&quot; ## [5] &quot;GBM_1_AutoML_1_20241229_51859&quot; ## [6] &quot;GBM_5_AutoML_1_20241229_51859&quot; ## [7] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_5&quot; ## [8] &quot;XRT_1_AutoML_1_20241229_51859&quot; ## [9] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_2&quot; ## [10] &quot;DRF_1_AutoML_1_20241229_51859&quot; ## [11] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_4&quot; ## [12] &quot;GBM_grid_1_AutoML_1_20241229_51859_model_3&quot; ## [13] &quot;DeepLearning_grid_3_AutoML_1_20241229_51859_model_1&quot; ## [14] &quot;DeepLearning_grid_3_AutoML_1_20241229_51859_model_2&quot; ## [15] &quot;DeepLearning_grid_1_AutoML_1_20241229_51859_model_1&quot; ## [16] &quot;DeepLearning_grid_2_AutoML_1_20241229_51859_model_2&quot; ## [17] &quot;DeepLearning_grid_1_AutoML_1_20241229_51859_model_2&quot; ## [18] &quot;DeepLearning_1_AutoML_1_20241229_51859&quot; ## [19] &quot;DeepLearning_grid_2_AutoML_1_20241229_51859_model_1&quot; ## [20] &quot;GLM_1_AutoML_1_20241229_51859&quot; now the meta one metalearner &lt;- best_model@model$metalearner metalearner ## $`__meta` ## $`__meta`$schema_version ## [1] 3 ## ## $`__meta`$schema_name ## [1] &quot;ModelKeyV3&quot; ## ## $`__meta`$schema_type ## [1] &quot;Key&lt;Model&gt;&quot; ## ## ## $name ## [1] &quot;metalearner_AUTO_StackedEnsemble_AllModels_1_AutoML_1_20241229_51859&quot; ## ## $type ## [1] &quot;Key&lt;Model&gt;&quot; ## ## $URL ## [1] &quot;/3/Models/metalearner_AUTO_StackedEnsemble_AllModels_1_AutoML_1_20241229_51859&quot; = GLM: Generalized Linear Model DRF: Distributed Random Forest GBM: Gradient Boosting Machine DeepLearning: Neural Networks StackedEnsemble: Ensemble methods XGBoost: eXtreme Gradient Boosting (if available in your H2O installation 8.3.4 XGBoost we need to set up the gpus for this # Check if XGBoost is available h2o.xgboost.available() ## [1] &quot;Cannot build a XGboost model - no backend found.&quot; ## [1] FALSE h2o.shutdown(prompt = FALSE) "],["econometrics.html", "Chapter 9 Econometrics 9.1 A. California Test Scores 9.2 Auto Correlation", " Chapter 9 Econometrics 9.1 A. California Test Scores 9.1.1 Introduction 9.1.1.1 Purpose The Purpose of this section is display my final assignment from ECON 4403 (econometrics) at SMU. I will also be adding some other important elements of what i have learned. I will not be going crazy with a large amount of theory but i will be acknowledging some of the main issues. 9.1.1.2 The data The California Standardized Testing and Reporting (STAR) dataset contains data on test performance, school characteristics and student demographic backgrounds. The data used here are from all 420 K-6 and K-8 districts in California with data available for 1998 and 1999. Test scores are the average of the reading and math scores on the Stanford 9 standardized test administered to 5th grade students. School characteristics (averaged across the district) include enrollment, number of teachers (measured as “full-time-equivalents”), number of computers per classroom, and expenditures per student. The studentteacher ratio used here is the number of full-time equivalent teachers in the district, divided by the number of students. Demographic variables for the students also are averaged across the district. The demographic variables include the percentage of students in the public assistance program CalWorks (formerly AFDC), the percentage of students that qualify for a reduced price lunch, and the percentage of students that are English Learners (that is, students for whom English is a second language). All of these data were obtained from the California Department of Education www.cde.ca.gov. Series in Data Set: DIST_CODE: District Code; READ_SCR: Average reading Score; MATH_SCR: Average math Score; COUNTY: County; DISTRICT: District; GR_SPAN: Grade Span of District; ENRL_TOT: Total enrollment; TEACHERS: Number of teachers; COMPUTER: Number of computers; TESTSCR: Average test Score (= (READ_SCR+MATH_SCR)/2 ); COMP_STU: Computer per student ( = COMPUTER/ENRL_TOT); EXPN_STU: Expenditures per student ($’S); STR: Student teacher ratio (ENRL_TOT/TEACHERS); EL_PCT: Percent of English learners; MEAL_PCT: Percent qualifying for reduced-price lunch; 1 CALW_PCT: Percent qualifying for CALWORKS; AVGINC: District average income (in $1000’S); library(readr) cas &lt;- data.frame(read_csv(&quot;caschool.csv&quot;)) ## Rows: 420 Columns: 18 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): county, district, gr_span ## dbl (15): Observation Number, dist_cod, enrl_tot, teachers, calw_pct, meal_p... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. str(cas) ## &#39;data.frame&#39;: 420 obs. of 18 variables: ## $ Observation.Number: num 1 2 3 4 5 6 7 8 9 10 ... ## $ dist_cod : num 75119 61499 61549 61457 61523 ... ## $ county : chr &quot;Alameda&quot; &quot;Butte&quot; &quot;Butte&quot; &quot;Butte&quot; ... ## $ district : chr &quot;Sunol Glen Unified&quot; &quot;Manzanita Elementary&quot; &quot;Thermalito Union Elementary&quot; &quot;Golden Feather Union Elementary&quot; ... ## $ gr_span : chr &quot;KK-08&quot; &quot;KK-08&quot; &quot;KK-08&quot; &quot;KK-08&quot; ... ## $ enrl_tot : num 195 240 1550 243 1335 ... ## $ teachers : num 10.9 11.1 82.9 14 71.5 ... ## $ calw_pct : num 0.51 15.42 55.03 36.48 33.11 ... ## $ meal_pct : num 2.04 47.92 76.32 77.05 78.43 ... ## $ computer : num 67 101 169 85 171 25 28 66 35 0 ... ## $ testscr : num 691 661 644 648 641 ... ## $ comp_stu : num 0.344 0.421 0.109 0.35 0.128 ... ## $ expn_stu : num 6385 5099 5502 7102 5236 ... ## $ str : num 17.9 21.5 18.7 17.4 18.7 ... ## $ avginc : num 22.69 9.82 8.98 8.98 9.08 ... ## $ el_pct : num 0 4.58 30 0 13.86 ... ## $ read_scr : num 692 660 636 652 642 ... ## $ math_scr : num 690 662 651 644 640 ... So first we need to Drop some collums out of Principle. read_scr and math_scr create testscr. cas &lt;- cas[, !names(cas) %in% c(&quot;read_scr&quot;, &quot;math_scr&quot;)] Now District and Country are very specific so they will have a large number of levels. Levels for county table(cas$county) ## ## Alameda Butte Calaveras Contra Costa El Dorado ## 1 6 1 7 10 ## Fresno Glenn Humboldt Imperial Inyo ## 12 3 17 6 1 ## Kern Kings Lake Lassen Los Angeles ## 27 9 2 5 27 ## Madera Marin Mendocino Merced Monterey ## 5 8 1 11 7 ## Nevada Orange Placer Riverside Sacramento ## 9 11 11 4 7 ## San Benito San Bernardino San Diego San Joaquin San Luis Obispo ## 3 10 21 6 2 ## San Mateo Santa Barbara Santa Clara Santa Cruz Shasta ## 17 11 20 7 13 ## Siskiyou Sonoma Stanislaus Sutter Tehama ## 9 29 7 6 8 ## Trinity Tulare Tuolumne Ventura Yuba ## 2 24 6 9 2 For district almost every every observation has its own level. We will group the counties These groupings come from https://www.calbhbc.org/region-map-and-listing.html # Define the regions as vectors of county names according to your specification superior &lt;- c(&quot;Butte&quot;, &quot;Colusa&quot;, &quot;Del Norte&quot;, &quot;Glenn&quot;, &quot;Humboldt&quot;, &quot;Lake&quot;, &quot;Lassen&quot;, &quot;Mendocino&quot;, &quot;Modoc&quot;, &quot;Nevada&quot;, &quot;Plumas&quot;, &quot;Shasta&quot;, &quot;Sierra&quot;, &quot;Siskiyou&quot;, &quot;Tehama&quot;, &quot;Trinity&quot;) central &lt;- c(&quot;Alpine&quot;, &quot;Amador&quot;, &quot;Calaveras&quot;, &quot;El Dorado&quot;, &quot;Fresno&quot;, &quot;Inyo&quot;, &quot;Kings&quot;, &quot;Madera&quot;, &quot;Mariposa&quot;, &quot;Merced&quot;, &quot;Mono&quot;, &quot;Placer&quot;, &quot;Sacramento&quot;, &quot;San Joaquin&quot;, &quot;Stanislaus&quot;, &quot;Sutter&quot;, &quot;Yuba&quot;, &quot;Tulare&quot;, &quot;Tuolumne&quot;, &quot;Yolo&quot;) bay_area &lt;- c(&quot;Alameda&quot;, &quot;Contra Costa&quot;, &quot;Marin&quot;, &quot;Monterey&quot;, &quot;Napa&quot;, &quot;San Benito&quot;, &quot;San Francisco&quot;, &quot;San Mateo&quot;, &quot;Santa Clara&quot;, &quot;Santa Cruz&quot;, &quot;Solano&quot;, &quot;Sonoma&quot;, &quot;Berkeley&quot;) southern &lt;- c(&quot;Imperial&quot;, &quot;Kern&quot;, &quot;Orange&quot;, &quot;Riverside&quot;, &quot;San Bernardino&quot;, &quot;San Diego&quot;, &quot;San Luis Obispo&quot;, &quot;Santa Barbara&quot;, &quot;Ventura&quot;, &quot;Tri-City&quot;) los_angeles &lt;- c(&quot;Los Angeles&quot;) # Assuming &#39;cas&#39; is your dataframe with columns &#39;county&#39; for the county names # and &#39;observation&#39; for the counts # Create a new column for region based on the county cas$region &lt;- ifelse(cas$county %in% superior, &quot;Superior&quot;, ifelse(cas$county %in% central, &quot;Central&quot;, ifelse(cas$county %in% bay_area, &quot;Bay Area&quot;, ifelse(cas$county %in% southern, &quot;Southern&quot;, ifelse(cas$county %in% los_angeles, &quot;Los Angeles&quot;, &quot;Other&quot;))))) table(cas$region) ## ## Bay Area Central Los Angeles Southern Superior ## 99 118 27 101 75 So now we can try using this region variable. However we have other ways of measuring location so we must proceed with caution. Now we will drop district and county # using dplyr library(dplyr) cas &lt;- cas[, !names(cas) %in% c(&quot;district&quot;, &quot;county&quot;)] we can Also drop district code and observation number cas &lt;- cas[, !names(cas) %in% c(&quot;dist_cod&quot;, &quot;Observation.Number&quot;)] Now we need to one hot code the region and gr_span variables. # One-hot encoding using model.matrix cas &lt;- data.frame(cas, model.matrix(~ gr_span + region - 1, data = cas)) # now remove the original variables cas &lt;- cas[, !names(cas) %in% c(&quot;gr_span&quot;, &quot;region&quot;)] glimpse(cas) ## Rows: 420 ## Columns: 17 ## $ enrl_tot &lt;dbl&gt; 195, 240, 1550, 243, 1335, 137, 195, 888, 379, 2247,… ## $ teachers &lt;dbl&gt; 10.90, 11.15, 82.90, 14.00, 71.50, 6.40, 10.00, 42.5… ## $ calw_pct &lt;dbl&gt; 0.5102, 15.4167, 55.0323, 36.4754, 33.1086, 12.3188,… ## $ meal_pct &lt;dbl&gt; 2.0408, 47.9167, 76.3226, 77.0492, 78.4270, 86.9565,… ## $ computer &lt;dbl&gt; 67, 101, 169, 85, 171, 25, 28, 66, 35, 0, 86, 56, 25… ## $ testscr &lt;dbl&gt; 690.80, 661.20, 643.60, 647.70, 640.85, 605.55, 606.… ## $ comp_stu &lt;dbl&gt; 0.34358975, 0.42083332, 0.10903226, 0.34979424, 0.12… ## $ expn_stu &lt;dbl&gt; 6384.911, 5099.381, 5501.955, 7101.831, 5235.988, 55… ## $ str &lt;dbl&gt; 17.88991, 21.52466, 18.69723, 17.35714, 18.67133, 21… ## $ avginc &lt;dbl&gt; 22.690001, 9.824000, 8.978000, 8.978000, 9.080333, 1… ## $ el_pct &lt;dbl&gt; 0.000000, 4.583333, 30.000002, 0.000000, 13.857677, … ## $ gr_spanKK.06 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0… ## $ gr_spanKK.08 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1… ## $ regionCentral &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0… ## $ regionLos.Angeles &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ regionSouthern &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1… ## $ regionSuperior &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… Now we can move into the theory of what SHOULD be estimators of test scores. 9.1.2 Theory While i will not be going deep at all in this section i want to brush on some of the main points. Theoretically what should influence test scores? How hard the student works. If a student is the type of person to study many hours in preperation for tests we would expect this student to do much better then other students. as a note this could be increased by someones previous grades. we should look at that bias. How smart the student is. This is speaking on that if a student is naturally gifted in a subject we would expect them to do better then other students. Capital invested into a student. If a student is being given more resources then other students we would expect them to do better. i.e. if i have a computer that will help me study. The school environment of the student. If a student is in a bad school environment we would expect them to do worse then other students. i.e. if you don’t have other students also striving to achieve good grades you may not strive to achieve those grades either. The child’s family environment. If a child is in a bad family environment we would expect them to do worse then other students. i.e. if a child is in a family that does not value education they may not value education either. The native language skills of the child. If a child is not a native english speaker we would expect them to do worse then other students. 9.1.3 Building The models Now we will build the models. First we must decide if we want to build the model on a per/student basis or a totals basis. For simplicity we will build the model on a per/student basis as it would help capture some of the relationships better. So we can drop the items that are totals cas &lt;- cas[, !names(cas) %in% c(&quot;enrl_tot&quot;, &quot;teachers&quot;, &quot;computer&quot;)] So lets build our first unrestricted model UR &lt;- lm(testscr ~ ., data = cas) summary(UR) ## ## Call: ## lm(formula = testscr ~ ., data = cas) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.9628 -4.9063 -0.2445 4.8772 26.7645 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.525e+02 9.208e+00 70.860 &lt; 2e-16 *** ## calw_pct -1.181e-01 5.666e-02 -2.084 0.03779 * ## meal_pct -3.716e-01 3.587e-02 -10.358 &lt; 2e-16 *** ## comp_stu 1.130e+01 6.710e+00 1.683 0.09308 . ## expn_stu 1.157e-03 8.869e-04 1.305 0.19265 ## str 3.282e-02 2.880e-01 0.114 0.90934 ## avginc 7.369e-01 9.138e-02 8.064 8.34e-15 *** ## el_pct -1.526e-01 3.606e-02 -4.233 2.85e-05 *** ## gr_spanKK.06 3.926e+00 1.195e+00 3.284 0.00111 ** ## gr_spanKK.08 NA NA NA NA ## regionCentral 1.788e+00 1.400e+00 1.277 0.20223 ## regionLos.Angeles 2.825e-01 1.924e+00 0.147 0.88335 ## regionSouthern 1.373e+00 1.293e+00 1.062 0.28885 ## regionSuperior 6.809e+00 1.640e+00 4.152 4.02e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.147 on 407 degrees of freedom ## Multiple R-squared: 0.8224, Adjusted R-squared: 0.8172 ## F-statistic: 157.1 on 12 and 407 DF, p-value: &lt; 2.2e-16 We can also do this with linear algebra.I will be using function but i wanted to put it here to show the process and how you have to change the data when using linear algebra. casl &lt;- cas #We must satesfy the full rank condition casl$regionSuperior &lt;- NULL casl$gr_spanKK.06 &lt;- NULL X &lt;- as.matrix(casl[, names(casl) != &quot;testscr&quot;]) X &lt;- cbind(1, X) Y &lt;- cas$testscr beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y beta_hat ## [,1] ## 6.647366e+02 ## calw_pct -9.086445e-02 ## meal_pct -3.534084e-01 ## comp_stu 1.208425e+01 ## expn_stu 9.328987e-04 ## str -1.204137e-01 ## avginc 6.121253e-01 ## el_pct -2.136800e-01 ## gr_spanKK.08 -3.309759e+00 ## regionCentral -1.965127e+00 ## regionLos.Angeles -2.363906e+00 ## regionSouthern -1.180958e+00 Now we will apply our theory and use proxies to create a more restricted mode. 9.1.4 Proofs 9.1.5 Tests 9.1.6 Significances and Tests 9.1.7 Conclusion 9.2 Auto Correlation "],["parrallel-processing.html", "Chapter 10 Parrallel Processing 10.1 1. parallel with parLapply 10.2 2. foreach with doParallel 10.3 3. future with future.apply 10.4 4. purrr with furrr 10.5 5. purrr Synchronously 10.6 comparisions", " Chapter 10 Parrallel Processing compute_means &lt;- function(n) { replicate(n, mean(runif(1e7))) } 10.1 1. parallel with parLapply library(parallel) # Create a cluster cl &lt;- makeCluster(detectCores() - 1) clusterExport(cl, varlist = &quot;compute_means&quot;) start_time &lt;- Sys.time() results &lt;- parLapply(cl, 1:100, function(x) compute_means(1)) end_time &lt;- Sys.time() stopCluster(cl) time_parallel &lt;- end_time - start_time 10.2 2. foreach with doParallel i think you need to export packages in this one library(foreach) library(doParallel) cl &lt;- makeCluster(detectCores() - 1) registerDoParallel(cl) start_time &lt;- Sys.time() results &lt;- foreach(i = 1:100, .combine = &#39;c&#39;) %dopar% { compute_means(1) } end_time &lt;- Sys.time() stopCluster(cl) time_foreach &lt;- end_time - start_time 10.3 3. future with future.apply library(future) ## ## Attaching package: &#39;future&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## cluster library(future.apply) plan(multisession, workers = detectCores() - 1) options(future.rng.onMisuse = &quot;ignore&quot;, future.seed = TRUE) # for the random number generating start_time &lt;- Sys.time() results &lt;- future_lapply(1:100, function(x) compute_means(1)) end_time &lt;- Sys.time() time_future &lt;- end_time - start_time 10.4 4. purrr with furrr library(furrr) ## Warning: package &#39;furrr&#39; was built under R version 4.3.3 library(future) plan(multisession, workers = detectCores() - 1) options(future.rng.onMisuse = &quot;ignore&quot;, future.seed = TRUE) # for the random number generating start_time &lt;- Sys.time() results &lt;- future_map_dbl(1:100, ~ compute_means(1)) end_time &lt;- Sys.time() time_furrr &lt;- end_time - start_time 10.5 5. purrr Synchronously library(purrr) start_time &lt;- Sys.time() results &lt;- map_dbl(1:100, ~ compute_means(1)) end_time &lt;- Sys.time() time_purrr &lt;- end_time - start_time 10.6 comparisions with no PP start_time &lt;- Sys.time() compute_means(1) ## [1] 0.4999964 end_time &lt;- Sys.time() No_parrellel_time &lt;- end_time - start_time Obviously we see the results of overhead print(paste(&quot;Time using parallel: &quot;, time_parallel)) ## [1] &quot;Time using parallel: 4.09647011756897&quot; print(paste(&quot;Time using foreach/doParallel: &quot;, time_foreach)) ## [1] &quot;Time using foreach/doParallel: 3.6144061088562&quot; print(paste(&quot;Time using future/future.apply: &quot;, time_future)) ## [1] &quot;Time using future/future.apply: 14.2198510169983&quot; print(paste(&quot;Time using furrr/future: &quot;, time_furrr)) ## [1] &quot;Time using furrr/future: 16.1471140384674&quot; print(paste(&quot;Time using purrr: &quot;, time_purrr)) ## [1] &quot;Time using purrr: 14.2722270488739&quot; print(paste(&quot;Time using NormalFunction: &quot;, No_parrellel_time)) ## [1] &quot;Time using NormalFunction: 0.103541135787964&quot; "],["randomforestinsight.html", "Chapter 11 RandomForestInsight", " Chapter 11 RandomForestInsight # Load necessary libraries library(titanic) ## Warning: package &#39;titanic&#39; was built under R version 4.3.3 library(dplyr) library(randomForest) library(randomForestExplainer) this is the basic set up of some data # Load necessary libraries library(titanic) # Load the Titanic dataset data(&quot;titanic_train&quot;) # Create copy for cleaning data_clean &lt;- titanic_train # Handle missing values data_clean$Age[is.na(data_clean$Age)] &lt;- median(data_clean$Age, na.rm = TRUE) data_clean$Embarked[is.na(data_clean$Embarked) | data_clean$Embarked == &quot;&quot;] &lt;- &quot;S&quot; data_clean$Fare[is.na(data_clean$Fare)] &lt;- median(data_clean$Fare, na.rm = TRUE) # Extract title and create binary indicators data_clean$Title &lt;- ifelse( grepl(&quot;, Mr\\\\.&quot;, data_clean$Name), &quot;Mr&quot;, ifelse(grepl(&quot;, Mrs\\\\.&quot;, data_clean$Name), &quot;Mrs&quot;, ifelse(grepl(&quot;, Miss\\\\.&quot;, data_clean$Name), &quot;Miss&quot;, ifelse(grepl(&quot;, Master\\\\.&quot;, data_clean$Name), &quot;Master&quot;, &quot;Other&quot;))) ) # Create binary title indicators data_clean$Title_Mr &lt;- as.factor(ifelse(data_clean$Title == &quot;Mr&quot;, 1, 0)) data_clean$Title_Mrs &lt;- as.factor(ifelse(data_clean$Title == &quot;Mrs&quot;, 1, 0)) data_clean$Title_Miss &lt;- as.factor(ifelse(data_clean$Title == &quot;Miss&quot;, 1, 0)) data_clean$Title_Master &lt;- as.factor(ifelse(data_clean$Title == &quot;Master&quot;, 1, 0)) data_clean$Title_Other &lt;- as.factor(ifelse(data_clean$Title == &quot;Other&quot;, 1, 0)) # Remove unnecessary columns cols_to_keep &lt;- !(names(data_clean) %in% c(&quot;Cabin&quot;, &quot;Ticket&quot;, &quot;Name&quot;, &quot;Title&quot;)) data_clean &lt;- data_clean[, cols_to_keep] # Convert to factors data_clean$Survived &lt;- as.factor(data_clean$Survived) data_clean$Pclass &lt;- as.factor(data_clean$Pclass) data_clean$Sex &lt;- as.factor(data_clean$Sex) data_clean$Embarked &lt;- as.factor(data_clean$Embarked) # Create FamilySize data_clean$FamilySize &lt;- data_clean$SibSp + data_clean$Parch + 1 # Remove first column data_clean &lt;- data_clean[, -1] # Verify the structure and summary of the cleaned dataset str(data_clean) ## &#39;data.frame&#39;: 891 obs. of 14 variables: ## $ Survived : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 1 1 1 1 2 2 ... ## $ Pclass : Factor w/ 3 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;: 3 1 3 1 3 3 1 3 3 2 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ... ## $ Age : num 22 38 26 35 35 28 54 2 27 14 ... ## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ... ## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ... ## $ Embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ... ## $ Title_Mr : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 1 1 1 2 2 2 1 1 1 ... ## $ Title_Mrs : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 1 2 1 1 1 1 2 2 ... ## $ Title_Miss : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 2 1 1 1 1 1 1 1 ... ## $ Title_Master: Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 2 1 1 ... ## $ Title_Other : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ FamilySize : num 2 2 1 2 1 1 1 5 3 2 ... summary(data_clean) ## Survived Pclass Sex Age SibSp Parch ## 0:549 1:216 female:314 Min. : 0.42 Min. :0.000 Min. :0.0000 ## 1:342 2:184 male :577 1st Qu.:22.00 1st Qu.:0.000 1st Qu.:0.0000 ## 3:491 Median :28.00 Median :0.000 Median :0.0000 ## Mean :29.36 Mean :0.523 Mean :0.3816 ## 3rd Qu.:35.00 3rd Qu.:1.000 3rd Qu.:0.0000 ## Max. :80.00 Max. :8.000 Max. :6.0000 ## Fare Embarked Title_Mr Title_Mrs Title_Miss Title_Master ## Min. : 0.00 C:168 0:374 0:766 0:709 0:851 ## 1st Qu.: 7.91 Q: 77 1:517 1:125 1:182 1: 40 ## Median : 14.45 S:646 ## Mean : 32.20 ## 3rd Qu.: 31.00 ## Max. :512.33 ## Title_Other FamilySize ## 0:864 Min. : 1.000 ## 1: 27 1st Qu.: 1.000 ## Median : 1.000 ## Mean : 1.905 ## 3rd Qu.: 2.000 ## Max. :11.000 Now we will use randomforest to predict the survival of the passengers and use random forest explainer to understand it more This is out titanic data that was cleaned up str(data) ## tibble [10,000 × 7] (S3: tbl_df/tbl/data.frame) ## $ y : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ type : Factor w/ 3 levels &quot;H&quot;,&quot;L&quot;,&quot;M&quot;: 3 2 2 2 2 3 2 2 3 3 ... ## $ air_temperature_k : num [1:10000] 298 298 298 298 298 ... ## $ process_temperature_k: num [1:10000] 309 309 308 309 309 ... ## $ rotational_speed_rpm : num [1:10000] 1551 1408 1498 1433 1408 ... ## $ torque_nm : num [1:10000] 42.8 46.3 49.4 39.5 40 41.9 42.4 40.2 28.6 28 ... ## $ tool_wear_min : num [1:10000] 0 3 5 7 9 11 14 16 18 21 ... MDI MDA PDPs "],["sequential-data-embedding.html", "Chapter 12 Sequential Data Embedding", " Chapter 12 Sequential Data Embedding With time series data we cannot shuffle the data due to the temporal nature of the data. This means we cannot shuffle the data. While models like ARIMA do exist they are not strong enough for forcasting. we want to get our data to the point where we can use our machine learning models on it. first lets make a toy dataset to work with. it will have a y and one X Since we set rho to be less then 1 our fake data is stationary. If rho is = 1 then we have non-stationary data and must decompose it to get to this position. You can take the first differance in that case. # Stationary data rho &lt; 1 n &lt;- 100 rho_y &lt;- 0.80 rho_x &lt;- 0.75 # A different rho for X y &lt;- numeric(n) x &lt;- numeric(n) e_y &lt;- rnorm(n, 0, 1) # White noise for y e_x &lt;- rnorm(n, 0, 1) # White noise for X # Generate y and X as separate autoregressive processes for (j in 1:(n - 1)) { y[j + 1] &lt;- y[j] * rho_y + e_y[j] x[j + 1] &lt;- x[j] * rho_x + e_x[j] } ylagged &lt;- y[2:n] xlagged &lt;- x[2:n] # y over time plot(y[1:n], type = &quot;l&quot;, col = &quot;red&quot;, ylab = &quot;y&quot;, xlab = &quot;t&quot;, main = &quot;y over time&quot;) # x over time plot(x[1:n], type = &quot;l&quot;, col = &quot;blue&quot;, ylab = &quot;x&quot;, xlab = &quot;t&quot;, main = &quot;x over time&quot;) So we begin by predicting y + 1 but we must first prove we can embed and shuffle the data we will use 2 points back of y and three points back of x to predict the next point of y. y2 &lt;- 1:10 x2 &lt;- 11:20 # Create the embedded matrix for Y and X y_emb &lt;- embed(y2, 4) x_emb &lt;- embed(x2, 4) colnames(y_emb) &lt;- c(&quot;Y(t)&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;, &quot;Y(t-3)&quot;) colnames(x_emb) &lt;- c(&quot;X(t)&quot;, &quot;X(t-1)&quot;, &quot;X(t-2)&quot;, &quot;X(t-3)&quot;) fd &lt;- cbind(y_emb, x_emb) head(fd) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 4 3 2 1 14 13 12 11 ## [2,] 5 4 3 2 15 14 13 12 ## [3,] 6 5 4 3 16 15 14 13 ## [4,] 7 6 5 4 17 16 15 14 ## [5,] 8 7 6 5 18 17 16 15 ## [6,] 9 8 7 6 19 18 17 16 now for each point we are predicting(y + 1), (y+2), and (y+3) we need to edit the data for predicting point y(t+1) the formula is \\[ \\hat{y}(t+1) = f\\left( y(t), y(t-1), y(t-2), x(t), x(t-1), x(t-2), x(t-3) \\right) \\] and the data is head(fd) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 4 3 2 1 14 13 12 11 ## [2,] 5 4 3 2 15 14 13 12 ## [3,] 6 5 4 3 16 15 14 13 ## [4,] 7 6 5 4 17 16 15 14 ## [5,] 8 7 6 5 18 17 16 15 ## [6,] 9 8 7 6 19 18 17 16 in this case row seven is y where t = 10… 10(t) - 3(lags) = 7(rows) for the seccond point we are predicting y(t+2) the formula is \\[ \\hat{y}(t+2) = f\\left( y(t), y(t-1), y(t-2), x(t), x(t-1), x(t-2), x(t-3) \\right) \\] and the data is # Shift the first column (Y(t)) up by one position sd &lt;- fd[-nrow(fd), ] # Remove the last row sd[, 1] &lt;- fd[-1, 1] # Shift the first column up by one # Display the result to verify head(sd) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 5 3 2 1 14 13 12 11 ## [2,] 6 4 3 2 15 14 13 12 ## [3,] 7 5 4 3 16 15 14 13 ## [4,] 8 6 5 4 17 16 15 14 ## [5,] 9 7 6 5 18 17 16 15 ## [6,] 10 8 7 6 19 18 17 16 \\[ \\hat{y}(t+3) = f\\left( y(t), y(t-1), y(t-2), x(t), x(t-1), x(t-2), x(t-3) \\right) \\] # Shift the first column (Y(t+2)) up by one position to prepare for y(t+3) td &lt;- sd[-nrow(sd), ] # Remove the last row td[, 1] &lt;- sd[-1, 1] # Shift the first column up by one # Display the result to verify head(td) ## Y(t) Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) X(t-2) X(t-3) ## [1,] 6 3 2 1 14 13 12 11 ## [2,] 7 4 3 2 15 14 13 12 ## [3,] 8 5 4 3 16 15 14 13 ## [4,] 9 6 5 4 17 16 15 14 ## [5,] 10 7 6 5 18 17 16 15 now we will aplly this to our data and prove that we can shuffle the data y_emb &lt;- embed(y, 4) x_emb &lt;- embed(x, 4) colnames(y_emb) &lt;- c(&quot;Y(t)&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;, &quot;Y(t-3)&quot;) colnames(x_emb) &lt;- c(&quot;X(t)&quot;, &quot;X(t-1)&quot;, &quot;X(t-2)&quot;, &quot;X(t-3)&quot;) They each lose 3 rows at the end of the data set. now get the data set for predicting y(t+1) same as before now y(t+2) and y(t+3) # First, embed the original data for y and x y_emb &lt;- embed(y, 4) x_emb &lt;- embed(x, 4) colnames(y_emb) &lt;- c(&quot;Yt&quot;, &quot;Y(t-1)&quot;, &quot;Y(t-2)&quot;, &quot;Y(t-3)&quot;) colnames(x_emb) &lt;- c(&quot;X(t)&quot;, &quot;X(t-1)&quot;, &quot;X(t-2)&quot;, &quot;X(t-3)&quot;) fd &lt;- cbind(y_emb, x_emb) # Shift the first column (Y(t)) up by one position for y(t+2) sd &lt;- fd[-nrow(fd), ] # Remove the last row sd[, 1] &lt;- fd[-1, 1] # Shift the first column up by one # Display the resulting dataset head(sd) ## Yt Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) ## [1,] 2.1325385 -0.7310319 -0.1930280 0.0000000 1.3300658 -0.5545853 ## [2,] 1.7935305 0.8271701 -0.7310319 -0.1930280 1.3796607 1.3300658 ## [3,] 1.6220635 2.1325385 0.8271701 -0.7310319 0.1782876 1.3796607 ## [4,] 1.0882613 1.7935305 2.1325385 0.8271701 -1.6826677 0.1782876 ## [5,] 0.3569833 1.6220635 1.7935305 2.1325385 0.1501499 -1.6826677 ## [6,] 1.2937136 1.0882613 1.6220635 1.7935305 0.5595134 0.1501499 ## X(t-2) X(t-3) ## [1,] -0.5105266 0.0000000 ## [2,] -0.5545853 -0.5105266 ## [3,] 1.3300658 -0.5545853 ## [4,] 1.3796607 1.3300658 ## [5,] 0.1782876 1.3796607 ## [6,] -1.6826677 0.1782876 # Shift the first column (Y(t+2)) up by one position to prepare for y(t+3) td &lt;- sd[-nrow(sd), ] # Remove the last row td[, 1] &lt;- sd[-1, 1] # Shift the first column up by one # Display the resulting dataset head(td) ## Yt Y(t-1) Y(t-2) Y(t-3) X(t) X(t-1) ## [1,] 1.7935305 -0.7310319 -0.1930280 0.0000000 1.3300658 -0.5545853 ## [2,] 1.6220635 0.8271701 -0.7310319 -0.1930280 1.3796607 1.3300658 ## [3,] 1.0882613 2.1325385 0.8271701 -0.7310319 0.1782876 1.3796607 ## [4,] 0.3569833 1.7935305 2.1325385 0.8271701 -1.6826677 0.1782876 ## [5,] 1.2937136 1.6220635 1.7935305 2.1325385 0.1501499 -1.6826677 ## [6,] 0.9607247 1.0882613 1.6220635 1.7935305 0.5595134 0.1501499 ## X(t-2) X(t-3) ## [1,] -0.5105266 0.0000000 ## [2,] -0.5545853 -0.5105266 ## [3,] 1.3300658 -0.5545853 ## [4,] 1.3796607 1.3300658 ## [5,] 0.1782876 1.3796607 ## [6,] -1.6826677 0.1782876 apply lm to each of the data sets then shuffle and check the coeficients # Fit a linear model to predict Y(t+1) fd &lt;- as.data.frame(fd) sd &lt;- as.data.frame(sd) td &lt;- as.data.frame(td) lm1 &lt;- lm(Yt ~ ., data = fd) lm2 &lt;- lm(Yt ~ ., data = sd) lm3 &lt;- lm(Yt ~ ., data = td) summary(lm1) ## ## Call: ## lm(formula = Yt ~ ., data = fd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.29967 -0.69150 -0.08307 0.75536 2.22483 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.029714 0.101257 -0.293 0.770 ## `Y(t-1)` 0.927013 0.104889 8.838 8.13e-14 *** ## `Y(t-2)` -0.025750 0.144336 -0.178 0.859 ## `Y(t-3)` -0.132191 0.111152 -1.189 0.237 ## `X(t)` 0.002915 0.095495 0.031 0.976 ## `X(t-1)` 0.061283 0.126142 0.486 0.628 ## `X(t-2)` 0.106785 0.126501 0.844 0.401 ## `X(t-3)` -0.065786 0.099501 -0.661 0.510 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9727 on 89 degrees of freedom ## Multiple R-squared: 0.6907, Adjusted R-squared: 0.6664 ## F-statistic: 28.4 on 7 and 89 DF, p-value: &lt; 2.2e-16 summary(lm2) ## ## Call: ## lm(formula = Yt ~ ., data = sd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6425 -0.8101 -0.0132 0.7738 2.9900 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07042 0.13845 -0.509 0.612 ## `Y(t-1)` 0.82510 0.14640 5.636 2.07e-07 *** ## `Y(t-2)` -0.19770 0.19730 -1.002 0.319 ## `Y(t-3)` -0.07978 0.15235 -0.524 0.602 ## `X(t)` 0.06221 0.13101 0.475 0.636 ## `X(t-1)` 0.18076 0.17281 1.046 0.298 ## `X(t-2)` -0.09900 0.17463 -0.567 0.572 ## `X(t-3)` 0.08903 0.13612 0.654 0.515 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.329 on 88 degrees of freedom ## Multiple R-squared: 0.4268, Adjusted R-squared: 0.3812 ## F-statistic: 9.36 on 7 and 88 DF, p-value: 1.27e-08 summary(lm3) ## ## Call: ## lm(formula = Yt ~ ., data = td) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9904 -0.7683 0.0629 0.9009 3.7052 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.11952 0.15962 -0.749 0.45602 ## `Y(t-1)` 0.52220 0.17056 3.062 0.00293 ** ## `Y(t-2)` -0.07831 0.22734 -0.344 0.73131 ## `Y(t-3)` -0.15597 0.17542 -0.889 0.37639 ## `X(t)` 0.23537 0.15087 1.560 0.12238 ## `X(t-1)` -0.06588 0.20121 -0.327 0.74415 ## `X(t-2)` 0.07253 0.20110 0.361 0.71921 ## `X(t-3)` 0.06070 0.15704 0.387 0.70004 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.531 on 87 degrees of freedom ## Multiple R-squared: 0.233, Adjusted R-squared: 0.1713 ## F-statistic: 3.776 on 7 and 87 DF, p-value: 0.001275 # Shuffle the data fd &lt;- fd[sample(nrow(fd)), ] sd &lt;- sd[sample(nrow(sd)), ] td &lt;- td[sample(nrow(td)), ] lm1 &lt;- lm(Yt ~ ., data = fd) lm2 &lt;- lm(Yt ~ ., data = sd) lm3 &lt;- lm(Yt ~ ., data = td) summary(lm1) ## ## Call: ## lm(formula = Yt ~ ., data = fd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.29967 -0.69150 -0.08307 0.75536 2.22483 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.029714 0.101257 -0.293 0.770 ## `Y(t-1)` 0.927013 0.104889 8.838 8.13e-14 *** ## `Y(t-2)` -0.025750 0.144336 -0.178 0.859 ## `Y(t-3)` -0.132191 0.111152 -1.189 0.237 ## `X(t)` 0.002915 0.095495 0.031 0.976 ## `X(t-1)` 0.061283 0.126142 0.486 0.628 ## `X(t-2)` 0.106785 0.126501 0.844 0.401 ## `X(t-3)` -0.065786 0.099501 -0.661 0.510 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9727 on 89 degrees of freedom ## Multiple R-squared: 0.6907, Adjusted R-squared: 0.6664 ## F-statistic: 28.4 on 7 and 89 DF, p-value: &lt; 2.2e-16 summary(lm2) ## ## Call: ## lm(formula = Yt ~ ., data = sd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6425 -0.8101 -0.0132 0.7738 2.9900 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07042 0.13845 -0.509 0.612 ## `Y(t-1)` 0.82510 0.14640 5.636 2.07e-07 *** ## `Y(t-2)` -0.19770 0.19730 -1.002 0.319 ## `Y(t-3)` -0.07978 0.15235 -0.524 0.602 ## `X(t)` 0.06221 0.13101 0.475 0.636 ## `X(t-1)` 0.18076 0.17281 1.046 0.298 ## `X(t-2)` -0.09900 0.17463 -0.567 0.572 ## `X(t-3)` 0.08903 0.13612 0.654 0.515 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.329 on 88 degrees of freedom ## Multiple R-squared: 0.4268, Adjusted R-squared: 0.3812 ## F-statistic: 9.36 on 7 and 88 DF, p-value: 1.27e-08 summary(lm3) ## ## Call: ## lm(formula = Yt ~ ., data = td) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9904 -0.7683 0.0629 0.9009 3.7052 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.11952 0.15962 -0.749 0.45602 ## `Y(t-1)` 0.52220 0.17056 3.062 0.00293 ** ## `Y(t-2)` -0.07831 0.22734 -0.344 0.73131 ## `Y(t-3)` -0.15597 0.17542 -0.889 0.37639 ## `X(t)` 0.23537 0.15087 1.560 0.12238 ## `X(t-1)` -0.06588 0.20121 -0.327 0.74415 ## `X(t-2)` 0.07253 0.20110 0.361 0.71921 ## `X(t-3)` 0.06070 0.15704 0.387 0.70004 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.531 on 87 degrees of freedom ## Multiple R-squared: 0.233, Adjusted R-squared: 0.1713 ## F-statistic: 3.776 on 7 and 87 DF, p-value: 0.001275 Now at this point we can use any machine learning model we want to predict the next three points in the time series. "],["hcc_survey_analysis.html", "Chapter 13 HCC_Survey_Analysis 13.1 Our data and Purpose 13.2 Exploratory Data Analysis 13.3 Modeling Random Forest 13.4 Global Importance 13.5 Local Importance 13.6 Partial Dependence Plots 13.7 Random Forest Explainer 13.8 Final Notes", " Chapter 13 HCC_Survey_Analysis 13.1 Our data and Purpose Load our libraries that we will use library(readr) library(dplyr) library(DataExplorer) library(tidyr) library(ggplot2) library(caret) library(randomForest) library(randomForestExplainer) library(pdp) ## Warning: package &#39;pdp&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;pdp&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## partial # Read and clean column names HCC_Survey &lt;- read_csv(&quot;HCC_Survey.csv&quot;) ## Rows: 46 Columns: 83 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (8): Time, Location, Campus, Involvment, Faith, Formation_Helpful, Eff_... ## dbl (75): cg_SMU, cg_Summit, cg_Other, cg_Dal, cg_Social, cg_Parish, cg_MW, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. names(HCC_Survey) &lt;- gsub(&quot; &quot;, &quot;_&quot;, names(HCC_Survey)) names(HCC_Survey) &lt;- gsub(&quot;&#39;&quot;, &quot;&quot;, names(HCC_Survey)) names(HCC_Survey) &lt;- gsub(&quot;[()]&quot;, &quot;&quot;, names(HCC_Survey)) names(HCC_Survey) &lt;- make.names(names(HCC_Survey), unique = TRUE) # Select initial columns cols_to_keep &lt;- c(&quot;Involvment&quot;, &quot;Relations&quot;, grep(&quot;^cg_|^dsc_|^frm_|^srv_&quot;, names(HCC_Survey), value = TRUE)) HCC_Survey &lt;- HCC_Survey[, cols_to_keep] # Create Total_cgs cg_cols &lt;- grep(&quot;^cg_&quot;, names(HCC_Survey), value = TRUE) HCC_Survey$Total_cgs &lt;- rowSums(HCC_Survey[, cg_cols], na.rm = TRUE) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% cg_cols] # Create CCO cco_cols &lt;- grep(&quot;^frm_CCO&quot;, names(HCC_Survey), value = TRUE) HCC_Survey$CCO &lt;- ifelse(rowSums(HCC_Survey[, cco_cols], na.rm = TRUE) &gt; 0, 1, 0) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% cco_cols] # Create srv_CCO srv_cco_cols &lt;- c(&quot;srv_Leading_a_faith_study&quot;, &quot;srv_CCO_Exec&quot;, &quot;srv_CCO_Events&quot;) HCC_Survey$srv_CCO &lt;- ifelse(rowSums(HCC_Survey[, srv_cco_cols], na.rm = TRUE) &gt; 0, 1, 0) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% srv_cco_cols] # Remove specified columns and rename cols_to_remove &lt;- c(&quot;dsc_Other&quot;, &quot;srv_Other_please_indicate&quot;, &quot;srv_I_didnt_serve_in_ministry&quot;) HCC_Survey &lt;- HCC_Survey[, !names(HCC_Survey) %in% cols_to_remove] names(HCC_Survey)[names(HCC_Survey) == &quot;srv_HCC_Volunteering_ex._CLT&quot;] &lt;- &quot;srv_HCC&quot; # Create Total_dsc and Total_frm dsc_cols &lt;- grep(&quot;^dsc_&quot;, names(HCC_Survey), value = TRUE) frm_cols &lt;- grep(&quot;^frm_&quot;, names(HCC_Survey), value = TRUE) data &lt;- HCC_Survey data$Total_dsc &lt;- rowSums(data[, dsc_cols], na.rm = TRUE) data$Total_frm &lt;- rowSums(data[, frm_cols], na.rm = TRUE) data &lt;- data[, !names(data) %in% c(dsc_cols, frm_cols)] # Convert factors and combine Impressions into Casual data$Involvment &lt;- factor(replace(data$Involvment, data$Involvment == &quot;Impressions&quot;, &quot;Casual&quot;)) data$Relations &lt;- factor(data$Relations) # Sort columns alphabetically data &lt;- data[, sort(names(data))] # Display structure and create data_rf glimpse(data) ## Rows: 46 ## Columns: 9 ## $ CCO &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,… ## $ Involvment &lt;fct&gt; Casual, Casual, Casual, Casual, Committed, Engaged… ## $ Relations &lt;fct&gt; 3 to 6, 3 to 6, 3 to 6, 3 to 6, 3 to 6, More than … ## $ srv_CCO &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,… ## $ srv_HCC &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ srv_Parish_ministry &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,… ## $ Total_cgs &lt;dbl&gt; 3, 3, 4, 4, 6, 5, 4, 4, 1, 1, 3, 3, 4, 4, 5, 4, 6,… ## $ Total_dsc &lt;dbl&gt; 7, 3, 9, 5, 9, 5, 4, 9, 5, 10, 10, 6, 4, 6, 10, 10… ## $ Total_frm &lt;dbl&gt; 4, 1, 0, 0, 3, 1, 3, 4, 0, 6, 3, 1, 2, 2, 2, 3, 6,… data_rf &lt;- data write the data then re read it write.csv(data, &quot;proccessed_surveyHCC.csv&quot;) The following data has been already pre-proccessed data &lt;- read.csv(&quot;proccessed_surveyHCC.csv&quot;) glimpse(data) ## Rows: 46 ## Columns: 10 ## $ X &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,… ## $ CCO &lt;int&gt; 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,… ## $ Involvment &lt;chr&gt; &quot;Casual&quot;, &quot;Casual&quot;, &quot;Casual&quot;, &quot;Casual&quot;, &quot;Committed… ## $ Relations &lt;chr&gt; &quot;3 to 6&quot;, &quot;3 to 6&quot;, &quot;3 to 6&quot;, &quot;3 to 6&quot;, &quot;3 to 6&quot;, … ## $ srv_CCO &lt;int&gt; 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,… ## $ srv_HCC &lt;int&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ srv_Parish_ministry &lt;int&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,… ## $ Total_cgs &lt;int&gt; 3, 3, 4, 4, 6, 5, 4, 4, 1, 1, 3, 3, 4, 4, 5, 4, 6,… ## $ Total_dsc &lt;int&gt; 7, 3, 9, 5, 9, 5, 4, 9, 5, 10, 10, 6, 4, 6, 10, 10… ## $ Total_frm &lt;int&gt; 4, 1, 0, 0, 3, 1, 3, 4, 0, 6, 3, 1, 2, 2, 2, 3, 6,… CCO: whether someone has taken a CCO fairht study or not Involvment: this is categorical and represents the level of involvement of the person Relations: This counts the number of people a individual identifies as having a close relationship with currently srv_CCO: This is a binary variable that represents whether someone has served in a CCO ministry or not srv_HCC: This is a binary variable that represents whether someone has served in a HCC ministry or not srv_Parish_ministry: This is a binary variable that represents whether someone has served in a Parish ministry or not Total_cgs: This is the total number of community groups a person is apart of. Total_dsc: This is the total number of spiritual disciplines a person has in their life currently Total_frm: This is the total number of formations that a person has attended via HCC. This is our data we will use 13.2 Exploratory Data Analysis # First read the data and convert categorical variables to factors data &lt;- read.csv(&quot;proccessed_surveyHCC.csv&quot;, stringsAsFactors = FALSE) data$Involvment &lt;- as.factor(data$Involvment) data$Relations &lt;- as.factor(data$Relations) # Create the model matrix for factors only factor_cols &lt;- sapply(data, is.factor) factor_data &lt;- data[, factor_cols, drop = FALSE] dummy_vars &lt;- model.matrix(~., data = factor_data)[, -1] # Remove intercept # Combine the dummy variables with the non-factor columns numeric_cols &lt;- sapply(data, is.numeric) data_encoded &lt;- cbind( as.data.frame(dummy_vars), data[, numeric_cols, drop = FALSE] ) # Make sure Total_dsc is included if(!&quot;Total_dsc&quot; %in% names(data_encoded)) { data_encoded$Total_dsc &lt;- data$Total_dsc } # Calculate correlations cols_for_cor &lt;- setdiff(names(data_encoded), &quot;Total_dsc&quot;) correlations_list &lt;- sapply(data_encoded[cols_for_cor], function(x) { cor(x, data_encoded$Total_dsc, use = &quot;complete.obs&quot;) }) # Create correlation data frame correlations &lt;- data.frame( column = names(correlations_list), correlation = unlist(correlations_list) ) # Sort by absolute correlation correlations &lt;- correlations[order(abs(correlations$correlation), decreasing = TRUE), ] rownames(correlations) &lt;- NULL # If you want to keep the ggplot visualization: library(ggplot2) ggplot(correlations, aes(x = reorder(column, correlation), y = correlation)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) + coord_flip() + labs(title = &quot;Correlation of All Columns with Total Disciplines&quot;, x = &quot;Column&quot;, y = &quot;Correlation&quot;) + theme_minimal() # Print correlations print(correlations) ## column correlation ## 1 Total_frm 0.62062501 ## 2 srv_Parish_ministry 0.59772804 ## 3 Total_cgs 0.47432114 ## 4 InvolvmentCommitted 0.44987501 ## 5 srv_HCC 0.44519710 ## 6 srv_CCO 0.41644453 ## 7 Relations7 to 10 0.24458238 ## 8 CCO 0.21960995 ## 9 Relations3 to 6 -0.16737169 ## 10 RelationsMore than 10 0.08832141 ## 11 X -0.05220109 ## 12 InvolvmentEngaged -0.01372312 13.3 Modeling Random Forest # Train the Random Forest model with 1500 trees rf_model &lt;- randomForest(Total_dsc ~ ., data = data_rf, ntree = 1500, importance = TRUE, localImp = TRUE) # Print the model summary print(rf_model) ## ## Call: ## randomForest(formula = Total_dsc ~ ., data = data_rf, ntree = 1500, importance = TRUE, localImp = TRUE) ## Type of random forest: regression ## Number of trees: 1500 ## No. of variables tried at each split: 2 ## ## Mean of squared residuals: 5.227071 ## % Var explained: 47.93 # Calculate the RMSE rmse &lt;- sqrt(mean((rf_model$predicted - data_rf$Total_dsc)^2)) rmse ## [1] 2.286279 # absolute error abs_error &lt;- mean(abs(rf_model$predicted - data_rf$Total_dsc)) abs_error ## [1] 1.836127 13.4 Global Importance Get MDI # Get the Mean Decrease in Impurity (MDI) mdi &lt;- importance(rf_model, type = 2) mdi ## IncNodePurity ## CCO 15.30687 ## Involvment 37.28179 ## Relations 34.91613 ## srv_CCO 26.94533 ## srv_HCC 24.26830 ## srv_Parish_ministry 73.51619 ## Total_cgs 53.47827 ## Total_frm 94.59487 get MDA # Get the Mean Decrease in Accuracy (MDA) mda &lt;- importance(rf_model, type = 1) mda ## %IncMSE ## CCO 6.6776629 ## Involvment 10.6010665 ## Relations 0.7501642 ## srv_CCO 12.0691105 ## srv_HCC 9.8546560 ## srv_Parish_ministry 23.7100450 ## Total_cgs 10.4084548 ## Total_frm 23.1308455 Plot Mda and mdi # Get MDI (Mean Decrease in Node Impurity) mdi &lt;- importance(rf_model, type = 2) # Get MDA (Mean Decrease in Accuracy) mda &lt;- importance(rf_model, type = 1) # Convert the importance values to a data frame for easier plotting importance_df &lt;- as.data.frame(mdi) importance_df$Feature &lt;- rownames(importance_df) importance_df$MDA &lt;- mda[, 1] # Rename the MDI column appropriately colnames(importance_df)[1] &lt;- &quot;IncNodePurity&quot; # Sort the data frames by MDA and MDI mda_df &lt;- importance_df %&gt;% arrange(desc(MDA)) mdi_df &lt;- importance_df %&gt;% arrange(desc(IncNodePurity)) # Plotting MDA ggplot(mda_df, aes(x = reorder(Feature, MDA), y = MDA)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;skyblue&quot;) + coord_flip() + labs(title = &quot;Feature Importance: Mean Decrease in Accuracy (MDA)&quot;, x = &quot;Feature&quot;, y = &quot;Mean Decrease in Accuracy&quot;) + theme_minimal() # Plotting MDI ggplot(mdi_df, aes(x = reorder(Feature, IncNodePurity), y = IncNodePurity)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;salmon&quot;) + coord_flip() + labs(title = &quot;Feature Importance: Mean Decrease in Node Impurity (MDI)&quot;, x = &quot;Feature&quot;, y = &quot;Mean Decrease in Node Impurity&quot;) + theme_minimal() MDA doesn’t rely on the internal structure of the model but rather on the model’s performance with altered data. MDI computes importance scores based on how much each feature contributes to homogeneity in nodes across all trees. For classification, this is often measured by the Gini impurity, and for regression, it can be measured by the variance reduction. 13.5 Local Importance # Extract the local importance local_importance &lt;- rf_model$localImp # View the dimensions of local_importance to understand its structure dim(local_importance) ## [1] 8 46 8 features and 46 observations library(ggplot2) library(reshape2) ## ## Attaching package: &#39;reshape2&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## smiths # Assuming local_importance is a matrix or data frame with observations as columns and features as rows local_importance_df &lt;- as.data.frame(local_importance) local_importance_df$Feature &lt;- rownames(local_importance_df) local_importance_melted &lt;- melt(local_importance_df, id.vars = &quot;Feature&quot;) ggplot(local_importance_melted, aes(x = variable, y = Feature, fill = value)) + geom_tile() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;blue&quot;) + labs(title = &quot;Heatmap of Local Importance&quot;, x = &quot;Observation&quot;, y = &quot;Feature&quot;) + theme_minimal() ggplot(local_importance_melted, aes(x = Feature, y = value)) + geom_violin(fill = &quot;lightgreen&quot;) + coord_flip() + labs(title = &quot;Violin Plot of Local Importance by Feature&quot;, x = &quot;Feature&quot;, y = &quot;Local Importance&quot;) + theme_minimal() avg_importance &lt;- rowMeans(local_importance) ggplot(data.frame(Feature = names(avg_importance), AvgImportance = avg_importance), aes(x = reorder(Feature, AvgImportance), y = AvgImportance)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;orange&quot;) + coord_flip() + labs(title = &quot;Average Local Importance by Feature&quot;, x = &quot;Feature&quot;, y = &quot;Average Local Importance&quot;) + theme_minimal() tfi &lt;- local_importance[&quot;Total_frm&quot;, ] tfv &lt;- data_rf$Total_frm # Create a data frame for plotting importance_frm_df &lt;- data.frame(Total_frm = tfv, Total_frmImportance = tfi) ggplot(importance_frm_df, aes(x = Total_frm, y = Total_frmImportance)) + geom_point(alpha = 0.5) + geom_smooth() + theme_minimal() + labs(title = &quot;Local Importance of &#39;Total_frm&#39;&quot;, x = &quot;Total_frm&quot;, y = &quot;Local Importance of Total_frm&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula = &#39;y ~ x&#39; # Extract local importance for &#39;srv_Parish_ministry&#39; spi &lt;- local_importance[&quot;srv_Parish_ministry&quot;, ] spv &lt;- data_rf$srv_Parish_ministry # Create a data frame for plotting imp_df &lt;- data.frame(SPM = spv, Imp = spi) # Plot the violin plot ggplot(imp_df, aes(x = factor(SPM), y = Imp)) + geom_violin(fill = &quot;lightgreen&quot;) + theme_minimal() + labs(title = &quot;Local Importance by &#39;srv_Parish_ministry&#39; Status&quot;, x = &quot;srv_Parish_ministry (0 or 1)&quot;, y = &quot;Local Importance&quot;) # Plot the jitter plot with boxplot overlay ggplot(imp_df, aes(x = factor(SPM), y = Imp)) + geom_jitter(width = 0.2, alpha = 0.5) + geom_boxplot(outlier.shape = NA, fill = &quot;lightblue&quot;, alpha = 0.3) + theme_minimal() + labs(title = &quot;Local Importance by &#39;srv_Parish_ministry&#39; Status&quot;, x = &quot;srv_Parish_ministry (0 or 1)&quot;, y = &quot;Local Importance&quot;) 13.6 Partial Dependence Plots Partial Dependence Plots (PDPs): PDPs help you understand the relationship between a feature (or features) and the target variable in a machine learning model, such as a Random Forest. Specifically, they show how the predicted outcome varies with changes in a particular feature, while averaging out the effects of all other features in the model. Single-Feature PDP: For a single feature, the PDP shows the marginal effect of that feature on the predicted outcome. How It Works: The model’s predictions are averaged over different values of the feature of interest, holding all other features constant. This allows you to see whether the relationship between the feature and the target is linear, monotonic, or more complex. pdp_total_frm &lt;- partial(rf_model, pred.var = &quot;Total_frm&quot;) plot(pdp_total_frm, type = &quot;l&quot;, main = &quot;Partial Dependence of Total_frm&quot;, xlab = &quot;Total_frm&quot;, ylab = &quot;Predicted Total_dsc&quot;) # Generate PDP data for the binary variable &#39;srv_Parish_ministry&#39; pdp_srv_parish &lt;- partial(rf_model, pred.var = &quot;srv_Parish_ministry&quot;, plot = FALSE) # Convert the result to a data frame pdp_df &lt;- as.data.frame(pdp_srv_parish) # Calculate the difference between the two bars difference &lt;- round(diff(pdp_df$yhat), 2) ggplot(pdp_df, aes(x = factor(srv_Parish_ministry), y = yhat)) + geom_bar(stat = &quot;identity&quot;, aes(fill = factor(srv_Parish_ministry)), width = 0.4) + scale_fill_manual(values = c(&quot;0&quot; = &quot;red&quot;, &quot;1&quot; = &quot;green&quot;)) + # Custom colors for the bars geom_text(aes(label = round(yhat, 2)), vjust = -0.5, size = 5) + # Display the exact values on top of the bars expand_limits(y = max(pdp_df$yhat) * 1.2) + # Expand y-axis limits slightly annotate(&quot;text&quot;, x = 1.5, y = max(pdp_df$yhat) * 1.1, label = paste(&quot;Difference of Serving\\nin Parish Ministry: &quot;, difference), size = 4, color = &quot;black&quot;) + # Smaller size and stacked text theme_minimal(base_size = 12) + theme(panel.background = element_rect(fill = &quot;grey&quot;, color = NA), # Grey background plot.background = element_rect(fill = &quot;grey&quot;, color = NA), legend.position = &quot;none&quot;) + # Remove the legend labs(title = &quot;Partial Dependence of srv_Parish_ministry&quot;, x = &quot;srv_Parish_ministry (0 or 1)&quot;, y = &quot;Predicted Total_dsc&quot;) 13.7 Random Forest Explainer # Extract the minimum depth distribution of variables min_depth_frame &lt;- min_depth_distribution(rf_model) # Measure variable importance impf &lt;- measure_importance(rf_model) impf ## variable mean_min_depth no_of_nodes mse_increase ## 1 CCO 2.978911 1299 0.24503666 ## 2 Involvment 2.175398 2149 0.58770740 ## 3 Relations 2.211549 2813 0.03856566 ## 4 srv_CCO 2.620511 1590 0.63170657 ## 5 srv_HCC 2.915309 1139 0.50335558 ## 6 srv_Parish_ministry 1.761408 1905 1.80269000 ## 7 Total_cgs 1.813915 3180 0.66647065 ## 8 Total_frm 1.475575 3295 2.00387879 ## node_purity_increase no_of_trees times_a_root p_value ## 1 15.30687 1014 29 1.000000e+00 ## 2 37.28179 1269 203 6.984167e-01 ## 3 34.91613 1330 87 7.703550e-46 ## 4 26.94533 1168 146 1.000000e+00 ## 5 24.26830 947 151 1.000000e+00 ## 6 73.51619 1332 317 1.000000e+00 ## 7 53.47827 1377 212 3.767204e-106 ## 8 94.59487 1392 355 9.037298e-130 plot_multi_way_importance(impf, x_measure = &quot;mean_min_depth&quot;, y_measure = &quot;node_purity_increase&quot;, #its regressional so not gini size_measure = &quot;p_value&quot;, no_of_labels = 6) ## Warning: Using alpha for a discrete variable is not advised. node_purity_increase: The total increase in node purity (reduction in variance or MSE) attributed to the variable. mean_min_depth: The average minimum depth at which the variable is used to split a node. look more at this plot_min_depth_distribution(min_depth_frame, mean_sample = &quot;all_trees&quot;, k = 20, main = &quot;Distribution of Minimal Depth and Its Mean&quot;) 13.8 Final Notes We dont loop the RF because it is on whole data and the amount of trees in the RF Model "],["what-is-stacked-ensambled.html", "Chapter 14 What Is Stacked Ensambled", " Chapter 14 What Is Stacked Ensambled WE USE RMSE NOT RMSPE… Stacked ensemble learning, or stacking, involves training multiple different types of models (e.g., decision trees, logistic regression, neural networks) and then combining their predictions using a “meta-model” or “meta-learner.” The meta-model learns to make the final prediction by considering the outputs of the individual models as input features. This approach often leads to better performance than any single model or bagging method like random forest. Homogeneity vs. Heterogeneity: Bagging typically uses homogeneous models (e.g., multiple CART trees), while stacked ensembles use heterogeneous models (e.g., GBM, random forest, neural networks). # Load the required libraries library(randomForest) library(gbm) library(parallel) library(dplyr) library(MASS) library(caret) # Get the number of cores minus 1 nc &lt;- detectCores() - 1 # Load the Boston dataset data &lt;- Boston # Convert the &#39;chas&#39; column to a factor data$chas &lt;- as.factor(data$chas) # Scale all numeric columns in the data numeric_columns &lt;- sapply(data, is.numeric) data[numeric_columns] &lt;- scale(data[numeric_columns]) # Verify that the column names are correct colnames(data) &lt;- gsub(&quot;scale\\\\((.+)\\\\)&quot;, &quot;\\\\1&quot;, colnames(data)) # Now, the &#39;medv&#39; column should still be named &#39;medv&#39; # Train linear regression model lm_mod &lt;- lm(medv ~ ., data = data) # Train random forest model rf_mod &lt;- randomForest(medv ~ ., data = data) # Train gradient boosting model gbm_mod &lt;- gbm(medv ~ ., data = data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1) # Generate predictions lm_pred &lt;- predict(lm_mod, data) rf_pred &lt;- predict(rf_mod, data) gbm_pred &lt;- predict(gbm_mod, data, n.trees = 100) # Combine predictions into a data frame preds &lt;- data.frame(lm_pred, rf_pred, gbm_pred) # Train linear regression as meta-model meta_lm &lt;- lm(medv ~ ., data = cbind(preds, medv = data$medv)) # Predict using meta-model meta_lm_pred &lt;- predict(meta_lm, preds) # Calculate RMSPE (now as RMSE since the percentage part is removed) rmspe_lm &lt;- sqrt(mean((meta_lm_pred - data$medv)^2)) # Train random forest as meta-model meta_rf &lt;- randomForest(medv ~ ., data = cbind(preds, medv = data$medv)) # Predict using meta-model meta_rf_pred &lt;- predict(meta_rf, preds) # Calculate RMSPE (now as RMSE since the percentage part is removed) rmspe_rf &lt;- sqrt(mean((meta_rf_pred - data$medv)^2)) # Train gradient boosting as meta-model meta_gbm &lt;- gbm(medv ~ ., data = cbind(preds, medv = data$medv), distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1) # Predict using meta-model meta_gbm_pred &lt;- predict(meta_gbm, preds, n.trees = 100) # Calculate RMSPE (now as RMSE since the percentage part is removed) rmspe_gbm &lt;- sqrt(mean((meta_gbm_pred - data$medv)^2)) # Calculate RMSPE for base models (now as RMSE since the percentage part is removed) rmspe_base_lm &lt;- sqrt(mean((lm_pred - data$medv)^2)) rmspe_base_rf &lt;- sqrt(mean((rf_pred - data$medv)^2)) rmspe_base_gbm &lt;- sqrt(mean((gbm_pred - data$medv)^2)) # Find the best base model&#39;s RMSPE best_base_rmspe &lt;- min(rmspe_base_lm, rmspe_base_rf, rmspe_base_gbm) worst_base_rmspe &lt;- max(rmspe_base_lm, rmspe_base_rf, rmspe_base_gbm) # Print the RMSPEs print(rmspe_lm) ## [1] 0.1033253 print(rmspe_rf) ## [1] 0.08868791 print(rmspe_gbm) ## [1] 0.1286706 print(best_base_rmspe) ## [1] 0.1547934 print(worst_base_rmspe) ## [1] 0.5087679 Now we see that the stacked ensemble models have lower RMSPE compared to the individual base models. However the GBM model wasnt tuned at all. what is the effect of tuning the GBM model? # Hyperparameter tuning grid grid &lt;- expand.grid( n.trees = seq(50, 200, by = 50), shrinkage = seq(0.05, 0.15, by = 0.04), interaction.depth = seq(1, 3, by = 2) ) results &lt;- data.frame(n.trees = integer(), shrinkage = numeric(), interaction.depth = integer(), cv.error = numeric()) # Loop over the grid for (i in 1:nrow(grid)) { cat(&quot;\\rProgress:&quot;, i, &quot;/&quot;, nrow(grid), &quot;iterations completed&quot;) gbm_mod &lt;- gbm(medv ~ ., data = data, distribution = &quot;gaussian&quot;, n.trees = grid$n.trees[i], shrinkage = grid$shrinkage[i], interaction.depth = grid$interaction.depth[i], cv.folds = 3, n.cores = nc, verbose = FALSE) # Get the cross-validation error cv_error &lt;- min(gbm_mod$cv.error) # Store the results results &lt;- rbind(results, data.frame(n.trees = grid$n.trees[i], shrinkage = grid$shrinkage[i], interaction.depth = grid$interaction.depth[i], cv.error = cv_error)) } ## Progress: 1 / 24 iterations completedProgress: 2 / 24 iterations completedProgress: 3 / 24 iterations completedProgress: 4 / 24 iterations completedProgress: 5 / 24 iterations completedProgress: 6 / 24 iterations completedProgress: 7 / 24 iterations completedProgress: 8 / 24 iterations completedProgress: 9 / 24 iterations completedProgress: 10 / 24 iterations completedProgress: 11 / 24 iterations completedProgress: 12 / 24 iterations completedProgress: 13 / 24 iterations completedProgress: 14 / 24 iterations completedProgress: 15 / 24 iterations completedProgress: 16 / 24 iterations completedProgress: 17 / 24 iterations completedProgress: 18 / 24 iterations completedProgress: 19 / 24 iterations completedProgress: 20 / 24 iterations completedProgress: 21 / 24 iterations completedProgress: 22 / 24 iterations completedProgress: 23 / 24 iterations completedProgress: 24 / 24 iterations completed # Find the best hyperparameters - base R version best_params &lt;- results[order(results$cv.error), ][1, ] print(best_params) ## n.trees shrinkage interaction.depth cv.error ## 24 200 0.13 3 0.1317512 rmspe_t2 &lt;- c() for(i in 1:1000){ train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train the GBM model with the best hyperparameters best_gbm &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, verbose = FALSE) # Predict on the test set test_pred &lt;- predict(best_gbm, test_data, n.trees = 100) # Calculate RMSPE on the test set (now as RMSE since the percentage part is removed) rmspe_t2[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } mean(rmspe_t2) ## [1] 0.4371209 rmspe_test &lt;- c() for(i in 1:1000){ train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train the GBM model with the best hyperparameters best_gbm &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth) # Predict on the test set test_pred &lt;- predict(best_gbm, test_data, n.trees = best_params$n.trees) # Calculate RMSPE on the test set (now as RMSE since the percentage part is removed) rmspe_test[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } mean(rmspe_test) ## [1] 0.3744278 Now we see that we have a slightly tuned gbm model. SO will this better tuned base model effect the stacked ensemble model? # Store RMSPE results for the two stacked ensembles rmspe_ensemble_original &lt;- c() rmspe_ensemble_tuned &lt;- c() for(i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Original Base Models lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1, verbose = FALSE) # Generate predictions lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) gbm_pred &lt;- predict(gbm_mod, test_data, n.trees = 100) # Combine predictions into a data frame preds &lt;- data.frame(lm_pred, rf_pred, gbm_pred) # Train and evaluate original stacked ensemble meta_lm &lt;- lm(medv ~ ., data = cbind(preds, medv = test_data$medv)) meta_lm_pred &lt;- predict(meta_lm, preds) rmspe_ensemble_original[i] &lt;- sqrt(mean((meta_lm_pred - test_data$medv)^2)) # Train and evaluate stacked ensemble with tuned GBM model tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) gbm_pred_tuned &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) # Combine predictions with tuned GBM preds_tuned &lt;- data.frame(lm_pred, rf_pred, gbm_pred_tuned) meta_lm_tuned &lt;- lm(medv ~ ., data = cbind(preds_tuned, medv = test_data$medv)) meta_lm_pred_tuned &lt;- predict(meta_lm_tuned, preds_tuned) rmspe_ensemble_tuned[i] &lt;- sqrt(mean((meta_lm_pred_tuned - test_data$medv)^2)) } ## Progress: 1 iterations completedProgress: 2 iterations completedProgress: 3 iterations completedProgress: 4 iterations completedProgress: 5 iterations completedProgress: 6 iterations completedProgress: 7 iterations completedProgress: 8 iterations completedProgress: 9 iterations completedProgress: 10 iterations completedProgress: 11 iterations completedProgress: 12 iterations completedProgress: 13 iterations completedProgress: 14 iterations completedProgress: 15 iterations completedProgress: 16 iterations completedProgress: 17 iterations completedProgress: 18 iterations completedProgress: 19 iterations completedProgress: 20 iterations completedProgress: 21 iterations completedProgress: 22 iterations completedProgress: 23 iterations completedProgress: 24 iterations completedProgress: 25 iterations completedProgress: 26 iterations completedProgress: 27 iterations completedProgress: 28 iterations completedProgress: 29 iterations completedProgress: 30 iterations completedProgress: 31 iterations completedProgress: 32 iterations completedProgress: 33 iterations completedProgress: 34 iterations completedProgress: 35 iterations completedProgress: 36 iterations completedProgress: 37 iterations completedProgress: 38 iterations completedProgress: 39 iterations completedProgress: 40 iterations completedProgress: 41 iterations completedProgress: 42 iterations completedProgress: 43 iterations completedProgress: 44 iterations completedProgress: 45 iterations completedProgress: 46 iterations completedProgress: 47 iterations completedProgress: 48 iterations completedProgress: 49 iterations completedProgress: 50 iterations completedProgress: 51 iterations completedProgress: 52 iterations completedProgress: 53 iterations completedProgress: 54 iterations completedProgress: 55 iterations completedProgress: 56 iterations completedProgress: 57 iterations completedProgress: 58 iterations completedProgress: 59 iterations completedProgress: 60 iterations completedProgress: 61 iterations completedProgress: 62 iterations completedProgress: 63 iterations completedProgress: 64 iterations completedProgress: 65 iterations completedProgress: 66 iterations completedProgress: 67 iterations completedProgress: 68 iterations completedProgress: 69 iterations completedProgress: 70 iterations completedProgress: 71 iterations completedProgress: 72 iterations completedProgress: 73 iterations completedProgress: 74 iterations completedProgress: 75 iterations completedProgress: 76 iterations completedProgress: 77 iterations completedProgress: 78 iterations completedProgress: 79 iterations completedProgress: 80 iterations completedProgress: 81 iterations completedProgress: 82 iterations completedProgress: 83 iterations completedProgress: 84 iterations completedProgress: 85 iterations completedProgress: 86 iterations completedProgress: 87 iterations completedProgress: 88 iterations completedProgress: 89 iterations completedProgress: 90 iterations completedProgress: 91 iterations completedProgress: 92 iterations completedProgress: 93 iterations completedProgress: 94 iterations completedProgress: 95 iterations completedProgress: 96 iterations completedProgress: 97 iterations completedProgress: 98 iterations completedProgress: 99 iterations completedProgress: 100 iterations completedProgress: 101 iterations completedProgress: 102 iterations completedProgress: 103 iterations completedProgress: 104 iterations completedProgress: 105 iterations completedProgress: 106 iterations completedProgress: 107 iterations completedProgress: 108 iterations completedProgress: 109 iterations completedProgress: 110 iterations completedProgress: 111 iterations completedProgress: 112 iterations completedProgress: 113 iterations completedProgress: 114 iterations completedProgress: 115 iterations completedProgress: 116 iterations completedProgress: 117 iterations completedProgress: 118 iterations completedProgress: 119 iterations completedProgress: 120 iterations completedProgress: 121 iterations completedProgress: 122 iterations completedProgress: 123 iterations completedProgress: 124 iterations completedProgress: 125 iterations completedProgress: 126 iterations completedProgress: 127 iterations completedProgress: 128 iterations completedProgress: 129 iterations completedProgress: 130 iterations completedProgress: 131 iterations completedProgress: 132 iterations completedProgress: 133 iterations completedProgress: 134 iterations completedProgress: 135 iterations completedProgress: 136 iterations completedProgress: 137 iterations completedProgress: 138 iterations completedProgress: 139 iterations completedProgress: 140 iterations completedProgress: 141 iterations completedProgress: 142 iterations completedProgress: 143 iterations completedProgress: 144 iterations completedProgress: 145 iterations completedProgress: 146 iterations completedProgress: 147 iterations completedProgress: 148 iterations completedProgress: 149 iterations completedProgress: 150 iterations completedProgress: 151 iterations completedProgress: 152 iterations completedProgress: 153 iterations completedProgress: 154 iterations completedProgress: 155 iterations completedProgress: 156 iterations completedProgress: 157 iterations completedProgress: 158 iterations completedProgress: 159 iterations completedProgress: 160 iterations completedProgress: 161 iterations completedProgress: 162 iterations completedProgress: 163 iterations completedProgress: 164 iterations completedProgress: 165 iterations completedProgress: 166 iterations completedProgress: 167 iterations completedProgress: 168 iterations completedProgress: 169 iterations completedProgress: 170 iterations completedProgress: 171 iterations completedProgress: 172 iterations completedProgress: 173 iterations completedProgress: 174 iterations completedProgress: 175 iterations completedProgress: 176 iterations completedProgress: 177 iterations completedProgress: 178 iterations completedProgress: 179 iterations completedProgress: 180 iterations completedProgress: 181 iterations completedProgress: 182 iterations completedProgress: 183 iterations completedProgress: 184 iterations completedProgress: 185 iterations completedProgress: 186 iterations completedProgress: 187 iterations completedProgress: 188 iterations completedProgress: 189 iterations completedProgress: 190 iterations completedProgress: 191 iterations completedProgress: 192 iterations completedProgress: 193 iterations completedProgress: 194 iterations completedProgress: 195 iterations completedProgress: 196 iterations completedProgress: 197 iterations completedProgress: 198 iterations completedProgress: 199 iterations completedProgress: 200 iterations completedProgress: 201 iterations completedProgress: 202 iterations completedProgress: 203 iterations completedProgress: 204 iterations completedProgress: 205 iterations completedProgress: 206 iterations completedProgress: 207 iterations completedProgress: 208 iterations completedProgress: 209 iterations completedProgress: 210 iterations completedProgress: 211 iterations completedProgress: 212 iterations completedProgress: 213 iterations completedProgress: 214 iterations completedProgress: 215 iterations completedProgress: 216 iterations completedProgress: 217 iterations completedProgress: 218 iterations completedProgress: 219 iterations completedProgress: 220 iterations completedProgress: 221 iterations completedProgress: 222 iterations completedProgress: 223 iterations completedProgress: 224 iterations completedProgress: 225 iterations completedProgress: 226 iterations completedProgress: 227 iterations completedProgress: 228 iterations completedProgress: 229 iterations completedProgress: 230 iterations completedProgress: 231 iterations completedProgress: 232 iterations completedProgress: 233 iterations completedProgress: 234 iterations completedProgress: 235 iterations completedProgress: 236 iterations completedProgress: 237 iterations completedProgress: 238 iterations completedProgress: 239 iterations completedProgress: 240 iterations completedProgress: 241 iterations completedProgress: 242 iterations completedProgress: 243 iterations completedProgress: 244 iterations completedProgress: 245 iterations completedProgress: 246 iterations completedProgress: 247 iterations completedProgress: 248 iterations completedProgress: 249 iterations completedProgress: 250 iterations completedProgress: 251 iterations completedProgress: 252 iterations completedProgress: 253 iterations completedProgress: 254 iterations completedProgress: 255 iterations completedProgress: 256 iterations completedProgress: 257 iterations completedProgress: 258 iterations completedProgress: 259 iterations completedProgress: 260 iterations completedProgress: 261 iterations completedProgress: 262 iterations completedProgress: 263 iterations completedProgress: 264 iterations completedProgress: 265 iterations completedProgress: 266 iterations completedProgress: 267 iterations completedProgress: 268 iterations completedProgress: 269 iterations completedProgress: 270 iterations completedProgress: 271 iterations completedProgress: 272 iterations completedProgress: 273 iterations completedProgress: 274 iterations completedProgress: 275 iterations completedProgress: 276 iterations completedProgress: 277 iterations completedProgress: 278 iterations completedProgress: 279 iterations completedProgress: 280 iterations completedProgress: 281 iterations completedProgress: 282 iterations completedProgress: 283 iterations completedProgress: 284 iterations completedProgress: 285 iterations completedProgress: 286 iterations completedProgress: 287 iterations completedProgress: 288 iterations completedProgress: 289 iterations completedProgress: 290 iterations completedProgress: 291 iterations completedProgress: 292 iterations completedProgress: 293 iterations completedProgress: 294 iterations completedProgress: 295 iterations completedProgress: 296 iterations completedProgress: 297 iterations completedProgress: 298 iterations completedProgress: 299 iterations completedProgress: 300 iterations completedProgress: 301 iterations completedProgress: 302 iterations completedProgress: 303 iterations completedProgress: 304 iterations completedProgress: 305 iterations completedProgress: 306 iterations completedProgress: 307 iterations completedProgress: 308 iterations completedProgress: 309 iterations completedProgress: 310 iterations completedProgress: 311 iterations completedProgress: 312 iterations completedProgress: 313 iterations completedProgress: 314 iterations completedProgress: 315 iterations completedProgress: 316 iterations completedProgress: 317 iterations completedProgress: 318 iterations completedProgress: 319 iterations completedProgress: 320 iterations completedProgress: 321 iterations completedProgress: 322 iterations completedProgress: 323 iterations completedProgress: 324 iterations completedProgress: 325 iterations completedProgress: 326 iterations completedProgress: 327 iterations completedProgress: 328 iterations completedProgress: 329 iterations completedProgress: 330 iterations completedProgress: 331 iterations completedProgress: 332 iterations completedProgress: 333 iterations completedProgress: 334 iterations completedProgress: 335 iterations completedProgress: 336 iterations completedProgress: 337 iterations completedProgress: 338 iterations completedProgress: 339 iterations completedProgress: 340 iterations completedProgress: 341 iterations completedProgress: 342 iterations completedProgress: 343 iterations completedProgress: 344 iterations completedProgress: 345 iterations completedProgress: 346 iterations completedProgress: 347 iterations completedProgress: 348 iterations completedProgress: 349 iterations completedProgress: 350 iterations completedProgress: 351 iterations completedProgress: 352 iterations completedProgress: 353 iterations completedProgress: 354 iterations completedProgress: 355 iterations completedProgress: 356 iterations completedProgress: 357 iterations completedProgress: 358 iterations completedProgress: 359 iterations completedProgress: 360 iterations completedProgress: 361 iterations completedProgress: 362 iterations completedProgress: 363 iterations completedProgress: 364 iterations completedProgress: 365 iterations completedProgress: 366 iterations completedProgress: 367 iterations completedProgress: 368 iterations completedProgress: 369 iterations completedProgress: 370 iterations completedProgress: 371 iterations completedProgress: 372 iterations completedProgress: 373 iterations completedProgress: 374 iterations completedProgress: 375 iterations completedProgress: 376 iterations completedProgress: 377 iterations completedProgress: 378 iterations completedProgress: 379 iterations completedProgress: 380 iterations completedProgress: 381 iterations completedProgress: 382 iterations completedProgress: 383 iterations completedProgress: 384 iterations completedProgress: 385 iterations completedProgress: 386 iterations completedProgress: 387 iterations completedProgress: 388 iterations completedProgress: 389 iterations completedProgress: 390 iterations completedProgress: 391 iterations completedProgress: 392 iterations completedProgress: 393 iterations completedProgress: 394 iterations completedProgress: 395 iterations completedProgress: 396 iterations completedProgress: 397 iterations completedProgress: 398 iterations completedProgress: 399 iterations completedProgress: 400 iterations completedProgress: 401 iterations completedProgress: 402 iterations completedProgress: 403 iterations completedProgress: 404 iterations completedProgress: 405 iterations completedProgress: 406 iterations completedProgress: 407 iterations completedProgress: 408 iterations completedProgress: 409 iterations completedProgress: 410 iterations completedProgress: 411 iterations completedProgress: 412 iterations completedProgress: 413 iterations completedProgress: 414 iterations completedProgress: 415 iterations completedProgress: 416 iterations completedProgress: 417 iterations completedProgress: 418 iterations completedProgress: 419 iterations completedProgress: 420 iterations completedProgress: 421 iterations completedProgress: 422 iterations completedProgress: 423 iterations completedProgress: 424 iterations completedProgress: 425 iterations completedProgress: 426 iterations completedProgress: 427 iterations completedProgress: 428 iterations completedProgress: 429 iterations completedProgress: 430 iterations completedProgress: 431 iterations completedProgress: 432 iterations completedProgress: 433 iterations completedProgress: 434 iterations completedProgress: 435 iterations completedProgress: 436 iterations completedProgress: 437 iterations completedProgress: 438 iterations completedProgress: 439 iterations completedProgress: 440 iterations completedProgress: 441 iterations completedProgress: 442 iterations completedProgress: 443 iterations completedProgress: 444 iterations completedProgress: 445 iterations completedProgress: 446 iterations completedProgress: 447 iterations completedProgress: 448 iterations completedProgress: 449 iterations completedProgress: 450 iterations completedProgress: 451 iterations completedProgress: 452 iterations completedProgress: 453 iterations completedProgress: 454 iterations completedProgress: 455 iterations completedProgress: 456 iterations completedProgress: 457 iterations completedProgress: 458 iterations completedProgress: 459 iterations completedProgress: 460 iterations completedProgress: 461 iterations completedProgress: 462 iterations completedProgress: 463 iterations completedProgress: 464 iterations completedProgress: 465 iterations completedProgress: 466 iterations completedProgress: 467 iterations completedProgress: 468 iterations completedProgress: 469 iterations completedProgress: 470 iterations completedProgress: 471 iterations completedProgress: 472 iterations completedProgress: 473 iterations completedProgress: 474 iterations completedProgress: 475 iterations completedProgress: 476 iterations completedProgress: 477 iterations completedProgress: 478 iterations completedProgress: 479 iterations completedProgress: 480 iterations completedProgress: 481 iterations completedProgress: 482 iterations completedProgress: 483 iterations completedProgress: 484 iterations completedProgress: 485 iterations completedProgress: 486 iterations completedProgress: 487 iterations completedProgress: 488 iterations completedProgress: 489 iterations completedProgress: 490 iterations completedProgress: 491 iterations completedProgress: 492 iterations completedProgress: 493 iterations completedProgress: 494 iterations completedProgress: 495 iterations completedProgress: 496 iterations completedProgress: 497 iterations completedProgress: 498 iterations completedProgress: 499 iterations completedProgress: 500 iterations completedProgress: 501 iterations completedProgress: 502 iterations completedProgress: 503 iterations completedProgress: 504 iterations completedProgress: 505 iterations completedProgress: 506 iterations completedProgress: 507 iterations completedProgress: 508 iterations completedProgress: 509 iterations completedProgress: 510 iterations completedProgress: 511 iterations completedProgress: 512 iterations completedProgress: 513 iterations completedProgress: 514 iterations completedProgress: 515 iterations completedProgress: 516 iterations completedProgress: 517 iterations completedProgress: 518 iterations completedProgress: 519 iterations completedProgress: 520 iterations completedProgress: 521 iterations completedProgress: 522 iterations completedProgress: 523 iterations completedProgress: 524 iterations completedProgress: 525 iterations completedProgress: 526 iterations completedProgress: 527 iterations completedProgress: 528 iterations completedProgress: 529 iterations completedProgress: 530 iterations completedProgress: 531 iterations completedProgress: 532 iterations completedProgress: 533 iterations completedProgress: 534 iterations completedProgress: 535 iterations completedProgress: 536 iterations completedProgress: 537 iterations completedProgress: 538 iterations completedProgress: 539 iterations completedProgress: 540 iterations completedProgress: 541 iterations completedProgress: 542 iterations completedProgress: 543 iterations completedProgress: 544 iterations completedProgress: 545 iterations completedProgress: 546 iterations completedProgress: 547 iterations completedProgress: 548 iterations completedProgress: 549 iterations completedProgress: 550 iterations completedProgress: 551 iterations completedProgress: 552 iterations completedProgress: 553 iterations completedProgress: 554 iterations completedProgress: 555 iterations completedProgress: 556 iterations completedProgress: 557 iterations completedProgress: 558 iterations completedProgress: 559 iterations completedProgress: 560 iterations completedProgress: 561 iterations completedProgress: 562 iterations completedProgress: 563 iterations completedProgress: 564 iterations completedProgress: 565 iterations completedProgress: 566 iterations completedProgress: 567 iterations completedProgress: 568 iterations completedProgress: 569 iterations completedProgress: 570 iterations completedProgress: 571 iterations completedProgress: 572 iterations completedProgress: 573 iterations completedProgress: 574 iterations completedProgress: 575 iterations completedProgress: 576 iterations completedProgress: 577 iterations completedProgress: 578 iterations completedProgress: 579 iterations completedProgress: 580 iterations completedProgress: 581 iterations completedProgress: 582 iterations completedProgress: 583 iterations completedProgress: 584 iterations completedProgress: 585 iterations completedProgress: 586 iterations completedProgress: 587 iterations completedProgress: 588 iterations completedProgress: 589 iterations completedProgress: 590 iterations completedProgress: 591 iterations completedProgress: 592 iterations completedProgress: 593 iterations completedProgress: 594 iterations completedProgress: 595 iterations completedProgress: 596 iterations completedProgress: 597 iterations completedProgress: 598 iterations completedProgress: 599 iterations completedProgress: 600 iterations completedProgress: 601 iterations completedProgress: 602 iterations completedProgress: 603 iterations completedProgress: 604 iterations completedProgress: 605 iterations completedProgress: 606 iterations completedProgress: 607 iterations completedProgress: 608 iterations completedProgress: 609 iterations completedProgress: 610 iterations completedProgress: 611 iterations completedProgress: 612 iterations completedProgress: 613 iterations completedProgress: 614 iterations completedProgress: 615 iterations completedProgress: 616 iterations completedProgress: 617 iterations completedProgress: 618 iterations completedProgress: 619 iterations completedProgress: 620 iterations completedProgress: 621 iterations completedProgress: 622 iterations completedProgress: 623 iterations completedProgress: 624 iterations completedProgress: 625 iterations completedProgress: 626 iterations completedProgress: 627 iterations completedProgress: 628 iterations completedProgress: 629 iterations completedProgress: 630 iterations completedProgress: 631 iterations completedProgress: 632 iterations completedProgress: 633 iterations completedProgress: 634 iterations completedProgress: 635 iterations completedProgress: 636 iterations completedProgress: 637 iterations completedProgress: 638 iterations completedProgress: 639 iterations completedProgress: 640 iterations completedProgress: 641 iterations completedProgress: 642 iterations completedProgress: 643 iterations completedProgress: 644 iterations completedProgress: 645 iterations completedProgress: 646 iterations completedProgress: 647 iterations completedProgress: 648 iterations completedProgress: 649 iterations completedProgress: 650 iterations completedProgress: 651 iterations completedProgress: 652 iterations completedProgress: 653 iterations completedProgress: 654 iterations completedProgress: 655 iterations completedProgress: 656 iterations completedProgress: 657 iterations completedProgress: 658 iterations completedProgress: 659 iterations completedProgress: 660 iterations completedProgress: 661 iterations completedProgress: 662 iterations completedProgress: 663 iterations completedProgress: 664 iterations completedProgress: 665 iterations completedProgress: 666 iterations completedProgress: 667 iterations completedProgress: 668 iterations completedProgress: 669 iterations completedProgress: 670 iterations completedProgress: 671 iterations completedProgress: 672 iterations completedProgress: 673 iterations completedProgress: 674 iterations completedProgress: 675 iterations completedProgress: 676 iterations completedProgress: 677 iterations completedProgress: 678 iterations completedProgress: 679 iterations completedProgress: 680 iterations completedProgress: 681 iterations completedProgress: 682 iterations completedProgress: 683 iterations completedProgress: 684 iterations completedProgress: 685 iterations completedProgress: 686 iterations completedProgress: 687 iterations completedProgress: 688 iterations completedProgress: 689 iterations completedProgress: 690 iterations completedProgress: 691 iterations completedProgress: 692 iterations completedProgress: 693 iterations completedProgress: 694 iterations completedProgress: 695 iterations completedProgress: 696 iterations completedProgress: 697 iterations completedProgress: 698 iterations completedProgress: 699 iterations completedProgress: 700 iterations completedProgress: 701 iterations completedProgress: 702 iterations completedProgress: 703 iterations completedProgress: 704 iterations completedProgress: 705 iterations completedProgress: 706 iterations completedProgress: 707 iterations completedProgress: 708 iterations completedProgress: 709 iterations completedProgress: 710 iterations completedProgress: 711 iterations completedProgress: 712 iterations completedProgress: 713 iterations completedProgress: 714 iterations completedProgress: 715 iterations completedProgress: 716 iterations completedProgress: 717 iterations completedProgress: 718 iterations completedProgress: 719 iterations completedProgress: 720 iterations completedProgress: 721 iterations completedProgress: 722 iterations completedProgress: 723 iterations completedProgress: 724 iterations completedProgress: 725 iterations completedProgress: 726 iterations completedProgress: 727 iterations completedProgress: 728 iterations completedProgress: 729 iterations completedProgress: 730 iterations completedProgress: 731 iterations completedProgress: 732 iterations completedProgress: 733 iterations completedProgress: 734 iterations completedProgress: 735 iterations completedProgress: 736 iterations completedProgress: 737 iterations completedProgress: 738 iterations completedProgress: 739 iterations completedProgress: 740 iterations completedProgress: 741 iterations completedProgress: 742 iterations completedProgress: 743 iterations completedProgress: 744 iterations completedProgress: 745 iterations completedProgress: 746 iterations completedProgress: 747 iterations completedProgress: 748 iterations completedProgress: 749 iterations completedProgress: 750 iterations completedProgress: 751 iterations completedProgress: 752 iterations completedProgress: 753 iterations completedProgress: 754 iterations completedProgress: 755 iterations completedProgress: 756 iterations completedProgress: 757 iterations completedProgress: 758 iterations completedProgress: 759 iterations completedProgress: 760 iterations completedProgress: 761 iterations completedProgress: 762 iterations completedProgress: 763 iterations completedProgress: 764 iterations completedProgress: 765 iterations completedProgress: 766 iterations completedProgress: 767 iterations completedProgress: 768 iterations completedProgress: 769 iterations completedProgress: 770 iterations completedProgress: 771 iterations completedProgress: 772 iterations completedProgress: 773 iterations completedProgress: 774 iterations completedProgress: 775 iterations completedProgress: 776 iterations completedProgress: 777 iterations completedProgress: 778 iterations completedProgress: 779 iterations completedProgress: 780 iterations completedProgress: 781 iterations completedProgress: 782 iterations completedProgress: 783 iterations completedProgress: 784 iterations completedProgress: 785 iterations completedProgress: 786 iterations completedProgress: 787 iterations completedProgress: 788 iterations completedProgress: 789 iterations completedProgress: 790 iterations completedProgress: 791 iterations completedProgress: 792 iterations completedProgress: 793 iterations completedProgress: 794 iterations completedProgress: 795 iterations completedProgress: 796 iterations completedProgress: 797 iterations completedProgress: 798 iterations completedProgress: 799 iterations completedProgress: 800 iterations completedProgress: 801 iterations completedProgress: 802 iterations completedProgress: 803 iterations completedProgress: 804 iterations completedProgress: 805 iterations completedProgress: 806 iterations completedProgress: 807 iterations completedProgress: 808 iterations completedProgress: 809 iterations completedProgress: 810 iterations completedProgress: 811 iterations completedProgress: 812 iterations completedProgress: 813 iterations completedProgress: 814 iterations completedProgress: 815 iterations completedProgress: 816 iterations completedProgress: 817 iterations completedProgress: 818 iterations completedProgress: 819 iterations completedProgress: 820 iterations completedProgress: 821 iterations completedProgress: 822 iterations completedProgress: 823 iterations completedProgress: 824 iterations completedProgress: 825 iterations completedProgress: 826 iterations completedProgress: 827 iterations completedProgress: 828 iterations completedProgress: 829 iterations completedProgress: 830 iterations completedProgress: 831 iterations completedProgress: 832 iterations completedProgress: 833 iterations completedProgress: 834 iterations completedProgress: 835 iterations completedProgress: 836 iterations completedProgress: 837 iterations completedProgress: 838 iterations completedProgress: 839 iterations completedProgress: 840 iterations completedProgress: 841 iterations completedProgress: 842 iterations completedProgress: 843 iterations completedProgress: 844 iterations completedProgress: 845 iterations completedProgress: 846 iterations completedProgress: 847 iterations completedProgress: 848 iterations completedProgress: 849 iterations completedProgress: 850 iterations completedProgress: 851 iterations completedProgress: 852 iterations completedProgress: 853 iterations completedProgress: 854 iterations completedProgress: 855 iterations completedProgress: 856 iterations completedProgress: 857 iterations completedProgress: 858 iterations completedProgress: 859 iterations completedProgress: 860 iterations completedProgress: 861 iterations completedProgress: 862 iterations completedProgress: 863 iterations completedProgress: 864 iterations completedProgress: 865 iterations completedProgress: 866 iterations completedProgress: 867 iterations completedProgress: 868 iterations completedProgress: 869 iterations completedProgress: 870 iterations completedProgress: 871 iterations completedProgress: 872 iterations completedProgress: 873 iterations completedProgress: 874 iterations completedProgress: 875 iterations completedProgress: 876 iterations completedProgress: 877 iterations completedProgress: 878 iterations completedProgress: 879 iterations completedProgress: 880 iterations completedProgress: 881 iterations completedProgress: 882 iterations completedProgress: 883 iterations completedProgress: 884 iterations completedProgress: 885 iterations completedProgress: 886 iterations completedProgress: 887 iterations completedProgress: 888 iterations completedProgress: 889 iterations completedProgress: 890 iterations completedProgress: 891 iterations completedProgress: 892 iterations completedProgress: 893 iterations completedProgress: 894 iterations completedProgress: 895 iterations completedProgress: 896 iterations completedProgress: 897 iterations completedProgress: 898 iterations completedProgress: 899 iterations completedProgress: 900 iterations completedProgress: 901 iterations completedProgress: 902 iterations completedProgress: 903 iterations completedProgress: 904 iterations completedProgress: 905 iterations completedProgress: 906 iterations completedProgress: 907 iterations completedProgress: 908 iterations completedProgress: 909 iterations completedProgress: 910 iterations completedProgress: 911 iterations completedProgress: 912 iterations completedProgress: 913 iterations completedProgress: 914 iterations completedProgress: 915 iterations completedProgress: 916 iterations completedProgress: 917 iterations completedProgress: 918 iterations completedProgress: 919 iterations completedProgress: 920 iterations completedProgress: 921 iterations completedProgress: 922 iterations completedProgress: 923 iterations completedProgress: 924 iterations completedProgress: 925 iterations completedProgress: 926 iterations completedProgress: 927 iterations completedProgress: 928 iterations completedProgress: 929 iterations completedProgress: 930 iterations completedProgress: 931 iterations completedProgress: 932 iterations completedProgress: 933 iterations completedProgress: 934 iterations completedProgress: 935 iterations completedProgress: 936 iterations completedProgress: 937 iterations completedProgress: 938 iterations completedProgress: 939 iterations completedProgress: 940 iterations completedProgress: 941 iterations completedProgress: 942 iterations completedProgress: 943 iterations completedProgress: 944 iterations completedProgress: 945 iterations completedProgress: 946 iterations completedProgress: 947 iterations completedProgress: 948 iterations completedProgress: 949 iterations completedProgress: 950 iterations completedProgress: 951 iterations completedProgress: 952 iterations completedProgress: 953 iterations completedProgress: 954 iterations completedProgress: 955 iterations completedProgress: 956 iterations completedProgress: 957 iterations completedProgress: 958 iterations completedProgress: 959 iterations completedProgress: 960 iterations completedProgress: 961 iterations completedProgress: 962 iterations completedProgress: 963 iterations completedProgress: 964 iterations completedProgress: 965 iterations completedProgress: 966 iterations completedProgress: 967 iterations completedProgress: 968 iterations completedProgress: 969 iterations completedProgress: 970 iterations completedProgress: 971 iterations completedProgress: 972 iterations completedProgress: 973 iterations completedProgress: 974 iterations completedProgress: 975 iterations completedProgress: 976 iterations completedProgress: 977 iterations completedProgress: 978 iterations completedProgress: 979 iterations completedProgress: 980 iterations completedProgress: 981 iterations completedProgress: 982 iterations completedProgress: 983 iterations completedProgress: 984 iterations completedProgress: 985 iterations completedProgress: 986 iterations completedProgress: 987 iterations completedProgress: 988 iterations completedProgress: 989 iterations completedProgress: 990 iterations completedProgress: 991 iterations completedProgress: 992 iterations completedProgress: 993 iterations completedProgress: 994 iterations completedProgress: 995 iterations completedProgress: 996 iterations completedProgress: 997 iterations completedProgress: 998 iterations completedProgress: 999 iterations completedProgress: 1000 iterations completed # Calculate mean RMSPE for both ensembles mean_rmspe_ensemble_original &lt;- mean(rmspe_ensemble_original) mean_rmspe_ensemble_tuned &lt;- mean(rmspe_ensemble_tuned) # Print the results print(paste(&quot;Mean RMSPE of Original Stacked Ensemble:&quot;, mean_rmspe_ensemble_original)) ## [1] &quot;Mean RMSPE of Original Stacked Ensemble: 0.352672885826131&quot; print(paste(&quot;Mean RMSPE of Tuned Stacked Ensemble:&quot;, mean_rmspe_ensemble_tuned)) ## [1] &quot;Mean RMSPE of Tuned Stacked Ensemble: 0.345268424936948&quot; We can see a difference in the average RMSPE now we will ask the question does adding a model that is simply worse and similar to the other models help the ensemble model? we will now stacked ensambled all 4 base models that we have # Store RMSPE results for the stacked ensemble with all four models rmspe_ensemble_four &lt;- c() for(i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) # Train both GBM models on the bootstrapped training data base_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1, verbose = FALSE) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) base_gbm_pred &lt;- predict(base_gbm_mod, test_data, n.trees = 100) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, base_gbm_pred, tuned_gbm_pred) # Train and evaluate the stacked ensemble with all four models meta_four &lt;- lm(medv ~ ., data = cbind(preds_four, medv = test_data$medv)) meta_four_pred &lt;- predict(meta_four, preds_four) # Calculate RMSPE for the ensemble with all four models rmspe_ensemble_four[i] &lt;- sqrt(mean((meta_four_pred - test_data$medv)^2)) } ## Progress: 1 iterations completedProgress: 2 iterations completedProgress: 3 iterations completedProgress: 4 iterations completedProgress: 5 iterations completedProgress: 6 iterations completedProgress: 7 iterations completedProgress: 8 iterations completedProgress: 9 iterations completedProgress: 10 iterations completedProgress: 11 iterations completedProgress: 12 iterations completedProgress: 13 iterations completedProgress: 14 iterations completedProgress: 15 iterations completedProgress: 16 iterations completedProgress: 17 iterations completedProgress: 18 iterations completedProgress: 19 iterations completedProgress: 20 iterations completedProgress: 21 iterations completedProgress: 22 iterations completedProgress: 23 iterations completedProgress: 24 iterations completedProgress: 25 iterations completedProgress: 26 iterations completedProgress: 27 iterations completedProgress: 28 iterations completedProgress: 29 iterations completedProgress: 30 iterations completedProgress: 31 iterations completedProgress: 32 iterations completedProgress: 33 iterations completedProgress: 34 iterations completedProgress: 35 iterations completedProgress: 36 iterations completedProgress: 37 iterations completedProgress: 38 iterations completedProgress: 39 iterations completedProgress: 40 iterations completedProgress: 41 iterations completedProgress: 42 iterations completedProgress: 43 iterations completedProgress: 44 iterations completedProgress: 45 iterations completedProgress: 46 iterations completedProgress: 47 iterations completedProgress: 48 iterations completedProgress: 49 iterations completedProgress: 50 iterations completedProgress: 51 iterations completedProgress: 52 iterations completedProgress: 53 iterations completedProgress: 54 iterations completedProgress: 55 iterations completedProgress: 56 iterations completedProgress: 57 iterations completedProgress: 58 iterations completedProgress: 59 iterations completedProgress: 60 iterations completedProgress: 61 iterations completedProgress: 62 iterations completedProgress: 63 iterations completedProgress: 64 iterations completedProgress: 65 iterations completedProgress: 66 iterations completedProgress: 67 iterations completedProgress: 68 iterations completedProgress: 69 iterations completedProgress: 70 iterations completedProgress: 71 iterations completedProgress: 72 iterations completedProgress: 73 iterations completedProgress: 74 iterations completedProgress: 75 iterations completedProgress: 76 iterations completedProgress: 77 iterations completedProgress: 78 iterations completedProgress: 79 iterations completedProgress: 80 iterations completedProgress: 81 iterations completedProgress: 82 iterations completedProgress: 83 iterations completedProgress: 84 iterations completedProgress: 85 iterations completedProgress: 86 iterations completedProgress: 87 iterations completedProgress: 88 iterations completedProgress: 89 iterations completedProgress: 90 iterations completedProgress: 91 iterations completedProgress: 92 iterations completedProgress: 93 iterations completedProgress: 94 iterations completedProgress: 95 iterations completedProgress: 96 iterations completedProgress: 97 iterations completedProgress: 98 iterations completedProgress: 99 iterations completedProgress: 100 iterations completedProgress: 101 iterations completedProgress: 102 iterations completedProgress: 103 iterations completedProgress: 104 iterations completedProgress: 105 iterations completedProgress: 106 iterations completedProgress: 107 iterations completedProgress: 108 iterations completedProgress: 109 iterations completedProgress: 110 iterations completedProgress: 111 iterations completedProgress: 112 iterations completedProgress: 113 iterations completedProgress: 114 iterations completedProgress: 115 iterations completedProgress: 116 iterations completedProgress: 117 iterations completedProgress: 118 iterations completedProgress: 119 iterations completedProgress: 120 iterations completedProgress: 121 iterations completedProgress: 122 iterations completedProgress: 123 iterations completedProgress: 124 iterations completedProgress: 125 iterations completedProgress: 126 iterations completedProgress: 127 iterations completedProgress: 128 iterations completedProgress: 129 iterations completedProgress: 130 iterations completedProgress: 131 iterations completedProgress: 132 iterations completedProgress: 133 iterations completedProgress: 134 iterations completedProgress: 135 iterations completedProgress: 136 iterations completedProgress: 137 iterations completedProgress: 138 iterations completedProgress: 139 iterations completedProgress: 140 iterations completedProgress: 141 iterations completedProgress: 142 iterations completedProgress: 143 iterations completedProgress: 144 iterations completedProgress: 145 iterations completedProgress: 146 iterations completedProgress: 147 iterations completedProgress: 148 iterations completedProgress: 149 iterations completedProgress: 150 iterations completedProgress: 151 iterations completedProgress: 152 iterations completedProgress: 153 iterations completedProgress: 154 iterations completedProgress: 155 iterations completedProgress: 156 iterations completedProgress: 157 iterations completedProgress: 158 iterations completedProgress: 159 iterations completedProgress: 160 iterations completedProgress: 161 iterations completedProgress: 162 iterations completedProgress: 163 iterations completedProgress: 164 iterations completedProgress: 165 iterations completedProgress: 166 iterations completedProgress: 167 iterations completedProgress: 168 iterations completedProgress: 169 iterations completedProgress: 170 iterations completedProgress: 171 iterations completedProgress: 172 iterations completedProgress: 173 iterations completedProgress: 174 iterations completedProgress: 175 iterations completedProgress: 176 iterations completedProgress: 177 iterations completedProgress: 178 iterations completedProgress: 179 iterations completedProgress: 180 iterations completedProgress: 181 iterations completedProgress: 182 iterations completedProgress: 183 iterations completedProgress: 184 iterations completedProgress: 185 iterations completedProgress: 186 iterations completedProgress: 187 iterations completedProgress: 188 iterations completedProgress: 189 iterations completedProgress: 190 iterations completedProgress: 191 iterations completedProgress: 192 iterations completedProgress: 193 iterations completedProgress: 194 iterations completedProgress: 195 iterations completedProgress: 196 iterations completedProgress: 197 iterations completedProgress: 198 iterations completedProgress: 199 iterations completedProgress: 200 iterations completedProgress: 201 iterations completedProgress: 202 iterations completedProgress: 203 iterations completedProgress: 204 iterations completedProgress: 205 iterations completedProgress: 206 iterations completedProgress: 207 iterations completedProgress: 208 iterations completedProgress: 209 iterations completedProgress: 210 iterations completedProgress: 211 iterations completedProgress: 212 iterations completedProgress: 213 iterations completedProgress: 214 iterations completedProgress: 215 iterations completedProgress: 216 iterations completedProgress: 217 iterations completedProgress: 218 iterations completedProgress: 219 iterations completedProgress: 220 iterations completedProgress: 221 iterations completedProgress: 222 iterations completedProgress: 223 iterations completedProgress: 224 iterations completedProgress: 225 iterations completedProgress: 226 iterations completedProgress: 227 iterations completedProgress: 228 iterations completedProgress: 229 iterations completedProgress: 230 iterations completedProgress: 231 iterations completedProgress: 232 iterations completedProgress: 233 iterations completedProgress: 234 iterations completedProgress: 235 iterations completedProgress: 236 iterations completedProgress: 237 iterations completedProgress: 238 iterations completedProgress: 239 iterations completedProgress: 240 iterations completedProgress: 241 iterations completedProgress: 242 iterations completedProgress: 243 iterations completedProgress: 244 iterations completedProgress: 245 iterations completedProgress: 246 iterations completedProgress: 247 iterations completedProgress: 248 iterations completedProgress: 249 iterations completedProgress: 250 iterations completedProgress: 251 iterations completedProgress: 252 iterations completedProgress: 253 iterations completedProgress: 254 iterations completedProgress: 255 iterations completedProgress: 256 iterations completedProgress: 257 iterations completedProgress: 258 iterations completedProgress: 259 iterations completedProgress: 260 iterations completedProgress: 261 iterations completedProgress: 262 iterations completedProgress: 263 iterations completedProgress: 264 iterations completedProgress: 265 iterations completedProgress: 266 iterations completedProgress: 267 iterations completedProgress: 268 iterations completedProgress: 269 iterations completedProgress: 270 iterations completedProgress: 271 iterations completedProgress: 272 iterations completedProgress: 273 iterations completedProgress: 274 iterations completedProgress: 275 iterations completedProgress: 276 iterations completedProgress: 277 iterations completedProgress: 278 iterations completedProgress: 279 iterations completedProgress: 280 iterations completedProgress: 281 iterations completedProgress: 282 iterations completedProgress: 283 iterations completedProgress: 284 iterations completedProgress: 285 iterations completedProgress: 286 iterations completedProgress: 287 iterations completedProgress: 288 iterations completedProgress: 289 iterations completedProgress: 290 iterations completedProgress: 291 iterations completedProgress: 292 iterations completedProgress: 293 iterations completedProgress: 294 iterations completedProgress: 295 iterations completedProgress: 296 iterations completedProgress: 297 iterations completedProgress: 298 iterations completedProgress: 299 iterations completedProgress: 300 iterations completedProgress: 301 iterations completedProgress: 302 iterations completedProgress: 303 iterations completedProgress: 304 iterations completedProgress: 305 iterations completedProgress: 306 iterations completedProgress: 307 iterations completedProgress: 308 iterations completedProgress: 309 iterations completedProgress: 310 iterations completedProgress: 311 iterations completedProgress: 312 iterations completedProgress: 313 iterations completedProgress: 314 iterations completedProgress: 315 iterations completedProgress: 316 iterations completedProgress: 317 iterations completedProgress: 318 iterations completedProgress: 319 iterations completedProgress: 320 iterations completedProgress: 321 iterations completedProgress: 322 iterations completedProgress: 323 iterations completedProgress: 324 iterations completedProgress: 325 iterations completedProgress: 326 iterations completedProgress: 327 iterations completedProgress: 328 iterations completedProgress: 329 iterations completedProgress: 330 iterations completedProgress: 331 iterations completedProgress: 332 iterations completedProgress: 333 iterations completedProgress: 334 iterations completedProgress: 335 iterations completedProgress: 336 iterations completedProgress: 337 iterations completedProgress: 338 iterations completedProgress: 339 iterations completedProgress: 340 iterations completedProgress: 341 iterations completedProgress: 342 iterations completedProgress: 343 iterations completedProgress: 344 iterations completedProgress: 345 iterations completedProgress: 346 iterations completedProgress: 347 iterations completedProgress: 348 iterations completedProgress: 349 iterations completedProgress: 350 iterations completedProgress: 351 iterations completedProgress: 352 iterations completedProgress: 353 iterations completedProgress: 354 iterations completedProgress: 355 iterations completedProgress: 356 iterations completedProgress: 357 iterations completedProgress: 358 iterations completedProgress: 359 iterations completedProgress: 360 iterations completedProgress: 361 iterations completedProgress: 362 iterations completedProgress: 363 iterations completedProgress: 364 iterations completedProgress: 365 iterations completedProgress: 366 iterations completedProgress: 367 iterations completedProgress: 368 iterations completedProgress: 369 iterations completedProgress: 370 iterations completedProgress: 371 iterations completedProgress: 372 iterations completedProgress: 373 iterations completedProgress: 374 iterations completedProgress: 375 iterations completedProgress: 376 iterations completedProgress: 377 iterations completedProgress: 378 iterations completedProgress: 379 iterations completedProgress: 380 iterations completedProgress: 381 iterations completedProgress: 382 iterations completedProgress: 383 iterations completedProgress: 384 iterations completedProgress: 385 iterations completedProgress: 386 iterations completedProgress: 387 iterations completedProgress: 388 iterations completedProgress: 389 iterations completedProgress: 390 iterations completedProgress: 391 iterations completedProgress: 392 iterations completedProgress: 393 iterations completedProgress: 394 iterations completedProgress: 395 iterations completedProgress: 396 iterations completedProgress: 397 iterations completedProgress: 398 iterations completedProgress: 399 iterations completedProgress: 400 iterations completedProgress: 401 iterations completedProgress: 402 iterations completedProgress: 403 iterations completedProgress: 404 iterations completedProgress: 405 iterations completedProgress: 406 iterations completedProgress: 407 iterations completedProgress: 408 iterations completedProgress: 409 iterations completedProgress: 410 iterations completedProgress: 411 iterations completedProgress: 412 iterations completedProgress: 413 iterations completedProgress: 414 iterations completedProgress: 415 iterations completedProgress: 416 iterations completedProgress: 417 iterations completedProgress: 418 iterations completedProgress: 419 iterations completedProgress: 420 iterations completedProgress: 421 iterations completedProgress: 422 iterations completedProgress: 423 iterations completedProgress: 424 iterations completedProgress: 425 iterations completedProgress: 426 iterations completedProgress: 427 iterations completedProgress: 428 iterations completedProgress: 429 iterations completedProgress: 430 iterations completedProgress: 431 iterations completedProgress: 432 iterations completedProgress: 433 iterations completedProgress: 434 iterations completedProgress: 435 iterations completedProgress: 436 iterations completedProgress: 437 iterations completedProgress: 438 iterations completedProgress: 439 iterations completedProgress: 440 iterations completedProgress: 441 iterations completedProgress: 442 iterations completedProgress: 443 iterations completedProgress: 444 iterations completedProgress: 445 iterations completedProgress: 446 iterations completedProgress: 447 iterations completedProgress: 448 iterations completedProgress: 449 iterations completedProgress: 450 iterations completedProgress: 451 iterations completedProgress: 452 iterations completedProgress: 453 iterations completedProgress: 454 iterations completedProgress: 455 iterations completedProgress: 456 iterations completedProgress: 457 iterations completedProgress: 458 iterations completedProgress: 459 iterations completedProgress: 460 iterations completedProgress: 461 iterations completedProgress: 462 iterations completedProgress: 463 iterations completedProgress: 464 iterations completedProgress: 465 iterations completedProgress: 466 iterations completedProgress: 467 iterations completedProgress: 468 iterations completedProgress: 469 iterations completedProgress: 470 iterations completedProgress: 471 iterations completedProgress: 472 iterations completedProgress: 473 iterations completedProgress: 474 iterations completedProgress: 475 iterations completedProgress: 476 iterations completedProgress: 477 iterations completedProgress: 478 iterations completedProgress: 479 iterations completedProgress: 480 iterations completedProgress: 481 iterations completedProgress: 482 iterations completedProgress: 483 iterations completedProgress: 484 iterations completedProgress: 485 iterations completedProgress: 486 iterations completedProgress: 487 iterations completedProgress: 488 iterations completedProgress: 489 iterations completedProgress: 490 iterations completedProgress: 491 iterations completedProgress: 492 iterations completedProgress: 493 iterations completedProgress: 494 iterations completedProgress: 495 iterations completedProgress: 496 iterations completedProgress: 497 iterations completedProgress: 498 iterations completedProgress: 499 iterations completedProgress: 500 iterations completedProgress: 501 iterations completedProgress: 502 iterations completedProgress: 503 iterations completedProgress: 504 iterations completedProgress: 505 iterations completedProgress: 506 iterations completedProgress: 507 iterations completedProgress: 508 iterations completedProgress: 509 iterations completedProgress: 510 iterations completedProgress: 511 iterations completedProgress: 512 iterations completedProgress: 513 iterations completedProgress: 514 iterations completedProgress: 515 iterations completedProgress: 516 iterations completedProgress: 517 iterations completedProgress: 518 iterations completedProgress: 519 iterations completedProgress: 520 iterations completedProgress: 521 iterations completedProgress: 522 iterations completedProgress: 523 iterations completedProgress: 524 iterations completedProgress: 525 iterations completedProgress: 526 iterations completedProgress: 527 iterations completedProgress: 528 iterations completedProgress: 529 iterations completedProgress: 530 iterations completedProgress: 531 iterations completedProgress: 532 iterations completedProgress: 533 iterations completedProgress: 534 iterations completedProgress: 535 iterations completedProgress: 536 iterations completedProgress: 537 iterations completedProgress: 538 iterations completedProgress: 539 iterations completedProgress: 540 iterations completedProgress: 541 iterations completedProgress: 542 iterations completedProgress: 543 iterations completedProgress: 544 iterations completedProgress: 545 iterations completedProgress: 546 iterations completedProgress: 547 iterations completedProgress: 548 iterations completedProgress: 549 iterations completedProgress: 550 iterations completedProgress: 551 iterations completedProgress: 552 iterations completedProgress: 553 iterations completedProgress: 554 iterations completedProgress: 555 iterations completedProgress: 556 iterations completedProgress: 557 iterations completedProgress: 558 iterations completedProgress: 559 iterations completedProgress: 560 iterations completedProgress: 561 iterations completedProgress: 562 iterations completedProgress: 563 iterations completedProgress: 564 iterations completedProgress: 565 iterations completedProgress: 566 iterations completedProgress: 567 iterations completedProgress: 568 iterations completedProgress: 569 iterations completedProgress: 570 iterations completedProgress: 571 iterations completedProgress: 572 iterations completedProgress: 573 iterations completedProgress: 574 iterations completedProgress: 575 iterations completedProgress: 576 iterations completedProgress: 577 iterations completedProgress: 578 iterations completedProgress: 579 iterations completedProgress: 580 iterations completedProgress: 581 iterations completedProgress: 582 iterations completedProgress: 583 iterations completedProgress: 584 iterations completedProgress: 585 iterations completedProgress: 586 iterations completedProgress: 587 iterations completedProgress: 588 iterations completedProgress: 589 iterations completedProgress: 590 iterations completedProgress: 591 iterations completedProgress: 592 iterations completedProgress: 593 iterations completedProgress: 594 iterations completedProgress: 595 iterations completedProgress: 596 iterations completedProgress: 597 iterations completedProgress: 598 iterations completedProgress: 599 iterations completedProgress: 600 iterations completedProgress: 601 iterations completedProgress: 602 iterations completedProgress: 603 iterations completedProgress: 604 iterations completedProgress: 605 iterations completedProgress: 606 iterations completedProgress: 607 iterations completedProgress: 608 iterations completedProgress: 609 iterations completedProgress: 610 iterations completedProgress: 611 iterations completedProgress: 612 iterations completedProgress: 613 iterations completedProgress: 614 iterations completedProgress: 615 iterations completedProgress: 616 iterations completedProgress: 617 iterations completedProgress: 618 iterations completedProgress: 619 iterations completedProgress: 620 iterations completedProgress: 621 iterations completedProgress: 622 iterations completedProgress: 623 iterations completedProgress: 624 iterations completedProgress: 625 iterations completedProgress: 626 iterations completedProgress: 627 iterations completedProgress: 628 iterations completedProgress: 629 iterations completedProgress: 630 iterations completedProgress: 631 iterations completedProgress: 632 iterations completedProgress: 633 iterations completedProgress: 634 iterations completedProgress: 635 iterations completedProgress: 636 iterations completedProgress: 637 iterations completedProgress: 638 iterations completedProgress: 639 iterations completedProgress: 640 iterations completedProgress: 641 iterations completedProgress: 642 iterations completedProgress: 643 iterations completedProgress: 644 iterations completedProgress: 645 iterations completedProgress: 646 iterations completedProgress: 647 iterations completedProgress: 648 iterations completedProgress: 649 iterations completedProgress: 650 iterations completedProgress: 651 iterations completedProgress: 652 iterations completedProgress: 653 iterations completedProgress: 654 iterations completedProgress: 655 iterations completedProgress: 656 iterations completedProgress: 657 iterations completedProgress: 658 iterations completedProgress: 659 iterations completedProgress: 660 iterations completedProgress: 661 iterations completedProgress: 662 iterations completedProgress: 663 iterations completedProgress: 664 iterations completedProgress: 665 iterations completedProgress: 666 iterations completedProgress: 667 iterations completedProgress: 668 iterations completedProgress: 669 iterations completedProgress: 670 iterations completedProgress: 671 iterations completedProgress: 672 iterations completedProgress: 673 iterations completedProgress: 674 iterations completedProgress: 675 iterations completedProgress: 676 iterations completedProgress: 677 iterations completedProgress: 678 iterations completedProgress: 679 iterations completedProgress: 680 iterations completedProgress: 681 iterations completedProgress: 682 iterations completedProgress: 683 iterations completedProgress: 684 iterations completedProgress: 685 iterations completedProgress: 686 iterations completedProgress: 687 iterations completedProgress: 688 iterations completedProgress: 689 iterations completedProgress: 690 iterations completedProgress: 691 iterations completedProgress: 692 iterations completedProgress: 693 iterations completedProgress: 694 iterations completedProgress: 695 iterations completedProgress: 696 iterations completedProgress: 697 iterations completedProgress: 698 iterations completedProgress: 699 iterations completedProgress: 700 iterations completedProgress: 701 iterations completedProgress: 702 iterations completedProgress: 703 iterations completedProgress: 704 iterations completedProgress: 705 iterations completedProgress: 706 iterations completedProgress: 707 iterations completedProgress: 708 iterations completedProgress: 709 iterations completedProgress: 710 iterations completedProgress: 711 iterations completedProgress: 712 iterations completedProgress: 713 iterations completedProgress: 714 iterations completedProgress: 715 iterations completedProgress: 716 iterations completedProgress: 717 iterations completedProgress: 718 iterations completedProgress: 719 iterations completedProgress: 720 iterations completedProgress: 721 iterations completedProgress: 722 iterations completedProgress: 723 iterations completedProgress: 724 iterations completedProgress: 725 iterations completedProgress: 726 iterations completedProgress: 727 iterations completedProgress: 728 iterations completedProgress: 729 iterations completedProgress: 730 iterations completedProgress: 731 iterations completedProgress: 732 iterations completedProgress: 733 iterations completedProgress: 734 iterations completedProgress: 735 iterations completedProgress: 736 iterations completedProgress: 737 iterations completedProgress: 738 iterations completedProgress: 739 iterations completedProgress: 740 iterations completedProgress: 741 iterations completedProgress: 742 iterations completedProgress: 743 iterations completedProgress: 744 iterations completedProgress: 745 iterations completedProgress: 746 iterations completedProgress: 747 iterations completedProgress: 748 iterations completedProgress: 749 iterations completedProgress: 750 iterations completedProgress: 751 iterations completedProgress: 752 iterations completedProgress: 753 iterations completedProgress: 754 iterations completedProgress: 755 iterations completedProgress: 756 iterations completedProgress: 757 iterations completedProgress: 758 iterations completedProgress: 759 iterations completedProgress: 760 iterations completedProgress: 761 iterations completedProgress: 762 iterations completedProgress: 763 iterations completedProgress: 764 iterations completedProgress: 765 iterations completedProgress: 766 iterations completedProgress: 767 iterations completedProgress: 768 iterations completedProgress: 769 iterations completedProgress: 770 iterations completedProgress: 771 iterations completedProgress: 772 iterations completedProgress: 773 iterations completedProgress: 774 iterations completedProgress: 775 iterations completedProgress: 776 iterations completedProgress: 777 iterations completedProgress: 778 iterations completedProgress: 779 iterations completedProgress: 780 iterations completedProgress: 781 iterations completedProgress: 782 iterations completedProgress: 783 iterations completedProgress: 784 iterations completedProgress: 785 iterations completedProgress: 786 iterations completedProgress: 787 iterations completedProgress: 788 iterations completedProgress: 789 iterations completedProgress: 790 iterations completedProgress: 791 iterations completedProgress: 792 iterations completedProgress: 793 iterations completedProgress: 794 iterations completedProgress: 795 iterations completedProgress: 796 iterations completedProgress: 797 iterations completedProgress: 798 iterations completedProgress: 799 iterations completedProgress: 800 iterations completedProgress: 801 iterations completedProgress: 802 iterations completedProgress: 803 iterations completedProgress: 804 iterations completedProgress: 805 iterations completedProgress: 806 iterations completedProgress: 807 iterations completedProgress: 808 iterations completedProgress: 809 iterations completedProgress: 810 iterations completedProgress: 811 iterations completedProgress: 812 iterations completedProgress: 813 iterations completedProgress: 814 iterations completedProgress: 815 iterations completedProgress: 816 iterations completedProgress: 817 iterations completedProgress: 818 iterations completedProgress: 819 iterations completedProgress: 820 iterations completedProgress: 821 iterations completedProgress: 822 iterations completedProgress: 823 iterations completedProgress: 824 iterations completedProgress: 825 iterations completedProgress: 826 iterations completedProgress: 827 iterations completedProgress: 828 iterations completedProgress: 829 iterations completedProgress: 830 iterations completedProgress: 831 iterations completedProgress: 832 iterations completedProgress: 833 iterations completedProgress: 834 iterations completedProgress: 835 iterations completedProgress: 836 iterations completedProgress: 837 iterations completedProgress: 838 iterations completedProgress: 839 iterations completedProgress: 840 iterations completedProgress: 841 iterations completedProgress: 842 iterations completedProgress: 843 iterations completedProgress: 844 iterations completedProgress: 845 iterations completedProgress: 846 iterations completedProgress: 847 iterations completedProgress: 848 iterations completedProgress: 849 iterations completedProgress: 850 iterations completedProgress: 851 iterations completedProgress: 852 iterations completedProgress: 853 iterations completedProgress: 854 iterations completedProgress: 855 iterations completedProgress: 856 iterations completedProgress: 857 iterations completedProgress: 858 iterations completedProgress: 859 iterations completedProgress: 860 iterations completedProgress: 861 iterations completedProgress: 862 iterations completedProgress: 863 iterations completedProgress: 864 iterations completedProgress: 865 iterations completedProgress: 866 iterations completedProgress: 867 iterations completedProgress: 868 iterations completedProgress: 869 iterations completedProgress: 870 iterations completedProgress: 871 iterations completedProgress: 872 iterations completedProgress: 873 iterations completedProgress: 874 iterations completedProgress: 875 iterations completedProgress: 876 iterations completedProgress: 877 iterations completedProgress: 878 iterations completedProgress: 879 iterations completedProgress: 880 iterations completedProgress: 881 iterations completedProgress: 882 iterations completedProgress: 883 iterations completedProgress: 884 iterations completedProgress: 885 iterations completedProgress: 886 iterations completedProgress: 887 iterations completedProgress: 888 iterations completedProgress: 889 iterations completedProgress: 890 iterations completedProgress: 891 iterations completedProgress: 892 iterations completedProgress: 893 iterations completedProgress: 894 iterations completedProgress: 895 iterations completedProgress: 896 iterations completedProgress: 897 iterations completedProgress: 898 iterations completedProgress: 899 iterations completedProgress: 900 iterations completedProgress: 901 iterations completedProgress: 902 iterations completedProgress: 903 iterations completedProgress: 904 iterations completedProgress: 905 iterations completedProgress: 906 iterations completedProgress: 907 iterations completedProgress: 908 iterations completedProgress: 909 iterations completedProgress: 910 iterations completedProgress: 911 iterations completedProgress: 912 iterations completedProgress: 913 iterations completedProgress: 914 iterations completedProgress: 915 iterations completedProgress: 916 iterations completedProgress: 917 iterations completedProgress: 918 iterations completedProgress: 919 iterations completedProgress: 920 iterations completedProgress: 921 iterations completedProgress: 922 iterations completedProgress: 923 iterations completedProgress: 924 iterations completedProgress: 925 iterations completedProgress: 926 iterations completedProgress: 927 iterations completedProgress: 928 iterations completedProgress: 929 iterations completedProgress: 930 iterations completedProgress: 931 iterations completedProgress: 932 iterations completedProgress: 933 iterations completedProgress: 934 iterations completedProgress: 935 iterations completedProgress: 936 iterations completedProgress: 937 iterations completedProgress: 938 iterations completedProgress: 939 iterations completedProgress: 940 iterations completedProgress: 941 iterations completedProgress: 942 iterations completedProgress: 943 iterations completedProgress: 944 iterations completedProgress: 945 iterations completedProgress: 946 iterations completedProgress: 947 iterations completedProgress: 948 iterations completedProgress: 949 iterations completedProgress: 950 iterations completedProgress: 951 iterations completedProgress: 952 iterations completedProgress: 953 iterations completedProgress: 954 iterations completedProgress: 955 iterations completedProgress: 956 iterations completedProgress: 957 iterations completedProgress: 958 iterations completedProgress: 959 iterations completedProgress: 960 iterations completedProgress: 961 iterations completedProgress: 962 iterations completedProgress: 963 iterations completedProgress: 964 iterations completedProgress: 965 iterations completedProgress: 966 iterations completedProgress: 967 iterations completedProgress: 968 iterations completedProgress: 969 iterations completedProgress: 970 iterations completedProgress: 971 iterations completedProgress: 972 iterations completedProgress: 973 iterations completedProgress: 974 iterations completedProgress: 975 iterations completedProgress: 976 iterations completedProgress: 977 iterations completedProgress: 978 iterations completedProgress: 979 iterations completedProgress: 980 iterations completedProgress: 981 iterations completedProgress: 982 iterations completedProgress: 983 iterations completedProgress: 984 iterations completedProgress: 985 iterations completedProgress: 986 iterations completedProgress: 987 iterations completedProgress: 988 iterations completedProgress: 989 iterations completedProgress: 990 iterations completedProgress: 991 iterations completedProgress: 992 iterations completedProgress: 993 iterations completedProgress: 994 iterations completedProgress: 995 iterations completedProgress: 996 iterations completedProgress: 997 iterations completedProgress: 998 iterations completedProgress: 999 iterations completedProgress: 1000 iterations completed # Calculate mean RMSPE for the ensemble with all four models mean_rmspe_ensemble_four &lt;- mean(rmspe_ensemble_four) # Print the results print(paste(&quot;Mean RMSPE of Stacked Ensemble with All Four Models:&quot;, mean_rmspe_ensemble_four)) ## [1] &quot;Mean RMSPE of Stacked Ensemble with All Four Models: 0.33885977131557&quot; So we do see a better rmspe probably due to having more base models. so lets look at the covariance matrix of the base models to see if they are correlated # Initialize the matrix to accumulate correlations cor_matrix_sum &lt;- matrix(0, nrow = 4, ncol = 4) for(i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) # Train both GBM models on the bootstrapped training data base_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = 100, interaction.depth = 1, verbose = FALSE) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) base_gbm_pred &lt;- predict(base_gbm_mod, test_data, n.trees = 100) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, base_gbm_pred, tuned_gbm_pred) # Calculate the correlation matrix for the current iteration cor_matrix &lt;- cor(preds_four) # Accumulate the correlation matrices cor_matrix_sum &lt;- cor_matrix_sum + cor_matrix } ## Progress: 1 iterations completedProgress: 2 iterations completedProgress: 3 iterations completedProgress: 4 iterations completedProgress: 5 iterations completedProgress: 6 iterations completedProgress: 7 iterations completedProgress: 8 iterations completedProgress: 9 iterations completedProgress: 10 iterations completedProgress: 11 iterations completedProgress: 12 iterations completedProgress: 13 iterations completedProgress: 14 iterations completedProgress: 15 iterations completedProgress: 16 iterations completedProgress: 17 iterations completedProgress: 18 iterations completedProgress: 19 iterations completedProgress: 20 iterations completedProgress: 21 iterations completedProgress: 22 iterations completedProgress: 23 iterations completedProgress: 24 iterations completedProgress: 25 iterations completedProgress: 26 iterations completedProgress: 27 iterations completedProgress: 28 iterations completedProgress: 29 iterations completedProgress: 30 iterations completedProgress: 31 iterations completedProgress: 32 iterations completedProgress: 33 iterations completedProgress: 34 iterations completedProgress: 35 iterations completedProgress: 36 iterations completedProgress: 37 iterations completedProgress: 38 iterations completedProgress: 39 iterations completedProgress: 40 iterations completedProgress: 41 iterations completedProgress: 42 iterations completedProgress: 43 iterations completedProgress: 44 iterations completedProgress: 45 iterations completedProgress: 46 iterations completedProgress: 47 iterations completedProgress: 48 iterations completedProgress: 49 iterations completedProgress: 50 iterations completedProgress: 51 iterations completedProgress: 52 iterations completedProgress: 53 iterations completedProgress: 54 iterations completedProgress: 55 iterations completedProgress: 56 iterations completedProgress: 57 iterations completedProgress: 58 iterations completedProgress: 59 iterations completedProgress: 60 iterations completedProgress: 61 iterations completedProgress: 62 iterations completedProgress: 63 iterations completedProgress: 64 iterations completedProgress: 65 iterations completedProgress: 66 iterations completedProgress: 67 iterations completedProgress: 68 iterations completedProgress: 69 iterations completedProgress: 70 iterations completedProgress: 71 iterations completedProgress: 72 iterations completedProgress: 73 iterations completedProgress: 74 iterations completedProgress: 75 iterations completedProgress: 76 iterations completedProgress: 77 iterations completedProgress: 78 iterations completedProgress: 79 iterations completedProgress: 80 iterations completedProgress: 81 iterations completedProgress: 82 iterations completedProgress: 83 iterations completedProgress: 84 iterations completedProgress: 85 iterations completedProgress: 86 iterations completedProgress: 87 iterations completedProgress: 88 iterations completedProgress: 89 iterations completedProgress: 90 iterations completedProgress: 91 iterations completedProgress: 92 iterations completedProgress: 93 iterations completedProgress: 94 iterations completedProgress: 95 iterations completedProgress: 96 iterations completedProgress: 97 iterations completedProgress: 98 iterations completedProgress: 99 iterations completedProgress: 100 iterations completedProgress: 101 iterations completedProgress: 102 iterations completedProgress: 103 iterations completedProgress: 104 iterations completedProgress: 105 iterations completedProgress: 106 iterations completedProgress: 107 iterations completedProgress: 108 iterations completedProgress: 109 iterations completedProgress: 110 iterations completedProgress: 111 iterations completedProgress: 112 iterations completedProgress: 113 iterations completedProgress: 114 iterations completedProgress: 115 iterations completedProgress: 116 iterations completedProgress: 117 iterations completedProgress: 118 iterations completedProgress: 119 iterations completedProgress: 120 iterations completedProgress: 121 iterations completedProgress: 122 iterations completedProgress: 123 iterations completedProgress: 124 iterations completedProgress: 125 iterations completedProgress: 126 iterations completedProgress: 127 iterations completedProgress: 128 iterations completedProgress: 129 iterations completedProgress: 130 iterations completedProgress: 131 iterations completedProgress: 132 iterations completedProgress: 133 iterations completedProgress: 134 iterations completedProgress: 135 iterations completedProgress: 136 iterations completedProgress: 137 iterations completedProgress: 138 iterations completedProgress: 139 iterations completedProgress: 140 iterations completedProgress: 141 iterations completedProgress: 142 iterations completedProgress: 143 iterations completedProgress: 144 iterations completedProgress: 145 iterations completedProgress: 146 iterations completedProgress: 147 iterations completedProgress: 148 iterations completedProgress: 149 iterations completedProgress: 150 iterations completedProgress: 151 iterations completedProgress: 152 iterations completedProgress: 153 iterations completedProgress: 154 iterations completedProgress: 155 iterations completedProgress: 156 iterations completedProgress: 157 iterations completedProgress: 158 iterations completedProgress: 159 iterations completedProgress: 160 iterations completedProgress: 161 iterations completedProgress: 162 iterations completedProgress: 163 iterations completedProgress: 164 iterations completedProgress: 165 iterations completedProgress: 166 iterations completedProgress: 167 iterations completedProgress: 168 iterations completedProgress: 169 iterations completedProgress: 170 iterations completedProgress: 171 iterations completedProgress: 172 iterations completedProgress: 173 iterations completedProgress: 174 iterations completedProgress: 175 iterations completedProgress: 176 iterations completedProgress: 177 iterations completedProgress: 178 iterations completedProgress: 179 iterations completedProgress: 180 iterations completedProgress: 181 iterations completedProgress: 182 iterations completedProgress: 183 iterations completedProgress: 184 iterations completedProgress: 185 iterations completedProgress: 186 iterations completedProgress: 187 iterations completedProgress: 188 iterations completedProgress: 189 iterations completedProgress: 190 iterations completedProgress: 191 iterations completedProgress: 192 iterations completedProgress: 193 iterations completedProgress: 194 iterations completedProgress: 195 iterations completedProgress: 196 iterations completedProgress: 197 iterations completedProgress: 198 iterations completedProgress: 199 iterations completedProgress: 200 iterations completedProgress: 201 iterations completedProgress: 202 iterations completedProgress: 203 iterations completedProgress: 204 iterations completedProgress: 205 iterations completedProgress: 206 iterations completedProgress: 207 iterations completedProgress: 208 iterations completedProgress: 209 iterations completedProgress: 210 iterations completedProgress: 211 iterations completedProgress: 212 iterations completedProgress: 213 iterations completedProgress: 214 iterations completedProgress: 215 iterations completedProgress: 216 iterations completedProgress: 217 iterations completedProgress: 218 iterations completedProgress: 219 iterations completedProgress: 220 iterations completedProgress: 221 iterations completedProgress: 222 iterations completedProgress: 223 iterations completedProgress: 224 iterations completedProgress: 225 iterations completedProgress: 226 iterations completedProgress: 227 iterations completedProgress: 228 iterations completedProgress: 229 iterations completedProgress: 230 iterations completedProgress: 231 iterations completedProgress: 232 iterations completedProgress: 233 iterations completedProgress: 234 iterations completedProgress: 235 iterations completedProgress: 236 iterations completedProgress: 237 iterations completedProgress: 238 iterations completedProgress: 239 iterations completedProgress: 240 iterations completedProgress: 241 iterations completedProgress: 242 iterations completedProgress: 243 iterations completedProgress: 244 iterations completedProgress: 245 iterations completedProgress: 246 iterations completedProgress: 247 iterations completedProgress: 248 iterations completedProgress: 249 iterations completedProgress: 250 iterations completedProgress: 251 iterations completedProgress: 252 iterations completedProgress: 253 iterations completedProgress: 254 iterations completedProgress: 255 iterations completedProgress: 256 iterations completedProgress: 257 iterations completedProgress: 258 iterations completedProgress: 259 iterations completedProgress: 260 iterations completedProgress: 261 iterations completedProgress: 262 iterations completedProgress: 263 iterations completedProgress: 264 iterations completedProgress: 265 iterations completedProgress: 266 iterations completedProgress: 267 iterations completedProgress: 268 iterations completedProgress: 269 iterations completedProgress: 270 iterations completedProgress: 271 iterations completedProgress: 272 iterations completedProgress: 273 iterations completedProgress: 274 iterations completedProgress: 275 iterations completedProgress: 276 iterations completedProgress: 277 iterations completedProgress: 278 iterations completedProgress: 279 iterations completedProgress: 280 iterations completedProgress: 281 iterations completedProgress: 282 iterations completedProgress: 283 iterations completedProgress: 284 iterations completedProgress: 285 iterations completedProgress: 286 iterations completedProgress: 287 iterations completedProgress: 288 iterations completedProgress: 289 iterations completedProgress: 290 iterations completedProgress: 291 iterations completedProgress: 292 iterations completedProgress: 293 iterations completedProgress: 294 iterations completedProgress: 295 iterations completedProgress: 296 iterations completedProgress: 297 iterations completedProgress: 298 iterations completedProgress: 299 iterations completedProgress: 300 iterations completedProgress: 301 iterations completedProgress: 302 iterations completedProgress: 303 iterations completedProgress: 304 iterations completedProgress: 305 iterations completedProgress: 306 iterations completedProgress: 307 iterations completedProgress: 308 iterations completedProgress: 309 iterations completedProgress: 310 iterations completedProgress: 311 iterations completedProgress: 312 iterations completedProgress: 313 iterations completedProgress: 314 iterations completedProgress: 315 iterations completedProgress: 316 iterations completedProgress: 317 iterations completedProgress: 318 iterations completedProgress: 319 iterations completedProgress: 320 iterations completedProgress: 321 iterations completedProgress: 322 iterations completedProgress: 323 iterations completedProgress: 324 iterations completedProgress: 325 iterations completedProgress: 326 iterations completedProgress: 327 iterations completedProgress: 328 iterations completedProgress: 329 iterations completedProgress: 330 iterations completedProgress: 331 iterations completedProgress: 332 iterations completedProgress: 333 iterations completedProgress: 334 iterations completedProgress: 335 iterations completedProgress: 336 iterations completedProgress: 337 iterations completedProgress: 338 iterations completedProgress: 339 iterations completedProgress: 340 iterations completedProgress: 341 iterations completedProgress: 342 iterations completedProgress: 343 iterations completedProgress: 344 iterations completedProgress: 345 iterations completedProgress: 346 iterations completedProgress: 347 iterations completedProgress: 348 iterations completedProgress: 349 iterations completedProgress: 350 iterations completedProgress: 351 iterations completedProgress: 352 iterations completedProgress: 353 iterations completedProgress: 354 iterations completedProgress: 355 iterations completedProgress: 356 iterations completedProgress: 357 iterations completedProgress: 358 iterations completedProgress: 359 iterations completedProgress: 360 iterations completedProgress: 361 iterations completedProgress: 362 iterations completedProgress: 363 iterations completedProgress: 364 iterations completedProgress: 365 iterations completedProgress: 366 iterations completedProgress: 367 iterations completedProgress: 368 iterations completedProgress: 369 iterations completedProgress: 370 iterations completedProgress: 371 iterations completedProgress: 372 iterations completedProgress: 373 iterations completedProgress: 374 iterations completedProgress: 375 iterations completedProgress: 376 iterations completedProgress: 377 iterations completedProgress: 378 iterations completedProgress: 379 iterations completedProgress: 380 iterations completedProgress: 381 iterations completedProgress: 382 iterations completedProgress: 383 iterations completedProgress: 384 iterations completedProgress: 385 iterations completedProgress: 386 iterations completedProgress: 387 iterations completedProgress: 388 iterations completedProgress: 389 iterations completedProgress: 390 iterations completedProgress: 391 iterations completedProgress: 392 iterations completedProgress: 393 iterations completedProgress: 394 iterations completedProgress: 395 iterations completedProgress: 396 iterations completedProgress: 397 iterations completedProgress: 398 iterations completedProgress: 399 iterations completedProgress: 400 iterations completedProgress: 401 iterations completedProgress: 402 iterations completedProgress: 403 iterations completedProgress: 404 iterations completedProgress: 405 iterations completedProgress: 406 iterations completedProgress: 407 iterations completedProgress: 408 iterations completedProgress: 409 iterations completedProgress: 410 iterations completedProgress: 411 iterations completedProgress: 412 iterations completedProgress: 413 iterations completedProgress: 414 iterations completedProgress: 415 iterations completedProgress: 416 iterations completedProgress: 417 iterations completedProgress: 418 iterations completedProgress: 419 iterations completedProgress: 420 iterations completedProgress: 421 iterations completedProgress: 422 iterations completedProgress: 423 iterations completedProgress: 424 iterations completedProgress: 425 iterations completedProgress: 426 iterations completedProgress: 427 iterations completedProgress: 428 iterations completedProgress: 429 iterations completedProgress: 430 iterations completedProgress: 431 iterations completedProgress: 432 iterations completedProgress: 433 iterations completedProgress: 434 iterations completedProgress: 435 iterations completedProgress: 436 iterations completedProgress: 437 iterations completedProgress: 438 iterations completedProgress: 439 iterations completedProgress: 440 iterations completedProgress: 441 iterations completedProgress: 442 iterations completedProgress: 443 iterations completedProgress: 444 iterations completedProgress: 445 iterations completedProgress: 446 iterations completedProgress: 447 iterations completedProgress: 448 iterations completedProgress: 449 iterations completedProgress: 450 iterations completedProgress: 451 iterations completedProgress: 452 iterations completedProgress: 453 iterations completedProgress: 454 iterations completedProgress: 455 iterations completedProgress: 456 iterations completedProgress: 457 iterations completedProgress: 458 iterations completedProgress: 459 iterations completedProgress: 460 iterations completedProgress: 461 iterations completedProgress: 462 iterations completedProgress: 463 iterations completedProgress: 464 iterations completedProgress: 465 iterations completedProgress: 466 iterations completedProgress: 467 iterations completedProgress: 468 iterations completedProgress: 469 iterations completedProgress: 470 iterations completedProgress: 471 iterations completedProgress: 472 iterations completedProgress: 473 iterations completedProgress: 474 iterations completedProgress: 475 iterations completedProgress: 476 iterations completedProgress: 477 iterations completedProgress: 478 iterations completedProgress: 479 iterations completedProgress: 480 iterations completedProgress: 481 iterations completedProgress: 482 iterations completedProgress: 483 iterations completedProgress: 484 iterations completedProgress: 485 iterations completedProgress: 486 iterations completedProgress: 487 iterations completedProgress: 488 iterations completedProgress: 489 iterations completedProgress: 490 iterations completedProgress: 491 iterations completedProgress: 492 iterations completedProgress: 493 iterations completedProgress: 494 iterations completedProgress: 495 iterations completedProgress: 496 iterations completedProgress: 497 iterations completedProgress: 498 iterations completedProgress: 499 iterations completedProgress: 500 iterations completedProgress: 501 iterations completedProgress: 502 iterations completedProgress: 503 iterations completedProgress: 504 iterations completedProgress: 505 iterations completedProgress: 506 iterations completedProgress: 507 iterations completedProgress: 508 iterations completedProgress: 509 iterations completedProgress: 510 iterations completedProgress: 511 iterations completedProgress: 512 iterations completedProgress: 513 iterations completedProgress: 514 iterations completedProgress: 515 iterations completedProgress: 516 iterations completedProgress: 517 iterations completedProgress: 518 iterations completedProgress: 519 iterations completedProgress: 520 iterations completedProgress: 521 iterations completedProgress: 522 iterations completedProgress: 523 iterations completedProgress: 524 iterations completedProgress: 525 iterations completedProgress: 526 iterations completedProgress: 527 iterations completedProgress: 528 iterations completedProgress: 529 iterations completedProgress: 530 iterations completedProgress: 531 iterations completedProgress: 532 iterations completedProgress: 533 iterations completedProgress: 534 iterations completedProgress: 535 iterations completedProgress: 536 iterations completedProgress: 537 iterations completedProgress: 538 iterations completedProgress: 539 iterations completedProgress: 540 iterations completedProgress: 541 iterations completedProgress: 542 iterations completedProgress: 543 iterations completedProgress: 544 iterations completedProgress: 545 iterations completedProgress: 546 iterations completedProgress: 547 iterations completedProgress: 548 iterations completedProgress: 549 iterations completedProgress: 550 iterations completedProgress: 551 iterations completedProgress: 552 iterations completedProgress: 553 iterations completedProgress: 554 iterations completedProgress: 555 iterations completedProgress: 556 iterations completedProgress: 557 iterations completedProgress: 558 iterations completedProgress: 559 iterations completedProgress: 560 iterations completedProgress: 561 iterations completedProgress: 562 iterations completedProgress: 563 iterations completedProgress: 564 iterations completedProgress: 565 iterations completedProgress: 566 iterations completedProgress: 567 iterations completedProgress: 568 iterations completedProgress: 569 iterations completedProgress: 570 iterations completedProgress: 571 iterations completedProgress: 572 iterations completedProgress: 573 iterations completedProgress: 574 iterations completedProgress: 575 iterations completedProgress: 576 iterations completedProgress: 577 iterations completedProgress: 578 iterations completedProgress: 579 iterations completedProgress: 580 iterations completedProgress: 581 iterations completedProgress: 582 iterations completedProgress: 583 iterations completedProgress: 584 iterations completedProgress: 585 iterations completedProgress: 586 iterations completedProgress: 587 iterations completedProgress: 588 iterations completedProgress: 589 iterations completedProgress: 590 iterations completedProgress: 591 iterations completedProgress: 592 iterations completedProgress: 593 iterations completedProgress: 594 iterations completedProgress: 595 iterations completedProgress: 596 iterations completedProgress: 597 iterations completedProgress: 598 iterations completedProgress: 599 iterations completedProgress: 600 iterations completedProgress: 601 iterations completedProgress: 602 iterations completedProgress: 603 iterations completedProgress: 604 iterations completedProgress: 605 iterations completedProgress: 606 iterations completedProgress: 607 iterations completedProgress: 608 iterations completedProgress: 609 iterations completedProgress: 610 iterations completedProgress: 611 iterations completedProgress: 612 iterations completedProgress: 613 iterations completedProgress: 614 iterations completedProgress: 615 iterations completedProgress: 616 iterations completedProgress: 617 iterations completedProgress: 618 iterations completedProgress: 619 iterations completedProgress: 620 iterations completedProgress: 621 iterations completedProgress: 622 iterations completedProgress: 623 iterations completedProgress: 624 iterations completedProgress: 625 iterations completedProgress: 626 iterations completedProgress: 627 iterations completedProgress: 628 iterations completedProgress: 629 iterations completedProgress: 630 iterations completedProgress: 631 iterations completedProgress: 632 iterations completedProgress: 633 iterations completedProgress: 634 iterations completedProgress: 635 iterations completedProgress: 636 iterations completedProgress: 637 iterations completedProgress: 638 iterations completedProgress: 639 iterations completedProgress: 640 iterations completedProgress: 641 iterations completedProgress: 642 iterations completedProgress: 643 iterations completedProgress: 644 iterations completedProgress: 645 iterations completedProgress: 646 iterations completedProgress: 647 iterations completedProgress: 648 iterations completedProgress: 649 iterations completedProgress: 650 iterations completedProgress: 651 iterations completedProgress: 652 iterations completedProgress: 653 iterations completedProgress: 654 iterations completedProgress: 655 iterations completedProgress: 656 iterations completedProgress: 657 iterations completedProgress: 658 iterations completedProgress: 659 iterations completedProgress: 660 iterations completedProgress: 661 iterations completedProgress: 662 iterations completedProgress: 663 iterations completedProgress: 664 iterations completedProgress: 665 iterations completedProgress: 666 iterations completedProgress: 667 iterations completedProgress: 668 iterations completedProgress: 669 iterations completedProgress: 670 iterations completedProgress: 671 iterations completedProgress: 672 iterations completedProgress: 673 iterations completedProgress: 674 iterations completedProgress: 675 iterations completedProgress: 676 iterations completedProgress: 677 iterations completedProgress: 678 iterations completedProgress: 679 iterations completedProgress: 680 iterations completedProgress: 681 iterations completedProgress: 682 iterations completedProgress: 683 iterations completedProgress: 684 iterations completedProgress: 685 iterations completedProgress: 686 iterations completedProgress: 687 iterations completedProgress: 688 iterations completedProgress: 689 iterations completedProgress: 690 iterations completedProgress: 691 iterations completedProgress: 692 iterations completedProgress: 693 iterations completedProgress: 694 iterations completedProgress: 695 iterations completedProgress: 696 iterations completedProgress: 697 iterations completedProgress: 698 iterations completedProgress: 699 iterations completedProgress: 700 iterations completedProgress: 701 iterations completedProgress: 702 iterations completedProgress: 703 iterations completedProgress: 704 iterations completedProgress: 705 iterations completedProgress: 706 iterations completedProgress: 707 iterations completedProgress: 708 iterations completedProgress: 709 iterations completedProgress: 710 iterations completedProgress: 711 iterations completedProgress: 712 iterations completedProgress: 713 iterations completedProgress: 714 iterations completedProgress: 715 iterations completedProgress: 716 iterations completedProgress: 717 iterations completedProgress: 718 iterations completedProgress: 719 iterations completedProgress: 720 iterations completedProgress: 721 iterations completedProgress: 722 iterations completedProgress: 723 iterations completedProgress: 724 iterations completedProgress: 725 iterations completedProgress: 726 iterations completedProgress: 727 iterations completedProgress: 728 iterations completedProgress: 729 iterations completedProgress: 730 iterations completedProgress: 731 iterations completedProgress: 732 iterations completedProgress: 733 iterations completedProgress: 734 iterations completedProgress: 735 iterations completedProgress: 736 iterations completedProgress: 737 iterations completedProgress: 738 iterations completedProgress: 739 iterations completedProgress: 740 iterations completedProgress: 741 iterations completedProgress: 742 iterations completedProgress: 743 iterations completedProgress: 744 iterations completedProgress: 745 iterations completedProgress: 746 iterations completedProgress: 747 iterations completedProgress: 748 iterations completedProgress: 749 iterations completedProgress: 750 iterations completedProgress: 751 iterations completedProgress: 752 iterations completedProgress: 753 iterations completedProgress: 754 iterations completedProgress: 755 iterations completedProgress: 756 iterations completedProgress: 757 iterations completedProgress: 758 iterations completedProgress: 759 iterations completedProgress: 760 iterations completedProgress: 761 iterations completedProgress: 762 iterations completedProgress: 763 iterations completedProgress: 764 iterations completedProgress: 765 iterations completedProgress: 766 iterations completedProgress: 767 iterations completedProgress: 768 iterations completedProgress: 769 iterations completedProgress: 770 iterations completedProgress: 771 iterations completedProgress: 772 iterations completedProgress: 773 iterations completedProgress: 774 iterations completedProgress: 775 iterations completedProgress: 776 iterations completedProgress: 777 iterations completedProgress: 778 iterations completedProgress: 779 iterations completedProgress: 780 iterations completedProgress: 781 iterations completedProgress: 782 iterations completedProgress: 783 iterations completedProgress: 784 iterations completedProgress: 785 iterations completedProgress: 786 iterations completedProgress: 787 iterations completedProgress: 788 iterations completedProgress: 789 iterations completedProgress: 790 iterations completedProgress: 791 iterations completedProgress: 792 iterations completedProgress: 793 iterations completedProgress: 794 iterations completedProgress: 795 iterations completedProgress: 796 iterations completedProgress: 797 iterations completedProgress: 798 iterations completedProgress: 799 iterations completedProgress: 800 iterations completedProgress: 801 iterations completedProgress: 802 iterations completedProgress: 803 iterations completedProgress: 804 iterations completedProgress: 805 iterations completedProgress: 806 iterations completedProgress: 807 iterations completedProgress: 808 iterations completedProgress: 809 iterations completedProgress: 810 iterations completedProgress: 811 iterations completedProgress: 812 iterations completedProgress: 813 iterations completedProgress: 814 iterations completedProgress: 815 iterations completedProgress: 816 iterations completedProgress: 817 iterations completedProgress: 818 iterations completedProgress: 819 iterations completedProgress: 820 iterations completedProgress: 821 iterations completedProgress: 822 iterations completedProgress: 823 iterations completedProgress: 824 iterations completedProgress: 825 iterations completedProgress: 826 iterations completedProgress: 827 iterations completedProgress: 828 iterations completedProgress: 829 iterations completedProgress: 830 iterations completedProgress: 831 iterations completedProgress: 832 iterations completedProgress: 833 iterations completedProgress: 834 iterations completedProgress: 835 iterations completedProgress: 836 iterations completedProgress: 837 iterations completedProgress: 838 iterations completedProgress: 839 iterations completedProgress: 840 iterations completedProgress: 841 iterations completedProgress: 842 iterations completedProgress: 843 iterations completedProgress: 844 iterations completedProgress: 845 iterations completedProgress: 846 iterations completedProgress: 847 iterations completedProgress: 848 iterations completedProgress: 849 iterations completedProgress: 850 iterations completedProgress: 851 iterations completedProgress: 852 iterations completedProgress: 853 iterations completedProgress: 854 iterations completedProgress: 855 iterations completedProgress: 856 iterations completedProgress: 857 iterations completedProgress: 858 iterations completedProgress: 859 iterations completedProgress: 860 iterations completedProgress: 861 iterations completedProgress: 862 iterations completedProgress: 863 iterations completedProgress: 864 iterations completedProgress: 865 iterations completedProgress: 866 iterations completedProgress: 867 iterations completedProgress: 868 iterations completedProgress: 869 iterations completedProgress: 870 iterations completedProgress: 871 iterations completedProgress: 872 iterations completedProgress: 873 iterations completedProgress: 874 iterations completedProgress: 875 iterations completedProgress: 876 iterations completedProgress: 877 iterations completedProgress: 878 iterations completedProgress: 879 iterations completedProgress: 880 iterations completedProgress: 881 iterations completedProgress: 882 iterations completedProgress: 883 iterations completedProgress: 884 iterations completedProgress: 885 iterations completedProgress: 886 iterations completedProgress: 887 iterations completedProgress: 888 iterations completedProgress: 889 iterations completedProgress: 890 iterations completedProgress: 891 iterations completedProgress: 892 iterations completedProgress: 893 iterations completedProgress: 894 iterations completedProgress: 895 iterations completedProgress: 896 iterations completedProgress: 897 iterations completedProgress: 898 iterations completedProgress: 899 iterations completedProgress: 900 iterations completedProgress: 901 iterations completedProgress: 902 iterations completedProgress: 903 iterations completedProgress: 904 iterations completedProgress: 905 iterations completedProgress: 906 iterations completedProgress: 907 iterations completedProgress: 908 iterations completedProgress: 909 iterations completedProgress: 910 iterations completedProgress: 911 iterations completedProgress: 912 iterations completedProgress: 913 iterations completedProgress: 914 iterations completedProgress: 915 iterations completedProgress: 916 iterations completedProgress: 917 iterations completedProgress: 918 iterations completedProgress: 919 iterations completedProgress: 920 iterations completedProgress: 921 iterations completedProgress: 922 iterations completedProgress: 923 iterations completedProgress: 924 iterations completedProgress: 925 iterations completedProgress: 926 iterations completedProgress: 927 iterations completedProgress: 928 iterations completedProgress: 929 iterations completedProgress: 930 iterations completedProgress: 931 iterations completedProgress: 932 iterations completedProgress: 933 iterations completedProgress: 934 iterations completedProgress: 935 iterations completedProgress: 936 iterations completedProgress: 937 iterations completedProgress: 938 iterations completedProgress: 939 iterations completedProgress: 940 iterations completedProgress: 941 iterations completedProgress: 942 iterations completedProgress: 943 iterations completedProgress: 944 iterations completedProgress: 945 iterations completedProgress: 946 iterations completedProgress: 947 iterations completedProgress: 948 iterations completedProgress: 949 iterations completedProgress: 950 iterations completedProgress: 951 iterations completedProgress: 952 iterations completedProgress: 953 iterations completedProgress: 954 iterations completedProgress: 955 iterations completedProgress: 956 iterations completedProgress: 957 iterations completedProgress: 958 iterations completedProgress: 959 iterations completedProgress: 960 iterations completedProgress: 961 iterations completedProgress: 962 iterations completedProgress: 963 iterations completedProgress: 964 iterations completedProgress: 965 iterations completedProgress: 966 iterations completedProgress: 967 iterations completedProgress: 968 iterations completedProgress: 969 iterations completedProgress: 970 iterations completedProgress: 971 iterations completedProgress: 972 iterations completedProgress: 973 iterations completedProgress: 974 iterations completedProgress: 975 iterations completedProgress: 976 iterations completedProgress: 977 iterations completedProgress: 978 iterations completedProgress: 979 iterations completedProgress: 980 iterations completedProgress: 981 iterations completedProgress: 982 iterations completedProgress: 983 iterations completedProgress: 984 iterations completedProgress: 985 iterations completedProgress: 986 iterations completedProgress: 987 iterations completedProgress: 988 iterations completedProgress: 989 iterations completedProgress: 990 iterations completedProgress: 991 iterations completedProgress: 992 iterations completedProgress: 993 iterations completedProgress: 994 iterations completedProgress: 995 iterations completedProgress: 996 iterations completedProgress: 997 iterations completedProgress: 998 iterations completedProgress: 999 iterations completedProgress: 1000 iterations completed # Average the correlation matrix over the 1000 iterations avg_cor_matrix &lt;- cor_matrix_sum / 1000 # Print the averaged correlation matrix print(&quot;Averaged Correlation Matrix:&quot;) ## [1] &quot;Averaged Correlation Matrix:&quot; print(avg_cor_matrix) ## lm_pred rf_pred base_gbm_pred tuned_gbm_pred ## lm_pred 1.0000000 0.9010177 0.8883905 0.8928445 ## rf_pred 0.9010177 1.0000000 0.9759521 0.9770704 ## base_gbm_pred 0.8883905 0.9759521 1.0000000 0.9693381 ## tuned_gbm_pred 0.8928445 0.9770704 0.9693381 1.0000000 # Calculate mean RMSPE for the ensemble with all four models mean_rmspe_ensemble_four &lt;- mean(rmspe_ensemble_four) # Print the results print(paste(&quot;Mean RMSPE of Stacked Ensemble with All Four Models:&quot;, mean_rmspe_ensemble_four)) ## [1] &quot;Mean RMSPE of Stacked Ensemble with All Four Models: 0.33885977131557&quot; now what if i use another model that is less correlated to the other models the first knn3 setup to make sure it works It needs to be knnreg… knn3 does not work for regression # Load the required package library(caret) # Sample data (assuming &#39;data&#39; is your dataset and &#39;medv&#39; is the target) ind &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train &lt;- data[ind, ] test &lt;- data[-ind, ] # Fit the KNN regression model model &lt;- knnreg(medv ~ ., data = train, k = 4) # Predict on the test set predictions &lt;- predict(model, test) # Calculate RMSE rmse &lt;- sqrt(mean((predictions - test$medv)^2)) print(rmse) ## [1] 0.4574346 # Define the grid for k values k_values &lt;- seq(1, 10, by = 1) # Initialize a vector to store the average RMSPE for each k rmspe_k &lt;- numeric(length(k_values)) # Loop over each k value for (j in seq_along(k_values)) { k &lt;- k_values[j] # Initialize a vector to store RMSPE for each bootstrap run rmspe_vals &lt;- numeric(100) for (i in 1:100) { # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train KNN model with current k using knn3 knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = k) #THIS IS HUGE test_pred &lt;- predict(knn_mod, test_data) rmspe_vals[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } # Store the average RMSPE for this k rmspe_k[j] &lt;- mean(rmspe_vals) cat(&quot;\\rProgress:&quot;, j, &quot;/&quot;, length(k_values), &quot;k =&quot;, k, &quot;Average RMSPE:&quot;, rmspe_k[j]) } ## Progress: 1 / 10 k = 1 Average RMSPE: 0.4949892Progress: 2 / 10 k = 2 Average RMSPE: 0.4302938Progress: 3 / 10 k = 3 Average RMSPE: 0.4310261Progress: 4 / 10 k = 4 Average RMSPE: 0.4354456Progress: 5 / 10 k = 5 Average RMSPE: 0.4509741Progress: 6 / 10 k = 6 Average RMSPE: 0.470581Progress: 7 / 10 k = 7 Average RMSPE: 0.4740371Progress: 8 / 10 k = 8 Average RMSPE: 0.4878077Progress: 9 / 10 k = 9 Average RMSPE: 0.496359Progress: 10 / 10 k = 10 Average RMSPE: 0.500633 # Identify the best k with the lowest average RMSPE best_k &lt;- k_values[which.min(rmspe_k)] print(paste(&quot;Best k value is:&quot;, best_k, &quot;with an average RMSPE of:&quot;, min(rmspe_k))) ## [1] &quot;Best k value is: 2 with an average RMSPE of: 0.43029384999798&quot; # Initialize a vector to store RMSPE results for the best k rmspe_best_k &lt;- numeric(100) for (i in 1:100) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Train KNN model with the best k using knn3 knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = best_k) # Predict on the test set test_pred &lt;- predict(knn_mod, test_data) # Calculate RMSPE for the best k rmspe_best_k[i] &lt;- sqrt(mean((test_pred - test_data$medv)^2)) } ## Progress: 1 iterations completedProgress: 2 iterations completedProgress: 3 iterations completedProgress: 4 iterations completedProgress: 5 iterations completedProgress: 6 iterations completedProgress: 7 iterations completedProgress: 8 iterations completedProgress: 9 iterations completedProgress: 10 iterations completedProgress: 11 iterations completedProgress: 12 iterations completedProgress: 13 iterations completedProgress: 14 iterations completedProgress: 15 iterations completedProgress: 16 iterations completedProgress: 17 iterations completedProgress: 18 iterations completedProgress: 19 iterations completedProgress: 20 iterations completedProgress: 21 iterations completedProgress: 22 iterations completedProgress: 23 iterations completedProgress: 24 iterations completedProgress: 25 iterations completedProgress: 26 iterations completedProgress: 27 iterations completedProgress: 28 iterations completedProgress: 29 iterations completedProgress: 30 iterations completedProgress: 31 iterations completedProgress: 32 iterations completedProgress: 33 iterations completedProgress: 34 iterations completedProgress: 35 iterations completedProgress: 36 iterations completedProgress: 37 iterations completedProgress: 38 iterations completedProgress: 39 iterations completedProgress: 40 iterations completedProgress: 41 iterations completedProgress: 42 iterations completedProgress: 43 iterations completedProgress: 44 iterations completedProgress: 45 iterations completedProgress: 46 iterations completedProgress: 47 iterations completedProgress: 48 iterations completedProgress: 49 iterations completedProgress: 50 iterations completedProgress: 51 iterations completedProgress: 52 iterations completedProgress: 53 iterations completedProgress: 54 iterations completedProgress: 55 iterations completedProgress: 56 iterations completedProgress: 57 iterations completedProgress: 58 iterations completedProgress: 59 iterations completedProgress: 60 iterations completedProgress: 61 iterations completedProgress: 62 iterations completedProgress: 63 iterations completedProgress: 64 iterations completedProgress: 65 iterations completedProgress: 66 iterations completedProgress: 67 iterations completedProgress: 68 iterations completedProgress: 69 iterations completedProgress: 70 iterations completedProgress: 71 iterations completedProgress: 72 iterations completedProgress: 73 iterations completedProgress: 74 iterations completedProgress: 75 iterations completedProgress: 76 iterations completedProgress: 77 iterations completedProgress: 78 iterations completedProgress: 79 iterations completedProgress: 80 iterations completedProgress: 81 iterations completedProgress: 82 iterations completedProgress: 83 iterations completedProgress: 84 iterations completedProgress: 85 iterations completedProgress: 86 iterations completedProgress: 87 iterations completedProgress: 88 iterations completedProgress: 89 iterations completedProgress: 90 iterations completedProgress: 91 iterations completedProgress: 92 iterations completedProgress: 93 iterations completedProgress: 94 iterations completedProgress: 95 iterations completedProgress: 96 iterations completedProgress: 97 iterations completedProgress: 98 iterations completedProgress: 99 iterations completedProgress: 100 iterations completed # Calculate the mean RMSPE for the best k mean_rmspe_best_k &lt;- mean(rmspe_best_k) # Print the results print(paste(&quot;Mean RMSPE with best k:&quot;, mean_rmspe_best_k)) ## [1] &quot;Mean RMSPE with best k: 0.438212386417574&quot; # Initialize storage for RMSPE results rmspe_ensemble_knn &lt;- numeric(1000) for (i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Train the KNN model with the best k knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = best_k) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) knn_pred &lt;- predict(knn_mod, test_data) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, tuned_gbm_pred, knn_pred) # Train and evaluate the stacked ensemble with KNN replacing base GBM meta_four &lt;- lm(medv ~ ., data = cbind(preds_four, medv = test_data$medv)) meta_four_pred &lt;- predict(meta_four, preds_four) # Calculate RMSPE for the ensemble with KNN replacing base GBM rmspe_ensemble_knn[i] &lt;- sqrt(mean((meta_four_pred - test_data$medv)^2)) } ## Progress: 1 iterations completedProgress: 2 iterations completedProgress: 3 iterations completedProgress: 4 iterations completedProgress: 5 iterations completedProgress: 6 iterations completedProgress: 7 iterations completedProgress: 8 iterations completedProgress: 9 iterations completedProgress: 10 iterations completedProgress: 11 iterations completedProgress: 12 iterations completedProgress: 13 iterations completedProgress: 14 iterations completedProgress: 15 iterations completedProgress: 16 iterations completedProgress: 17 iterations completedProgress: 18 iterations completedProgress: 19 iterations completedProgress: 20 iterations completedProgress: 21 iterations completedProgress: 22 iterations completedProgress: 23 iterations completedProgress: 24 iterations completedProgress: 25 iterations completedProgress: 26 iterations completedProgress: 27 iterations completedProgress: 28 iterations completedProgress: 29 iterations completedProgress: 30 iterations completedProgress: 31 iterations completedProgress: 32 iterations completedProgress: 33 iterations completedProgress: 34 iterations completedProgress: 35 iterations completedProgress: 36 iterations completedProgress: 37 iterations completedProgress: 38 iterations completedProgress: 39 iterations completedProgress: 40 iterations completedProgress: 41 iterations completedProgress: 42 iterations completedProgress: 43 iterations completedProgress: 44 iterations completedProgress: 45 iterations completedProgress: 46 iterations completedProgress: 47 iterations completedProgress: 48 iterations completedProgress: 49 iterations completedProgress: 50 iterations completedProgress: 51 iterations completedProgress: 52 iterations completedProgress: 53 iterations completedProgress: 54 iterations completedProgress: 55 iterations completedProgress: 56 iterations completedProgress: 57 iterations completedProgress: 58 iterations completedProgress: 59 iterations completedProgress: 60 iterations completedProgress: 61 iterations completedProgress: 62 iterations completedProgress: 63 iterations completedProgress: 64 iterations completedProgress: 65 iterations completedProgress: 66 iterations completedProgress: 67 iterations completedProgress: 68 iterations completedProgress: 69 iterations completedProgress: 70 iterations completedProgress: 71 iterations completedProgress: 72 iterations completedProgress: 73 iterations completedProgress: 74 iterations completedProgress: 75 iterations completedProgress: 76 iterations completedProgress: 77 iterations completedProgress: 78 iterations completedProgress: 79 iterations completedProgress: 80 iterations completedProgress: 81 iterations completedProgress: 82 iterations completedProgress: 83 iterations completedProgress: 84 iterations completedProgress: 85 iterations completedProgress: 86 iterations completedProgress: 87 iterations completedProgress: 88 iterations completedProgress: 89 iterations completedProgress: 90 iterations completedProgress: 91 iterations completedProgress: 92 iterations completedProgress: 93 iterations completedProgress: 94 iterations completedProgress: 95 iterations completedProgress: 96 iterations completedProgress: 97 iterations completedProgress: 98 iterations completedProgress: 99 iterations completedProgress: 100 iterations completedProgress: 101 iterations completedProgress: 102 iterations completedProgress: 103 iterations completedProgress: 104 iterations completedProgress: 105 iterations completedProgress: 106 iterations completedProgress: 107 iterations completedProgress: 108 iterations completedProgress: 109 iterations completedProgress: 110 iterations completedProgress: 111 iterations completedProgress: 112 iterations completedProgress: 113 iterations completedProgress: 114 iterations completedProgress: 115 iterations completedProgress: 116 iterations completedProgress: 117 iterations completedProgress: 118 iterations completedProgress: 119 iterations completedProgress: 120 iterations completedProgress: 121 iterations completedProgress: 122 iterations completedProgress: 123 iterations completedProgress: 124 iterations completedProgress: 125 iterations completedProgress: 126 iterations completedProgress: 127 iterations completedProgress: 128 iterations completedProgress: 129 iterations completedProgress: 130 iterations completedProgress: 131 iterations completedProgress: 132 iterations completedProgress: 133 iterations completedProgress: 134 iterations completedProgress: 135 iterations completedProgress: 136 iterations completedProgress: 137 iterations completedProgress: 138 iterations completedProgress: 139 iterations completedProgress: 140 iterations completedProgress: 141 iterations completedProgress: 142 iterations completedProgress: 143 iterations completedProgress: 144 iterations completedProgress: 145 iterations completedProgress: 146 iterations completedProgress: 147 iterations completedProgress: 148 iterations completedProgress: 149 iterations completedProgress: 150 iterations completedProgress: 151 iterations completedProgress: 152 iterations completedProgress: 153 iterations completedProgress: 154 iterations completedProgress: 155 iterations completedProgress: 156 iterations completedProgress: 157 iterations completedProgress: 158 iterations completedProgress: 159 iterations completedProgress: 160 iterations completedProgress: 161 iterations completedProgress: 162 iterations completedProgress: 163 iterations completedProgress: 164 iterations completedProgress: 165 iterations completedProgress: 166 iterations completedProgress: 167 iterations completedProgress: 168 iterations completedProgress: 169 iterations completedProgress: 170 iterations completedProgress: 171 iterations completedProgress: 172 iterations completedProgress: 173 iterations completedProgress: 174 iterations completedProgress: 175 iterations completedProgress: 176 iterations completedProgress: 177 iterations completedProgress: 178 iterations completedProgress: 179 iterations completedProgress: 180 iterations completedProgress: 181 iterations completedProgress: 182 iterations completedProgress: 183 iterations completedProgress: 184 iterations completedProgress: 185 iterations completedProgress: 186 iterations completedProgress: 187 iterations completedProgress: 188 iterations completedProgress: 189 iterations completedProgress: 190 iterations completedProgress: 191 iterations completedProgress: 192 iterations completedProgress: 193 iterations completedProgress: 194 iterations completedProgress: 195 iterations completedProgress: 196 iterations completedProgress: 197 iterations completedProgress: 198 iterations completedProgress: 199 iterations completedProgress: 200 iterations completedProgress: 201 iterations completedProgress: 202 iterations completedProgress: 203 iterations completedProgress: 204 iterations completedProgress: 205 iterations completedProgress: 206 iterations completedProgress: 207 iterations completedProgress: 208 iterations completedProgress: 209 iterations completedProgress: 210 iterations completedProgress: 211 iterations completedProgress: 212 iterations completedProgress: 213 iterations completedProgress: 214 iterations completedProgress: 215 iterations completedProgress: 216 iterations completedProgress: 217 iterations completedProgress: 218 iterations completedProgress: 219 iterations completedProgress: 220 iterations completedProgress: 221 iterations completedProgress: 222 iterations completedProgress: 223 iterations completedProgress: 224 iterations completedProgress: 225 iterations completedProgress: 226 iterations completedProgress: 227 iterations completedProgress: 228 iterations completedProgress: 229 iterations completedProgress: 230 iterations completedProgress: 231 iterations completedProgress: 232 iterations completedProgress: 233 iterations completedProgress: 234 iterations completedProgress: 235 iterations completedProgress: 236 iterations completedProgress: 237 iterations completedProgress: 238 iterations completedProgress: 239 iterations completedProgress: 240 iterations completedProgress: 241 iterations completedProgress: 242 iterations completedProgress: 243 iterations completedProgress: 244 iterations completedProgress: 245 iterations completedProgress: 246 iterations completedProgress: 247 iterations completedProgress: 248 iterations completedProgress: 249 iterations completedProgress: 250 iterations completedProgress: 251 iterations completedProgress: 252 iterations completedProgress: 253 iterations completedProgress: 254 iterations completedProgress: 255 iterations completedProgress: 256 iterations completedProgress: 257 iterations completedProgress: 258 iterations completedProgress: 259 iterations completedProgress: 260 iterations completedProgress: 261 iterations completedProgress: 262 iterations completedProgress: 263 iterations completedProgress: 264 iterations completedProgress: 265 iterations completedProgress: 266 iterations completedProgress: 267 iterations completedProgress: 268 iterations completedProgress: 269 iterations completedProgress: 270 iterations completedProgress: 271 iterations completedProgress: 272 iterations completedProgress: 273 iterations completedProgress: 274 iterations completedProgress: 275 iterations completedProgress: 276 iterations completedProgress: 277 iterations completedProgress: 278 iterations completedProgress: 279 iterations completedProgress: 280 iterations completedProgress: 281 iterations completedProgress: 282 iterations completedProgress: 283 iterations completedProgress: 284 iterations completedProgress: 285 iterations completedProgress: 286 iterations completedProgress: 287 iterations completedProgress: 288 iterations completedProgress: 289 iterations completedProgress: 290 iterations completedProgress: 291 iterations completedProgress: 292 iterations completedProgress: 293 iterations completedProgress: 294 iterations completedProgress: 295 iterations completedProgress: 296 iterations completedProgress: 297 iterations completedProgress: 298 iterations completedProgress: 299 iterations completedProgress: 300 iterations completedProgress: 301 iterations completedProgress: 302 iterations completedProgress: 303 iterations completedProgress: 304 iterations completedProgress: 305 iterations completedProgress: 306 iterations completedProgress: 307 iterations completedProgress: 308 iterations completedProgress: 309 iterations completedProgress: 310 iterations completedProgress: 311 iterations completedProgress: 312 iterations completedProgress: 313 iterations completedProgress: 314 iterations completedProgress: 315 iterations completedProgress: 316 iterations completedProgress: 317 iterations completedProgress: 318 iterations completedProgress: 319 iterations completedProgress: 320 iterations completedProgress: 321 iterations completedProgress: 322 iterations completedProgress: 323 iterations completedProgress: 324 iterations completedProgress: 325 iterations completedProgress: 326 iterations completedProgress: 327 iterations completedProgress: 328 iterations completedProgress: 329 iterations completedProgress: 330 iterations completedProgress: 331 iterations completedProgress: 332 iterations completedProgress: 333 iterations completedProgress: 334 iterations completedProgress: 335 iterations completedProgress: 336 iterations completedProgress: 337 iterations completedProgress: 338 iterations completedProgress: 339 iterations completedProgress: 340 iterations completedProgress: 341 iterations completedProgress: 342 iterations completedProgress: 343 iterations completedProgress: 344 iterations completedProgress: 345 iterations completedProgress: 346 iterations completedProgress: 347 iterations completedProgress: 348 iterations completedProgress: 349 iterations completedProgress: 350 iterations completedProgress: 351 iterations completedProgress: 352 iterations completedProgress: 353 iterations completedProgress: 354 iterations completedProgress: 355 iterations completedProgress: 356 iterations completedProgress: 357 iterations completedProgress: 358 iterations completedProgress: 359 iterations completedProgress: 360 iterations completedProgress: 361 iterations completedProgress: 362 iterations completedProgress: 363 iterations completedProgress: 364 iterations completedProgress: 365 iterations completedProgress: 366 iterations completedProgress: 367 iterations completedProgress: 368 iterations completedProgress: 369 iterations completedProgress: 370 iterations completedProgress: 371 iterations completedProgress: 372 iterations completedProgress: 373 iterations completedProgress: 374 iterations completedProgress: 375 iterations completedProgress: 376 iterations completedProgress: 377 iterations completedProgress: 378 iterations completedProgress: 379 iterations completedProgress: 380 iterations completedProgress: 381 iterations completedProgress: 382 iterations completedProgress: 383 iterations completedProgress: 384 iterations completedProgress: 385 iterations completedProgress: 386 iterations completedProgress: 387 iterations completedProgress: 388 iterations completedProgress: 389 iterations completedProgress: 390 iterations completedProgress: 391 iterations completedProgress: 392 iterations completedProgress: 393 iterations completedProgress: 394 iterations completedProgress: 395 iterations completedProgress: 396 iterations completedProgress: 397 iterations completedProgress: 398 iterations completedProgress: 399 iterations completedProgress: 400 iterations completedProgress: 401 iterations completedProgress: 402 iterations completedProgress: 403 iterations completedProgress: 404 iterations completedProgress: 405 iterations completedProgress: 406 iterations completedProgress: 407 iterations completedProgress: 408 iterations completedProgress: 409 iterations completedProgress: 410 iterations completedProgress: 411 iterations completedProgress: 412 iterations completedProgress: 413 iterations completedProgress: 414 iterations completedProgress: 415 iterations completedProgress: 416 iterations completedProgress: 417 iterations completedProgress: 418 iterations completedProgress: 419 iterations completedProgress: 420 iterations completedProgress: 421 iterations completedProgress: 422 iterations completedProgress: 423 iterations completedProgress: 424 iterations completedProgress: 425 iterations completedProgress: 426 iterations completedProgress: 427 iterations completedProgress: 428 iterations completedProgress: 429 iterations completedProgress: 430 iterations completedProgress: 431 iterations completedProgress: 432 iterations completedProgress: 433 iterations completedProgress: 434 iterations completedProgress: 435 iterations completedProgress: 436 iterations completedProgress: 437 iterations completedProgress: 438 iterations completedProgress: 439 iterations completedProgress: 440 iterations completedProgress: 441 iterations completedProgress: 442 iterations completedProgress: 443 iterations completedProgress: 444 iterations completedProgress: 445 iterations completedProgress: 446 iterations completedProgress: 447 iterations completedProgress: 448 iterations completedProgress: 449 iterations completedProgress: 450 iterations completedProgress: 451 iterations completedProgress: 452 iterations completedProgress: 453 iterations completedProgress: 454 iterations completedProgress: 455 iterations completedProgress: 456 iterations completedProgress: 457 iterations completedProgress: 458 iterations completedProgress: 459 iterations completedProgress: 460 iterations completedProgress: 461 iterations completedProgress: 462 iterations completedProgress: 463 iterations completedProgress: 464 iterations completedProgress: 465 iterations completedProgress: 466 iterations completedProgress: 467 iterations completedProgress: 468 iterations completedProgress: 469 iterations completedProgress: 470 iterations completedProgress: 471 iterations completedProgress: 472 iterations completedProgress: 473 iterations completedProgress: 474 iterations completedProgress: 475 iterations completedProgress: 476 iterations completedProgress: 477 iterations completedProgress: 478 iterations completedProgress: 479 iterations completedProgress: 480 iterations completedProgress: 481 iterations completedProgress: 482 iterations completedProgress: 483 iterations completedProgress: 484 iterations completedProgress: 485 iterations completedProgress: 486 iterations completedProgress: 487 iterations completedProgress: 488 iterations completedProgress: 489 iterations completedProgress: 490 iterations completedProgress: 491 iterations completedProgress: 492 iterations completedProgress: 493 iterations completedProgress: 494 iterations completedProgress: 495 iterations completedProgress: 496 iterations completedProgress: 497 iterations completedProgress: 498 iterations completedProgress: 499 iterations completedProgress: 500 iterations completedProgress: 501 iterations completedProgress: 502 iterations completedProgress: 503 iterations completedProgress: 504 iterations completedProgress: 505 iterations completedProgress: 506 iterations completedProgress: 507 iterations completedProgress: 508 iterations completedProgress: 509 iterations completedProgress: 510 iterations completedProgress: 511 iterations completedProgress: 512 iterations completedProgress: 513 iterations completedProgress: 514 iterations completedProgress: 515 iterations completedProgress: 516 iterations completedProgress: 517 iterations completedProgress: 518 iterations completedProgress: 519 iterations completedProgress: 520 iterations completedProgress: 521 iterations completedProgress: 522 iterations completedProgress: 523 iterations completedProgress: 524 iterations completedProgress: 525 iterations completedProgress: 526 iterations completedProgress: 527 iterations completedProgress: 528 iterations completedProgress: 529 iterations completedProgress: 530 iterations completedProgress: 531 iterations completedProgress: 532 iterations completedProgress: 533 iterations completedProgress: 534 iterations completedProgress: 535 iterations completedProgress: 536 iterations completedProgress: 537 iterations completedProgress: 538 iterations completedProgress: 539 iterations completedProgress: 540 iterations completedProgress: 541 iterations completedProgress: 542 iterations completedProgress: 543 iterations completedProgress: 544 iterations completedProgress: 545 iterations completedProgress: 546 iterations completedProgress: 547 iterations completedProgress: 548 iterations completedProgress: 549 iterations completedProgress: 550 iterations completedProgress: 551 iterations completedProgress: 552 iterations completedProgress: 553 iterations completedProgress: 554 iterations completedProgress: 555 iterations completedProgress: 556 iterations completedProgress: 557 iterations completedProgress: 558 iterations completedProgress: 559 iterations completedProgress: 560 iterations completedProgress: 561 iterations completedProgress: 562 iterations completedProgress: 563 iterations completedProgress: 564 iterations completedProgress: 565 iterations completedProgress: 566 iterations completedProgress: 567 iterations completedProgress: 568 iterations completedProgress: 569 iterations completedProgress: 570 iterations completedProgress: 571 iterations completedProgress: 572 iterations completedProgress: 573 iterations completedProgress: 574 iterations completedProgress: 575 iterations completedProgress: 576 iterations completedProgress: 577 iterations completedProgress: 578 iterations completedProgress: 579 iterations completedProgress: 580 iterations completedProgress: 581 iterations completedProgress: 582 iterations completedProgress: 583 iterations completedProgress: 584 iterations completedProgress: 585 iterations completedProgress: 586 iterations completedProgress: 587 iterations completedProgress: 588 iterations completedProgress: 589 iterations completedProgress: 590 iterations completedProgress: 591 iterations completedProgress: 592 iterations completedProgress: 593 iterations completedProgress: 594 iterations completedProgress: 595 iterations completedProgress: 596 iterations completedProgress: 597 iterations completedProgress: 598 iterations completedProgress: 599 iterations completedProgress: 600 iterations completedProgress: 601 iterations completedProgress: 602 iterations completedProgress: 603 iterations completedProgress: 604 iterations completedProgress: 605 iterations completedProgress: 606 iterations completedProgress: 607 iterations completedProgress: 608 iterations completedProgress: 609 iterations completedProgress: 610 iterations completedProgress: 611 iterations completedProgress: 612 iterations completedProgress: 613 iterations completedProgress: 614 iterations completedProgress: 615 iterations completedProgress: 616 iterations completedProgress: 617 iterations completedProgress: 618 iterations completedProgress: 619 iterations completedProgress: 620 iterations completedProgress: 621 iterations completedProgress: 622 iterations completedProgress: 623 iterations completedProgress: 624 iterations completedProgress: 625 iterations completedProgress: 626 iterations completedProgress: 627 iterations completedProgress: 628 iterations completedProgress: 629 iterations completedProgress: 630 iterations completedProgress: 631 iterations completedProgress: 632 iterations completedProgress: 633 iterations completedProgress: 634 iterations completedProgress: 635 iterations completedProgress: 636 iterations completedProgress: 637 iterations completedProgress: 638 iterations completedProgress: 639 iterations completedProgress: 640 iterations completedProgress: 641 iterations completedProgress: 642 iterations completedProgress: 643 iterations completedProgress: 644 iterations completedProgress: 645 iterations completedProgress: 646 iterations completedProgress: 647 iterations completedProgress: 648 iterations completedProgress: 649 iterations completedProgress: 650 iterations completedProgress: 651 iterations completedProgress: 652 iterations completedProgress: 653 iterations completedProgress: 654 iterations completedProgress: 655 iterations completedProgress: 656 iterations completedProgress: 657 iterations completedProgress: 658 iterations completedProgress: 659 iterations completedProgress: 660 iterations completedProgress: 661 iterations completedProgress: 662 iterations completedProgress: 663 iterations completedProgress: 664 iterations completedProgress: 665 iterations completedProgress: 666 iterations completedProgress: 667 iterations completedProgress: 668 iterations completedProgress: 669 iterations completedProgress: 670 iterations completedProgress: 671 iterations completedProgress: 672 iterations completedProgress: 673 iterations completedProgress: 674 iterations completedProgress: 675 iterations completedProgress: 676 iterations completedProgress: 677 iterations completedProgress: 678 iterations completedProgress: 679 iterations completedProgress: 680 iterations completedProgress: 681 iterations completedProgress: 682 iterations completedProgress: 683 iterations completedProgress: 684 iterations completedProgress: 685 iterations completedProgress: 686 iterations completedProgress: 687 iterations completedProgress: 688 iterations completedProgress: 689 iterations completedProgress: 690 iterations completedProgress: 691 iterations completedProgress: 692 iterations completedProgress: 693 iterations completedProgress: 694 iterations completedProgress: 695 iterations completedProgress: 696 iterations completedProgress: 697 iterations completedProgress: 698 iterations completedProgress: 699 iterations completedProgress: 700 iterations completedProgress: 701 iterations completedProgress: 702 iterations completedProgress: 703 iterations completedProgress: 704 iterations completedProgress: 705 iterations completedProgress: 706 iterations completedProgress: 707 iterations completedProgress: 708 iterations completedProgress: 709 iterations completedProgress: 710 iterations completedProgress: 711 iterations completedProgress: 712 iterations completedProgress: 713 iterations completedProgress: 714 iterations completedProgress: 715 iterations completedProgress: 716 iterations completedProgress: 717 iterations completedProgress: 718 iterations completedProgress: 719 iterations completedProgress: 720 iterations completedProgress: 721 iterations completedProgress: 722 iterations completedProgress: 723 iterations completedProgress: 724 iterations completedProgress: 725 iterations completedProgress: 726 iterations completedProgress: 727 iterations completedProgress: 728 iterations completedProgress: 729 iterations completedProgress: 730 iterations completedProgress: 731 iterations completedProgress: 732 iterations completedProgress: 733 iterations completedProgress: 734 iterations completedProgress: 735 iterations completedProgress: 736 iterations completedProgress: 737 iterations completedProgress: 738 iterations completedProgress: 739 iterations completedProgress: 740 iterations completedProgress: 741 iterations completedProgress: 742 iterations completedProgress: 743 iterations completedProgress: 744 iterations completedProgress: 745 iterations completedProgress: 746 iterations completedProgress: 747 iterations completedProgress: 748 iterations completedProgress: 749 iterations completedProgress: 750 iterations completedProgress: 751 iterations completedProgress: 752 iterations completedProgress: 753 iterations completedProgress: 754 iterations completedProgress: 755 iterations completedProgress: 756 iterations completedProgress: 757 iterations completedProgress: 758 iterations completedProgress: 759 iterations completedProgress: 760 iterations completedProgress: 761 iterations completedProgress: 762 iterations completedProgress: 763 iterations completedProgress: 764 iterations completedProgress: 765 iterations completedProgress: 766 iterations completedProgress: 767 iterations completedProgress: 768 iterations completedProgress: 769 iterations completedProgress: 770 iterations completedProgress: 771 iterations completedProgress: 772 iterations completedProgress: 773 iterations completedProgress: 774 iterations completedProgress: 775 iterations completedProgress: 776 iterations completedProgress: 777 iterations completedProgress: 778 iterations completedProgress: 779 iterations completedProgress: 780 iterations completedProgress: 781 iterations completedProgress: 782 iterations completedProgress: 783 iterations completedProgress: 784 iterations completedProgress: 785 iterations completedProgress: 786 iterations completedProgress: 787 iterations completedProgress: 788 iterations completedProgress: 789 iterations completedProgress: 790 iterations completedProgress: 791 iterations completedProgress: 792 iterations completedProgress: 793 iterations completedProgress: 794 iterations completedProgress: 795 iterations completedProgress: 796 iterations completedProgress: 797 iterations completedProgress: 798 iterations completedProgress: 799 iterations completedProgress: 800 iterations completedProgress: 801 iterations completedProgress: 802 iterations completedProgress: 803 iterations completedProgress: 804 iterations completedProgress: 805 iterations completedProgress: 806 iterations completedProgress: 807 iterations completedProgress: 808 iterations completedProgress: 809 iterations completedProgress: 810 iterations completedProgress: 811 iterations completedProgress: 812 iterations completedProgress: 813 iterations completedProgress: 814 iterations completedProgress: 815 iterations completedProgress: 816 iterations completedProgress: 817 iterations completedProgress: 818 iterations completedProgress: 819 iterations completedProgress: 820 iterations completedProgress: 821 iterations completedProgress: 822 iterations completedProgress: 823 iterations completedProgress: 824 iterations completedProgress: 825 iterations completedProgress: 826 iterations completedProgress: 827 iterations completedProgress: 828 iterations completedProgress: 829 iterations completedProgress: 830 iterations completedProgress: 831 iterations completedProgress: 832 iterations completedProgress: 833 iterations completedProgress: 834 iterations completedProgress: 835 iterations completedProgress: 836 iterations completedProgress: 837 iterations completedProgress: 838 iterations completedProgress: 839 iterations completedProgress: 840 iterations completedProgress: 841 iterations completedProgress: 842 iterations completedProgress: 843 iterations completedProgress: 844 iterations completedProgress: 845 iterations completedProgress: 846 iterations completedProgress: 847 iterations completedProgress: 848 iterations completedProgress: 849 iterations completedProgress: 850 iterations completedProgress: 851 iterations completedProgress: 852 iterations completedProgress: 853 iterations completedProgress: 854 iterations completedProgress: 855 iterations completedProgress: 856 iterations completedProgress: 857 iterations completedProgress: 858 iterations completedProgress: 859 iterations completedProgress: 860 iterations completedProgress: 861 iterations completedProgress: 862 iterations completedProgress: 863 iterations completedProgress: 864 iterations completedProgress: 865 iterations completedProgress: 866 iterations completedProgress: 867 iterations completedProgress: 868 iterations completedProgress: 869 iterations completedProgress: 870 iterations completedProgress: 871 iterations completedProgress: 872 iterations completedProgress: 873 iterations completedProgress: 874 iterations completedProgress: 875 iterations completedProgress: 876 iterations completedProgress: 877 iterations completedProgress: 878 iterations completedProgress: 879 iterations completedProgress: 880 iterations completedProgress: 881 iterations completedProgress: 882 iterations completedProgress: 883 iterations completedProgress: 884 iterations completedProgress: 885 iterations completedProgress: 886 iterations completedProgress: 887 iterations completedProgress: 888 iterations completedProgress: 889 iterations completedProgress: 890 iterations completedProgress: 891 iterations completedProgress: 892 iterations completedProgress: 893 iterations completedProgress: 894 iterations completedProgress: 895 iterations completedProgress: 896 iterations completedProgress: 897 iterations completedProgress: 898 iterations completedProgress: 899 iterations completedProgress: 900 iterations completedProgress: 901 iterations completedProgress: 902 iterations completedProgress: 903 iterations completedProgress: 904 iterations completedProgress: 905 iterations completedProgress: 906 iterations completedProgress: 907 iterations completedProgress: 908 iterations completedProgress: 909 iterations completedProgress: 910 iterations completedProgress: 911 iterations completedProgress: 912 iterations completedProgress: 913 iterations completedProgress: 914 iterations completedProgress: 915 iterations completedProgress: 916 iterations completedProgress: 917 iterations completedProgress: 918 iterations completedProgress: 919 iterations completedProgress: 920 iterations completedProgress: 921 iterations completedProgress: 922 iterations completedProgress: 923 iterations completedProgress: 924 iterations completedProgress: 925 iterations completedProgress: 926 iterations completedProgress: 927 iterations completedProgress: 928 iterations completedProgress: 929 iterations completedProgress: 930 iterations completedProgress: 931 iterations completedProgress: 932 iterations completedProgress: 933 iterations completedProgress: 934 iterations completedProgress: 935 iterations completedProgress: 936 iterations completedProgress: 937 iterations completedProgress: 938 iterations completedProgress: 939 iterations completedProgress: 940 iterations completedProgress: 941 iterations completedProgress: 942 iterations completedProgress: 943 iterations completedProgress: 944 iterations completedProgress: 945 iterations completedProgress: 946 iterations completedProgress: 947 iterations completedProgress: 948 iterations completedProgress: 949 iterations completedProgress: 950 iterations completedProgress: 951 iterations completedProgress: 952 iterations completedProgress: 953 iterations completedProgress: 954 iterations completedProgress: 955 iterations completedProgress: 956 iterations completedProgress: 957 iterations completedProgress: 958 iterations completedProgress: 959 iterations completedProgress: 960 iterations completedProgress: 961 iterations completedProgress: 962 iterations completedProgress: 963 iterations completedProgress: 964 iterations completedProgress: 965 iterations completedProgress: 966 iterations completedProgress: 967 iterations completedProgress: 968 iterations completedProgress: 969 iterations completedProgress: 970 iterations completedProgress: 971 iterations completedProgress: 972 iterations completedProgress: 973 iterations completedProgress: 974 iterations completedProgress: 975 iterations completedProgress: 976 iterations completedProgress: 977 iterations completedProgress: 978 iterations completedProgress: 979 iterations completedProgress: 980 iterations completedProgress: 981 iterations completedProgress: 982 iterations completedProgress: 983 iterations completedProgress: 984 iterations completedProgress: 985 iterations completedProgress: 986 iterations completedProgress: 987 iterations completedProgress: 988 iterations completedProgress: 989 iterations completedProgress: 990 iterations completedProgress: 991 iterations completedProgress: 992 iterations completedProgress: 993 iterations completedProgress: 994 iterations completedProgress: 995 iterations completedProgress: 996 iterations completedProgress: 997 iterations completedProgress: 998 iterations completedProgress: 999 iterations completedProgress: 1000 iterations completed # Calculate mean RMSPE for the ensemble with KNN mean_rmspe_ensemble_knn &lt;- mean(rmspe_ensemble_knn) # Print the results print(paste(&quot;Mean RMSPE of Stacked Ensemble with Tuned KNN Model:&quot;, mean_rmspe_ensemble_knn)) ## [1] &quot;Mean RMSPE of Stacked Ensemble with Tuned KNN Model: 0.33259406977605&quot; # Initialize storage for the sum of correlation matrices cor_matrix_sum &lt;- matrix(0, nrow = 4, ncol = 4) # Assuming 4 base models for (i in 1:1000) { cat(&quot;\\rProgress:&quot;, i, &quot;iterations completed&quot;) # Bootstrapping train_index &lt;- unique(sample(nrow(data), nrow(data), replace = TRUE)) train_data &lt;- data[train_index, ] test_data &lt;- data[-train_index, ] # Re-train the original models on bootstrapped training data lm_mod &lt;- lm(medv ~ ., data = train_data) rf_mod &lt;- randomForest(medv ~ ., data = train_data) tuned_gbm_mod &lt;- gbm(medv ~ ., data = train_data, distribution = &quot;gaussian&quot;, n.trees = best_params$n.trees, shrinkage = best_params$shrinkage, interaction.depth = best_params$interaction.depth, verbose = FALSE) # Train the KNN model with the best k knn_mod &lt;- knnreg(medv ~ ., data = train_data, k = best_k) # Generate predictions on the bootstrapped test set lm_pred &lt;- predict(lm_mod, test_data) rf_pred &lt;- predict(rf_mod, test_data) tuned_gbm_pred &lt;- predict(tuned_gbm_mod, test_data, n.trees = best_params$n.trees) knn_pred &lt;- predict(knn_mod, test_data) # Combine predictions into a data frame preds_four &lt;- data.frame(lm_pred, rf_pred, tuned_gbm_pred, knn_pred) # Calculate the correlation matrix for the current iteration cor_matrix &lt;- cor(preds_four) # Accumulate the correlation matrix cor_matrix_sum &lt;- cor_matrix_sum + cor_matrix } ## Progress: 1 iterations completedProgress: 2 iterations completedProgress: 3 iterations completedProgress: 4 iterations completedProgress: 5 iterations completedProgress: 6 iterations completedProgress: 7 iterations completedProgress: 8 iterations completedProgress: 9 iterations completedProgress: 10 iterations completedProgress: 11 iterations completedProgress: 12 iterations completedProgress: 13 iterations completedProgress: 14 iterations completedProgress: 15 iterations completedProgress: 16 iterations completedProgress: 17 iterations completedProgress: 18 iterations completedProgress: 19 iterations completedProgress: 20 iterations completedProgress: 21 iterations completedProgress: 22 iterations completedProgress: 23 iterations completedProgress: 24 iterations completedProgress: 25 iterations completedProgress: 26 iterations completedProgress: 27 iterations completedProgress: 28 iterations completedProgress: 29 iterations completedProgress: 30 iterations completedProgress: 31 iterations completedProgress: 32 iterations completedProgress: 33 iterations completedProgress: 34 iterations completedProgress: 35 iterations completedProgress: 36 iterations completedProgress: 37 iterations completedProgress: 38 iterations completedProgress: 39 iterations completedProgress: 40 iterations completedProgress: 41 iterations completedProgress: 42 iterations completedProgress: 43 iterations completedProgress: 44 iterations completedProgress: 45 iterations completedProgress: 46 iterations completedProgress: 47 iterations completedProgress: 48 iterations completedProgress: 49 iterations completedProgress: 50 iterations completedProgress: 51 iterations completedProgress: 52 iterations completedProgress: 53 iterations completedProgress: 54 iterations completedProgress: 55 iterations completedProgress: 56 iterations completedProgress: 57 iterations completedProgress: 58 iterations completedProgress: 59 iterations completedProgress: 60 iterations completedProgress: 61 iterations completedProgress: 62 iterations completedProgress: 63 iterations completedProgress: 64 iterations completedProgress: 65 iterations completedProgress: 66 iterations completedProgress: 67 iterations completedProgress: 68 iterations completedProgress: 69 iterations completedProgress: 70 iterations completedProgress: 71 iterations completedProgress: 72 iterations completedProgress: 73 iterations completedProgress: 74 iterations completedProgress: 75 iterations completedProgress: 76 iterations completedProgress: 77 iterations completedProgress: 78 iterations completedProgress: 79 iterations completedProgress: 80 iterations completedProgress: 81 iterations completedProgress: 82 iterations completedProgress: 83 iterations completedProgress: 84 iterations completedProgress: 85 iterations completedProgress: 86 iterations completedProgress: 87 iterations completedProgress: 88 iterations completedProgress: 89 iterations completedProgress: 90 iterations completedProgress: 91 iterations completedProgress: 92 iterations completedProgress: 93 iterations completedProgress: 94 iterations completedProgress: 95 iterations completedProgress: 96 iterations completedProgress: 97 iterations completedProgress: 98 iterations completedProgress: 99 iterations completedProgress: 100 iterations completedProgress: 101 iterations completedProgress: 102 iterations completedProgress: 103 iterations completedProgress: 104 iterations completedProgress: 105 iterations completedProgress: 106 iterations completedProgress: 107 iterations completedProgress: 108 iterations completedProgress: 109 iterations completedProgress: 110 iterations completedProgress: 111 iterations completedProgress: 112 iterations completedProgress: 113 iterations completedProgress: 114 iterations completedProgress: 115 iterations completedProgress: 116 iterations completedProgress: 117 iterations completedProgress: 118 iterations completedProgress: 119 iterations completedProgress: 120 iterations completedProgress: 121 iterations completedProgress: 122 iterations completedProgress: 123 iterations completedProgress: 124 iterations completedProgress: 125 iterations completedProgress: 126 iterations completedProgress: 127 iterations completedProgress: 128 iterations completedProgress: 129 iterations completedProgress: 130 iterations completedProgress: 131 iterations completedProgress: 132 iterations completedProgress: 133 iterations completedProgress: 134 iterations completedProgress: 135 iterations completedProgress: 136 iterations completedProgress: 137 iterations completedProgress: 138 iterations completedProgress: 139 iterations completedProgress: 140 iterations completedProgress: 141 iterations completedProgress: 142 iterations completedProgress: 143 iterations completedProgress: 144 iterations completedProgress: 145 iterations completedProgress: 146 iterations completedProgress: 147 iterations completedProgress: 148 iterations completedProgress: 149 iterations completedProgress: 150 iterations completedProgress: 151 iterations completedProgress: 152 iterations completedProgress: 153 iterations completedProgress: 154 iterations completedProgress: 155 iterations completedProgress: 156 iterations completedProgress: 157 iterations completedProgress: 158 iterations completedProgress: 159 iterations completedProgress: 160 iterations completedProgress: 161 iterations completedProgress: 162 iterations completedProgress: 163 iterations completedProgress: 164 iterations completedProgress: 165 iterations completedProgress: 166 iterations completedProgress: 167 iterations completedProgress: 168 iterations completedProgress: 169 iterations completedProgress: 170 iterations completedProgress: 171 iterations completedProgress: 172 iterations completedProgress: 173 iterations completedProgress: 174 iterations completedProgress: 175 iterations completedProgress: 176 iterations completedProgress: 177 iterations completedProgress: 178 iterations completedProgress: 179 iterations completedProgress: 180 iterations completedProgress: 181 iterations completedProgress: 182 iterations completedProgress: 183 iterations completedProgress: 184 iterations completedProgress: 185 iterations completedProgress: 186 iterations completedProgress: 187 iterations completedProgress: 188 iterations completedProgress: 189 iterations completedProgress: 190 iterations completedProgress: 191 iterations completedProgress: 192 iterations completedProgress: 193 iterations completedProgress: 194 iterations completedProgress: 195 iterations completedProgress: 196 iterations completedProgress: 197 iterations completedProgress: 198 iterations completedProgress: 199 iterations completedProgress: 200 iterations completedProgress: 201 iterations completedProgress: 202 iterations completedProgress: 203 iterations completedProgress: 204 iterations completedProgress: 205 iterations completedProgress: 206 iterations completedProgress: 207 iterations completedProgress: 208 iterations completedProgress: 209 iterations completedProgress: 210 iterations completedProgress: 211 iterations completedProgress: 212 iterations completedProgress: 213 iterations completedProgress: 214 iterations completedProgress: 215 iterations completedProgress: 216 iterations completedProgress: 217 iterations completedProgress: 218 iterations completedProgress: 219 iterations completedProgress: 220 iterations completedProgress: 221 iterations completedProgress: 222 iterations completedProgress: 223 iterations completedProgress: 224 iterations completedProgress: 225 iterations completedProgress: 226 iterations completedProgress: 227 iterations completedProgress: 228 iterations completedProgress: 229 iterations completedProgress: 230 iterations completedProgress: 231 iterations completedProgress: 232 iterations completedProgress: 233 iterations completedProgress: 234 iterations completedProgress: 235 iterations completedProgress: 236 iterations completedProgress: 237 iterations completedProgress: 238 iterations completedProgress: 239 iterations completedProgress: 240 iterations completedProgress: 241 iterations completedProgress: 242 iterations completedProgress: 243 iterations completedProgress: 244 iterations completedProgress: 245 iterations completedProgress: 246 iterations completedProgress: 247 iterations completedProgress: 248 iterations completedProgress: 249 iterations completedProgress: 250 iterations completedProgress: 251 iterations completedProgress: 252 iterations completedProgress: 253 iterations completedProgress: 254 iterations completedProgress: 255 iterations completedProgress: 256 iterations completedProgress: 257 iterations completedProgress: 258 iterations completedProgress: 259 iterations completedProgress: 260 iterations completedProgress: 261 iterations completedProgress: 262 iterations completedProgress: 263 iterations completedProgress: 264 iterations completedProgress: 265 iterations completedProgress: 266 iterations completedProgress: 267 iterations completedProgress: 268 iterations completedProgress: 269 iterations completedProgress: 270 iterations completedProgress: 271 iterations completedProgress: 272 iterations completedProgress: 273 iterations completedProgress: 274 iterations completedProgress: 275 iterations completedProgress: 276 iterations completedProgress: 277 iterations completedProgress: 278 iterations completedProgress: 279 iterations completedProgress: 280 iterations completedProgress: 281 iterations completedProgress: 282 iterations completedProgress: 283 iterations completedProgress: 284 iterations completedProgress: 285 iterations completedProgress: 286 iterations completedProgress: 287 iterations completedProgress: 288 iterations completedProgress: 289 iterations completedProgress: 290 iterations completedProgress: 291 iterations completedProgress: 292 iterations completedProgress: 293 iterations completedProgress: 294 iterations completedProgress: 295 iterations completedProgress: 296 iterations completedProgress: 297 iterations completedProgress: 298 iterations completedProgress: 299 iterations completedProgress: 300 iterations completedProgress: 301 iterations completedProgress: 302 iterations completedProgress: 303 iterations completedProgress: 304 iterations completedProgress: 305 iterations completedProgress: 306 iterations completedProgress: 307 iterations completedProgress: 308 iterations completedProgress: 309 iterations completedProgress: 310 iterations completedProgress: 311 iterations completedProgress: 312 iterations completedProgress: 313 iterations completedProgress: 314 iterations completedProgress: 315 iterations completedProgress: 316 iterations completedProgress: 317 iterations completedProgress: 318 iterations completedProgress: 319 iterations completedProgress: 320 iterations completedProgress: 321 iterations completedProgress: 322 iterations completedProgress: 323 iterations completedProgress: 324 iterations completedProgress: 325 iterations completedProgress: 326 iterations completedProgress: 327 iterations completedProgress: 328 iterations completedProgress: 329 iterations completedProgress: 330 iterations completedProgress: 331 iterations completedProgress: 332 iterations completedProgress: 333 iterations completedProgress: 334 iterations completedProgress: 335 iterations completedProgress: 336 iterations completedProgress: 337 iterations completedProgress: 338 iterations completedProgress: 339 iterations completedProgress: 340 iterations completedProgress: 341 iterations completedProgress: 342 iterations completedProgress: 343 iterations completedProgress: 344 iterations completedProgress: 345 iterations completedProgress: 346 iterations completedProgress: 347 iterations completedProgress: 348 iterations completedProgress: 349 iterations completedProgress: 350 iterations completedProgress: 351 iterations completedProgress: 352 iterations completedProgress: 353 iterations completedProgress: 354 iterations completedProgress: 355 iterations completedProgress: 356 iterations completedProgress: 357 iterations completedProgress: 358 iterations completedProgress: 359 iterations completedProgress: 360 iterations completedProgress: 361 iterations completedProgress: 362 iterations completedProgress: 363 iterations completedProgress: 364 iterations completedProgress: 365 iterations completedProgress: 366 iterations completedProgress: 367 iterations completedProgress: 368 iterations completedProgress: 369 iterations completedProgress: 370 iterations completedProgress: 371 iterations completedProgress: 372 iterations completedProgress: 373 iterations completedProgress: 374 iterations completedProgress: 375 iterations completedProgress: 376 iterations completedProgress: 377 iterations completedProgress: 378 iterations completedProgress: 379 iterations completedProgress: 380 iterations completedProgress: 381 iterations completedProgress: 382 iterations completedProgress: 383 iterations completedProgress: 384 iterations completedProgress: 385 iterations completedProgress: 386 iterations completedProgress: 387 iterations completedProgress: 388 iterations completedProgress: 389 iterations completedProgress: 390 iterations completedProgress: 391 iterations completedProgress: 392 iterations completedProgress: 393 iterations completedProgress: 394 iterations completedProgress: 395 iterations completedProgress: 396 iterations completedProgress: 397 iterations completedProgress: 398 iterations completedProgress: 399 iterations completedProgress: 400 iterations completedProgress: 401 iterations completedProgress: 402 iterations completedProgress: 403 iterations completedProgress: 404 iterations completedProgress: 405 iterations completedProgress: 406 iterations completedProgress: 407 iterations completedProgress: 408 iterations completedProgress: 409 iterations completedProgress: 410 iterations completedProgress: 411 iterations completedProgress: 412 iterations completedProgress: 413 iterations completedProgress: 414 iterations completedProgress: 415 iterations completedProgress: 416 iterations completedProgress: 417 iterations completedProgress: 418 iterations completedProgress: 419 iterations completedProgress: 420 iterations completedProgress: 421 iterations completedProgress: 422 iterations completedProgress: 423 iterations completedProgress: 424 iterations completedProgress: 425 iterations completedProgress: 426 iterations completedProgress: 427 iterations completedProgress: 428 iterations completedProgress: 429 iterations completedProgress: 430 iterations completedProgress: 431 iterations completedProgress: 432 iterations completedProgress: 433 iterations completedProgress: 434 iterations completedProgress: 435 iterations completedProgress: 436 iterations completedProgress: 437 iterations completedProgress: 438 iterations completedProgress: 439 iterations completedProgress: 440 iterations completedProgress: 441 iterations completedProgress: 442 iterations completedProgress: 443 iterations completedProgress: 444 iterations completedProgress: 445 iterations completedProgress: 446 iterations completedProgress: 447 iterations completedProgress: 448 iterations completedProgress: 449 iterations completedProgress: 450 iterations completedProgress: 451 iterations completedProgress: 452 iterations completedProgress: 453 iterations completedProgress: 454 iterations completedProgress: 455 iterations completedProgress: 456 iterations completedProgress: 457 iterations completedProgress: 458 iterations completedProgress: 459 iterations completedProgress: 460 iterations completedProgress: 461 iterations completedProgress: 462 iterations completedProgress: 463 iterations completedProgress: 464 iterations completedProgress: 465 iterations completedProgress: 466 iterations completedProgress: 467 iterations completedProgress: 468 iterations completedProgress: 469 iterations completedProgress: 470 iterations completedProgress: 471 iterations completedProgress: 472 iterations completedProgress: 473 iterations completedProgress: 474 iterations completedProgress: 475 iterations completedProgress: 476 iterations completedProgress: 477 iterations completedProgress: 478 iterations completedProgress: 479 iterations completedProgress: 480 iterations completedProgress: 481 iterations completedProgress: 482 iterations completedProgress: 483 iterations completedProgress: 484 iterations completedProgress: 485 iterations completedProgress: 486 iterations completedProgress: 487 iterations completedProgress: 488 iterations completedProgress: 489 iterations completedProgress: 490 iterations completedProgress: 491 iterations completedProgress: 492 iterations completedProgress: 493 iterations completedProgress: 494 iterations completedProgress: 495 iterations completedProgress: 496 iterations completedProgress: 497 iterations completedProgress: 498 iterations completedProgress: 499 iterations completedProgress: 500 iterations completedProgress: 501 iterations completedProgress: 502 iterations completedProgress: 503 iterations completedProgress: 504 iterations completedProgress: 505 iterations completedProgress: 506 iterations completedProgress: 507 iterations completedProgress: 508 iterations completedProgress: 509 iterations completedProgress: 510 iterations completedProgress: 511 iterations completedProgress: 512 iterations completedProgress: 513 iterations completedProgress: 514 iterations completedProgress: 515 iterations completedProgress: 516 iterations completedProgress: 517 iterations completedProgress: 518 iterations completedProgress: 519 iterations completedProgress: 520 iterations completedProgress: 521 iterations completedProgress: 522 iterations completedProgress: 523 iterations completedProgress: 524 iterations completedProgress: 525 iterations completedProgress: 526 iterations completedProgress: 527 iterations completedProgress: 528 iterations completedProgress: 529 iterations completedProgress: 530 iterations completedProgress: 531 iterations completedProgress: 532 iterations completedProgress: 533 iterations completedProgress: 534 iterations completedProgress: 535 iterations completedProgress: 536 iterations completedProgress: 537 iterations completedProgress: 538 iterations completedProgress: 539 iterations completedProgress: 540 iterations completedProgress: 541 iterations completedProgress: 542 iterations completedProgress: 543 iterations completedProgress: 544 iterations completedProgress: 545 iterations completedProgress: 546 iterations completedProgress: 547 iterations completedProgress: 548 iterations completedProgress: 549 iterations completedProgress: 550 iterations completedProgress: 551 iterations completedProgress: 552 iterations completedProgress: 553 iterations completedProgress: 554 iterations completedProgress: 555 iterations completedProgress: 556 iterations completedProgress: 557 iterations completedProgress: 558 iterations completedProgress: 559 iterations completedProgress: 560 iterations completedProgress: 561 iterations completedProgress: 562 iterations completedProgress: 563 iterations completedProgress: 564 iterations completedProgress: 565 iterations completedProgress: 566 iterations completedProgress: 567 iterations completedProgress: 568 iterations completedProgress: 569 iterations completedProgress: 570 iterations completedProgress: 571 iterations completedProgress: 572 iterations completedProgress: 573 iterations completedProgress: 574 iterations completedProgress: 575 iterations completedProgress: 576 iterations completedProgress: 577 iterations completedProgress: 578 iterations completedProgress: 579 iterations completedProgress: 580 iterations completedProgress: 581 iterations completedProgress: 582 iterations completedProgress: 583 iterations completedProgress: 584 iterations completedProgress: 585 iterations completedProgress: 586 iterations completedProgress: 587 iterations completedProgress: 588 iterations completedProgress: 589 iterations completedProgress: 590 iterations completedProgress: 591 iterations completedProgress: 592 iterations completedProgress: 593 iterations completedProgress: 594 iterations completedProgress: 595 iterations completedProgress: 596 iterations completedProgress: 597 iterations completedProgress: 598 iterations completedProgress: 599 iterations completedProgress: 600 iterations completedProgress: 601 iterations completedProgress: 602 iterations completedProgress: 603 iterations completedProgress: 604 iterations completedProgress: 605 iterations completedProgress: 606 iterations completedProgress: 607 iterations completedProgress: 608 iterations completedProgress: 609 iterations completedProgress: 610 iterations completedProgress: 611 iterations completedProgress: 612 iterations completedProgress: 613 iterations completedProgress: 614 iterations completedProgress: 615 iterations completedProgress: 616 iterations completedProgress: 617 iterations completedProgress: 618 iterations completedProgress: 619 iterations completedProgress: 620 iterations completedProgress: 621 iterations completedProgress: 622 iterations completedProgress: 623 iterations completedProgress: 624 iterations completedProgress: 625 iterations completedProgress: 626 iterations completedProgress: 627 iterations completedProgress: 628 iterations completedProgress: 629 iterations completedProgress: 630 iterations completedProgress: 631 iterations completedProgress: 632 iterations completedProgress: 633 iterations completedProgress: 634 iterations completedProgress: 635 iterations completedProgress: 636 iterations completedProgress: 637 iterations completedProgress: 638 iterations completedProgress: 639 iterations completedProgress: 640 iterations completedProgress: 641 iterations completedProgress: 642 iterations completedProgress: 643 iterations completedProgress: 644 iterations completedProgress: 645 iterations completedProgress: 646 iterations completedProgress: 647 iterations completedProgress: 648 iterations completedProgress: 649 iterations completedProgress: 650 iterations completedProgress: 651 iterations completedProgress: 652 iterations completedProgress: 653 iterations completedProgress: 654 iterations completedProgress: 655 iterations completedProgress: 656 iterations completedProgress: 657 iterations completedProgress: 658 iterations completedProgress: 659 iterations completedProgress: 660 iterations completedProgress: 661 iterations completedProgress: 662 iterations completedProgress: 663 iterations completedProgress: 664 iterations completedProgress: 665 iterations completedProgress: 666 iterations completedProgress: 667 iterations completedProgress: 668 iterations completedProgress: 669 iterations completedProgress: 670 iterations completedProgress: 671 iterations completedProgress: 672 iterations completedProgress: 673 iterations completedProgress: 674 iterations completedProgress: 675 iterations completedProgress: 676 iterations completedProgress: 677 iterations completedProgress: 678 iterations completedProgress: 679 iterations completedProgress: 680 iterations completedProgress: 681 iterations completedProgress: 682 iterations completedProgress: 683 iterations completedProgress: 684 iterations completedProgress: 685 iterations completedProgress: 686 iterations completedProgress: 687 iterations completedProgress: 688 iterations completedProgress: 689 iterations completedProgress: 690 iterations completedProgress: 691 iterations completedProgress: 692 iterations completedProgress: 693 iterations completedProgress: 694 iterations completedProgress: 695 iterations completedProgress: 696 iterations completedProgress: 697 iterations completedProgress: 698 iterations completedProgress: 699 iterations completedProgress: 700 iterations completedProgress: 701 iterations completedProgress: 702 iterations completedProgress: 703 iterations completedProgress: 704 iterations completedProgress: 705 iterations completedProgress: 706 iterations completedProgress: 707 iterations completedProgress: 708 iterations completedProgress: 709 iterations completedProgress: 710 iterations completedProgress: 711 iterations completedProgress: 712 iterations completedProgress: 713 iterations completedProgress: 714 iterations completedProgress: 715 iterations completedProgress: 716 iterations completedProgress: 717 iterations completedProgress: 718 iterations completedProgress: 719 iterations completedProgress: 720 iterations completedProgress: 721 iterations completedProgress: 722 iterations completedProgress: 723 iterations completedProgress: 724 iterations completedProgress: 725 iterations completedProgress: 726 iterations completedProgress: 727 iterations completedProgress: 728 iterations completedProgress: 729 iterations completedProgress: 730 iterations completedProgress: 731 iterations completedProgress: 732 iterations completedProgress: 733 iterations completedProgress: 734 iterations completedProgress: 735 iterations completedProgress: 736 iterations completedProgress: 737 iterations completedProgress: 738 iterations completedProgress: 739 iterations completedProgress: 740 iterations completedProgress: 741 iterations completedProgress: 742 iterations completedProgress: 743 iterations completedProgress: 744 iterations completedProgress: 745 iterations completedProgress: 746 iterations completedProgress: 747 iterations completedProgress: 748 iterations completedProgress: 749 iterations completedProgress: 750 iterations completedProgress: 751 iterations completedProgress: 752 iterations completedProgress: 753 iterations completedProgress: 754 iterations completedProgress: 755 iterations completedProgress: 756 iterations completedProgress: 757 iterations completedProgress: 758 iterations completedProgress: 759 iterations completedProgress: 760 iterations completedProgress: 761 iterations completedProgress: 762 iterations completedProgress: 763 iterations completedProgress: 764 iterations completedProgress: 765 iterations completedProgress: 766 iterations completedProgress: 767 iterations completedProgress: 768 iterations completedProgress: 769 iterations completedProgress: 770 iterations completedProgress: 771 iterations completedProgress: 772 iterations completedProgress: 773 iterations completedProgress: 774 iterations completedProgress: 775 iterations completedProgress: 776 iterations completedProgress: 777 iterations completedProgress: 778 iterations completedProgress: 779 iterations completedProgress: 780 iterations completedProgress: 781 iterations completedProgress: 782 iterations completedProgress: 783 iterations completedProgress: 784 iterations completedProgress: 785 iterations completedProgress: 786 iterations completedProgress: 787 iterations completedProgress: 788 iterations completedProgress: 789 iterations completedProgress: 790 iterations completedProgress: 791 iterations completedProgress: 792 iterations completedProgress: 793 iterations completedProgress: 794 iterations completedProgress: 795 iterations completedProgress: 796 iterations completedProgress: 797 iterations completedProgress: 798 iterations completedProgress: 799 iterations completedProgress: 800 iterations completedProgress: 801 iterations completedProgress: 802 iterations completedProgress: 803 iterations completedProgress: 804 iterations completedProgress: 805 iterations completedProgress: 806 iterations completedProgress: 807 iterations completedProgress: 808 iterations completedProgress: 809 iterations completedProgress: 810 iterations completedProgress: 811 iterations completedProgress: 812 iterations completedProgress: 813 iterations completedProgress: 814 iterations completedProgress: 815 iterations completedProgress: 816 iterations completedProgress: 817 iterations completedProgress: 818 iterations completedProgress: 819 iterations completedProgress: 820 iterations completedProgress: 821 iterations completedProgress: 822 iterations completedProgress: 823 iterations completedProgress: 824 iterations completedProgress: 825 iterations completedProgress: 826 iterations completedProgress: 827 iterations completedProgress: 828 iterations completedProgress: 829 iterations completedProgress: 830 iterations completedProgress: 831 iterations completedProgress: 832 iterations completedProgress: 833 iterations completedProgress: 834 iterations completedProgress: 835 iterations completedProgress: 836 iterations completedProgress: 837 iterations completedProgress: 838 iterations completedProgress: 839 iterations completedProgress: 840 iterations completedProgress: 841 iterations completedProgress: 842 iterations completedProgress: 843 iterations completedProgress: 844 iterations completedProgress: 845 iterations completedProgress: 846 iterations completedProgress: 847 iterations completedProgress: 848 iterations completedProgress: 849 iterations completedProgress: 850 iterations completedProgress: 851 iterations completedProgress: 852 iterations completedProgress: 853 iterations completedProgress: 854 iterations completedProgress: 855 iterations completedProgress: 856 iterations completedProgress: 857 iterations completedProgress: 858 iterations completedProgress: 859 iterations completedProgress: 860 iterations completedProgress: 861 iterations completedProgress: 862 iterations completedProgress: 863 iterations completedProgress: 864 iterations completedProgress: 865 iterations completedProgress: 866 iterations completedProgress: 867 iterations completedProgress: 868 iterations completedProgress: 869 iterations completedProgress: 870 iterations completedProgress: 871 iterations completedProgress: 872 iterations completedProgress: 873 iterations completedProgress: 874 iterations completedProgress: 875 iterations completedProgress: 876 iterations completedProgress: 877 iterations completedProgress: 878 iterations completedProgress: 879 iterations completedProgress: 880 iterations completedProgress: 881 iterations completedProgress: 882 iterations completedProgress: 883 iterations completedProgress: 884 iterations completedProgress: 885 iterations completedProgress: 886 iterations completedProgress: 887 iterations completedProgress: 888 iterations completedProgress: 889 iterations completedProgress: 890 iterations completedProgress: 891 iterations completedProgress: 892 iterations completedProgress: 893 iterations completedProgress: 894 iterations completedProgress: 895 iterations completedProgress: 896 iterations completedProgress: 897 iterations completedProgress: 898 iterations completedProgress: 899 iterations completedProgress: 900 iterations completedProgress: 901 iterations completedProgress: 902 iterations completedProgress: 903 iterations completedProgress: 904 iterations completedProgress: 905 iterations completedProgress: 906 iterations completedProgress: 907 iterations completedProgress: 908 iterations completedProgress: 909 iterations completedProgress: 910 iterations completedProgress: 911 iterations completedProgress: 912 iterations completedProgress: 913 iterations completedProgress: 914 iterations completedProgress: 915 iterations completedProgress: 916 iterations completedProgress: 917 iterations completedProgress: 918 iterations completedProgress: 919 iterations completedProgress: 920 iterations completedProgress: 921 iterations completedProgress: 922 iterations completedProgress: 923 iterations completedProgress: 924 iterations completedProgress: 925 iterations completedProgress: 926 iterations completedProgress: 927 iterations completedProgress: 928 iterations completedProgress: 929 iterations completedProgress: 930 iterations completedProgress: 931 iterations completedProgress: 932 iterations completedProgress: 933 iterations completedProgress: 934 iterations completedProgress: 935 iterations completedProgress: 936 iterations completedProgress: 937 iterations completedProgress: 938 iterations completedProgress: 939 iterations completedProgress: 940 iterations completedProgress: 941 iterations completedProgress: 942 iterations completedProgress: 943 iterations completedProgress: 944 iterations completedProgress: 945 iterations completedProgress: 946 iterations completedProgress: 947 iterations completedProgress: 948 iterations completedProgress: 949 iterations completedProgress: 950 iterations completedProgress: 951 iterations completedProgress: 952 iterations completedProgress: 953 iterations completedProgress: 954 iterations completedProgress: 955 iterations completedProgress: 956 iterations completedProgress: 957 iterations completedProgress: 958 iterations completedProgress: 959 iterations completedProgress: 960 iterations completedProgress: 961 iterations completedProgress: 962 iterations completedProgress: 963 iterations completedProgress: 964 iterations completedProgress: 965 iterations completedProgress: 966 iterations completedProgress: 967 iterations completedProgress: 968 iterations completedProgress: 969 iterations completedProgress: 970 iterations completedProgress: 971 iterations completedProgress: 972 iterations completedProgress: 973 iterations completedProgress: 974 iterations completedProgress: 975 iterations completedProgress: 976 iterations completedProgress: 977 iterations completedProgress: 978 iterations completedProgress: 979 iterations completedProgress: 980 iterations completedProgress: 981 iterations completedProgress: 982 iterations completedProgress: 983 iterations completedProgress: 984 iterations completedProgress: 985 iterations completedProgress: 986 iterations completedProgress: 987 iterations completedProgress: 988 iterations completedProgress: 989 iterations completedProgress: 990 iterations completedProgress: 991 iterations completedProgress: 992 iterations completedProgress: 993 iterations completedProgress: 994 iterations completedProgress: 995 iterations completedProgress: 996 iterations completedProgress: 997 iterations completedProgress: 998 iterations completedProgress: 999 iterations completedProgress: 1000 iterations completed # Average the correlation matrix over the 1000 iterations avg_cor_matrix &lt;- cor_matrix_sum / 1000 # Print the averaged correlation matrix print(&quot;Averaged Correlation Matrix:&quot;) ## [1] &quot;Averaged Correlation Matrix:&quot; print(avg_cor_matrix) ## lm_pred rf_pred tuned_gbm_pred knn_pred ## lm_pred 1.0000000 0.9009099 0.8921886 0.8645067 ## rf_pred 0.9009099 1.0000000 0.9771105 0.9339113 ## tuned_gbm_pred 0.8921886 0.9771105 1.0000000 0.9153983 ## knn_pred 0.8645067 0.9339113 0.9153983 1.0000000 "],["intro-econ-stats.html", "Chapter 15 Intro Econ Stats", " Chapter 15 Intro Econ Stats Work in progess "],["nfl.html", "Chapter 16 NFL 16.1 WBs 16.2 RBs 16.3 QBs 16.4 Plots and Analysis 16.5 Lists to do 16.6 team mates 16.7 Testing", " Chapter 16 NFL 16.1 WBs library(readr) library(randomForest) library(caret) library(tidyr) library(pdp) library(dplyr) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Sort the dataset by player name and season data &lt;- data[order(data$name, data$season), ] names(data)[names(data) == &quot;height_cm&quot;] &lt;- &quot;height_in&quot; data &lt;- data[, !names(data) %in% c(&quot;height_ft&quot;)] # Split the dataset by position data_split &lt;- split(data, data$position) # Access the WR data wr_data &lt;- data_split$WR # Identify zero-variance columns zero_var_cols &lt;- nearZeroVar(wr_data) # Check and remove zero-variance columns, excluding &#39;position&#39; zero_var_colnames &lt;- colnames(wr_data)[zero_var_cols] cols_to_keep &lt;- names(wr_data)[!(names(wr_data) %in% zero_var_colnames) | names(wr_data) == &quot;position&quot;] wr_data &lt;- wr_data[, cols_to_keep] # Filter the data for the 2023 season to save it before we kill it wr_2023 &lt;- subset(wr_data, season == 2023) # Create new columns for the next season&#39;s fantasy points wr_data$next_fantasy_points &lt;- ave(wr_data$fantasy_points, wr_data$name, FUN = function(x) c(x[-1], NA)) wr_data$next_fantasy_points_ppr &lt;- ave(wr_data$fantasy_points_ppr, wr_data$name, FUN = function(x) c(x[-1], NA)) # View the first few rows of the updated data head(data) ## # A tibble: 6 × 75 ## id name height_in position team season season_type completions attempts ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 A.J. B… 73 WR TEN 2019 REG 0 0 ## 2 1 A.J. B… 72 WR TEN 2020 REG 0 0 ## 3 1 A.J. B… 73 WR TEN 2021 REG 0 2 ## 4 1 A.J. B… 73 WR PHI 2022 REG 0 0 ## 5 1 A.J. B… 73 WR PHI 2023 REG 0 0 ## 6 2 A.J. D… 77 TE DEN 2016 REG 0 0 ## # ℹ 66 more variables: passing_yards &lt;dbl&gt;, passing_tds &lt;dbl&gt;, ## # interceptions &lt;dbl&gt;, sacks &lt;dbl&gt;, sack_yards &lt;dbl&gt;, sack_fumbles &lt;dbl&gt;, ## # sack_fumbles_lost &lt;dbl&gt;, passing_air_yards &lt;dbl&gt;, ## # passing_yards_after_catch &lt;dbl&gt;, passing_first_downs &lt;dbl&gt;, ## # passing_2pt_conversions &lt;dbl&gt;, carries &lt;dbl&gt;, rushing_yards &lt;dbl&gt;, ## # rushing_tds &lt;dbl&gt;, rushing_fumbles &lt;dbl&gt;, rushing_fumbles_lost &lt;dbl&gt;, ## # rushing_first_downs &lt;dbl&gt;, rushing_2pt_conversions &lt;dbl&gt;, … wr_data &lt;- wr_data[wr_data$rookie_season != 0, ] # Count the number of NA values in each column na_table &lt;- colSums(is.na(wr_data)) print(na_table) ## id name ## 0 0 ## height_in position ## 0 0 ## team season ## 0 0 ## attempts carries ## 0 0 ## rushing_first_downs receptions ## 0 0 ## targets receiving_yards ## 0 0 ## receiving_tds receiving_fumbles ## 0 0 ## receiving_fumbles_lost receiving_air_yards ## 0 0 ## receiving_yards_after_catch receiving_first_downs ## 0 0 ## receiving_2pt_conversions target_share ## 0 0 ## air_yards_share fantasy_points ## 0 0 ## fantasy_points_ppr total_yards ## 0 0 ## games offense_snaps ## 0 0 ## teams_offense_snaps ypc ## 0 0 ## ypr touches ## 0 0 ## rec_td_percentage total_tds ## 0 0 ## td_percentage offense_pct ## 0 77 ## rush_ypg rec_ypg ## 0 0 ## ppg ppr_ppg ## 0 0 ## yps ypg ## 4 0 ## rookie_season round ## 0 0 ## forty bench ## 0 0 ## vertical years_played ## 0 0 ## fp_ps ppr_fp_ps ## 2 2 ## next_fantasy_points next_fantasy_points_ppr ## 675 675 # For now, remove all rows with NAs wr_data &lt;- na.omit(wr_data) # Drop the next_fantasy_points_ppr column wr_data &lt;- wr_data[, !names(wr_data) %in% &quot;next_fantasy_points_ppr&quot;] # Build Random Forest model wrrf_model &lt;- randomForest(next_fantasy_points ~ ., data = wr_data, ntree = 500, importance = TRUE) # Create a simplified dataset for linear regression lm_data &lt;- wr_data[, !names(wr_data) %in% c(&quot;name&quot;, &quot;id&quot;, &quot;position&quot;)] # Initialize a vector to store RMSPE values for each run rmspe &lt;- c() for(i in 1:10) { # Resample the dataset with replacement ind &lt;- unique(sample(nrow(lm_data), nrow(lm_data), replace = TRUE)) train &lt;- lm_data[ind, ] test &lt;- lm_data[-ind, ] # Build Linear Regression model lm_model &lt;- lm(next_fantasy_points ~ ., data = train) # Predict on the test data yhat &lt;- predict(lm_model, newdata = test) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((test$next_fantasy_points - yhat)^2)) } ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases # Calculate the mean RMSPE over the 10 runs mean_rmspe &lt;- mean(rmspe) print(paste(&quot;Mean RMSPE over 10 runs:&quot;, mean_rmspe)) ## [1] &quot;Mean RMSPE over 10 runs: 41.3810107307706&quot; # Optionally, print the RMSPE for each run to see variability print(rmspe) ## [1] 42.86088 40.23122 39.88330 41.61438 39.93325 39.26592 39.48694 46.47567 ## [9] 41.58921 42.46933 # Evaluate feature importance importance_values &lt;- importance(wrrf_model) # Sort and extract the top 10 features by %IncMSE and IncNodePurity top10_mse &lt;- importance_values[order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE), ][1:10, ] top10_purity &lt;- importance_values[order(importance_values[, &quot;IncNodePurity&quot;], decreasing = TRUE), ][1:10, ] # Visualize top 10 features by %IncMSE par(mar = c(5, 12, 4, 2)) # Increase margins for labels barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Mean Decrease in Accuracy)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Visualize top 10 features by IncNodePurity barplot(top10_purity[, &quot;IncNodePurity&quot;], names.arg = rownames(top10_purity), main = &quot;Top 10 Features by IncNodePurity (Mean Decrease in Gini)&quot;, las = 2, col = &quot;red&quot;, horiz = TRUE, cex.names = 0.8) oob_mse &lt;- wrrf_model$mse[wrrf_model$ntree] oobrmse &lt;- sqrt(oob_mse) # Print the OOB MSE print(paste(&quot;OOB rMSE:&quot;, oobrmse)) ## [1] &quot;OOB rMSE: 41.0136385880389&quot; # Extract OOB predictions oob_predictions &lt;- wrrf_model$predicted # Calculate the absolute errors absolute_errors &lt;- abs(oob_predictions - wrrf_model$y) # Calculate the Mean Absolute Error (MAE) oob_mae &lt;- mean(absolute_errors) # Print the OOB MAE print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 31.1016458934681&quot; # Predict 2024 fantasy points for 2023 WRs predicted_2024_fantasy_points &lt;- predict(wrrf_model, newdata = wr_2023) # Create a data frame with the names and the predicted 2024 fantasy points results_wr &lt;- data.frame(name = wr_2023$name, predicted_2024_fantasy_points = predicted_2024_fantasy_points) print(results_wr) ## name predicted_2024_fantasy_points ## 1 A.J. Brown 158.45374 ## 2 A.T. Perry 46.98033 ## 3 Adam Thielen 124.95694 ## 4 Alec Pierce 68.77777 ## 5 Alex Erickson 36.21638 ## 6 Allen Lazard 46.50182 ## 7 Allen Robinson 47.73667 ## 8 Amari Cooper 131.86314 ## 9 Amari Rodgers 26.28798 ## 10 Amon-Ra St. Brown 191.46057 ## 11 Andrei Iosivas 39.76441 ## 12 Antoine Green 23.44094 ## 13 Austin Trammell 18.70520 ## 14 Ben Skowronek 19.05888 ## 15 Bo Melton 66.26192 ## 16 Brandin Cooks 107.72528 ## 17 Brandon Aiyuk 146.09899 ## 18 Brandon Johnson 44.25428 ## 19 Brandon Powell 42.75481 ## 20 Braxton Berrios 39.57019 ## 21 Britain Covey 16.38348 ## 22 Byron Pringle 29.09906 ## 23 Calvin Austin 37.07823 ## 24 Calvin Ridley 129.37458 ## 25 Cedric Tillman 44.77454 ## 26 CeeDee Lamb 178.42080 ## 27 Charlie Jones 24.52458 ## 28 Chase Claypool 13.44951 ## 29 Chris Conley 54.77228 ## 30 Chris Godwin 116.59863 ## 31 Chris Moore 40.81207 ## 32 Chris Olave 140.68480 ## 33 Christian Kirk 94.48912 ## 34 Christian Watson 79.16359 ## 35 Cody Thompson 13.46552 ## 36 Collin Johnson 11.60196 ## 37 Colton Dowell 17.63330 ## 38 Cooper Kupp 108.18179 ## 39 Courtland Sutton 106.16625 ## 40 Curtis Samuel 70.29533 ## 41 D&#39;Wayne Eskridge 16.81773 ## 42 D.J. Chark NA ## 43 D.J. Montgomery 21.68741 ## 44 D.J. Moore 147.79211 ## 45 Darius Slayton 71.65614 ## 46 Darnell Mooney 54.48127 ## 47 Davante Adams 137.07375 ## 48 David Bell 34.92270 ## 49 David Moore 15.66785 ## 50 DeAndre Carter 18.94561 ## 51 DeAndre Hopkins 118.90245 ## 52 Deebo Samuel 128.11221 ## 53 Demarcus Robinson 55.84088 ## 54 Demario Douglas 69.68733 ## 55 Deonte Harty 22.67475 ## 56 Derius Davis 33.21416 ## 57 DeVante Parker 39.42930 ## 58 Deven Thompkins 24.45811 ## 59 Devin Duvernay 17.36587 ## 60 DeVonta Smith 132.16115 ## 61 Diontae Johnson 102.36024 ## 62 DK Metcalf NA ## 63 Donovan Peoples-Jones 14.79269 ## 64 Dontayvion Wicks 61.94750 ## 65 Drake London 99.63021 ## 66 Dyami Brown 33.08390 ## 67 Elijah Cooks 15.59058 ## 68 Elijah Moore 79.96089 ## 69 Equanimeous St. Brown 13.00758 ## 70 Erik Ezukanma 19.90174 ## 71 Gabe Davis NA ## 72 Garrett Wilson 126.36224 ## 73 George Pickens 119.40340 ## 74 Greg Dortch 43.35514 ## 75 Gunner Olszewski 22.09867 ## 76 Hunter Renfrow 40.32242 ## 77 Ihmir Smith-Marsette 26.83223 ## 78 Irvin Charles 17.05696 ## 79 Isaiah Hodgins 38.72475 ## 80 Isaiah McKenzie 26.47391 ## 81 Ja&#39;Marr Chase 136.88921 ## 82 Jahan Dotson 55.74635 ## 83 Jake Bobo 36.70905 ## 84 Jakobi Meyers 117.90660 ## 85 Jalen Brooks 24.20460 ## 86 Jalen Guyton 19.92515 ## 87 Jalen Nailor 16.68614 ## 88 Jalen Reagor 26.19103 ## 89 Jalen Tolbert 43.54475 ## 90 Jalin Hyatt 59.36283 ## 91 Jamal Agnew 33.10912 ## 92 James Proche 30.56557 ## 93 Jameson Williams 50.67995 ## 94 Jamison Crowder 32.64889 ## 95 Jason Brownlee 26.17210 ## 96 Jauan Jennings 35.18360 ## 97 Jaxon Smith-Njigba 73.66326 ## 98 Jayden Reed 130.41729 ## 99 Jaylen Waddle 131.01759 ## 100 Jerry Jeudy 75.48801 ## 101 Jonathan Mingo 81.77836 ## 102 Jordan Addison 122.30508 ## 103 Josh Downs 86.53400 ## 104 Josh Palmer 80.52307 ## 105 Josh Reynolds 51.50464 ## 106 JuJu Smith-Schuster 41.62285 ## 107 Julio Jones 22.27734 ## 108 Justin Jefferson 140.28859 ## 109 Justin Watson 59.12881 ## 110 Justyn Ross 16.03790 ## 111 Juwann Winfree 15.22209 ## 112 K.J. Osborn 46.32797 ## 113 Kadarius Toney 44.20388 ## 114 Kalif Raymond 52.16240 ## 115 Kayshon Boutte 21.17600 ## 116 Keelan Doss 19.44566 ## 117 Keenan Allen 151.40707 ## 118 Keith Kirkwood 13.54516 ## 119 Kendrick Bourne 65.40660 ## 120 KhaDarel Hodge 32.66039 ## 121 Khalil Shakir 64.91994 ## 122 Kwamie Lassiter NA ## 123 Kyle Philips 27.24527 ## 124 Laquon Treadwell 20.44431 ## 125 Laviska Shenault NA ## 126 Lil&#39;Jordan Humphrey 34.63423 ## 127 Lynn Bowden NA ## 128 Mack Hollins 33.64196 ## 129 Malik Heath 31.86248 ## 130 Malik Taylor 11.45799 ## 131 Marquez Valdes-Scantling 48.29091 ## 132 Marquise Brown 76.59450 ## 133 Marquise Goodwin 25.12094 ## 134 Marvin Jones 15.00472 ## 135 Marvin Mims 58.58277 ## 136 Mason Kinsey 16.65310 ## 137 Mecole Hardman 28.69464 ## 138 Michael Gallup 39.49602 ## 139 Michael Pittman NA ## 140 Michael Thomas 47.17063 ## 141 Michael Wilson 75.01664 ## 142 Mike Evans 137.53211 ## 143 Mike Strachan 19.72002 ## 144 Mike Williams 70.02213 ## 145 Miles Boykin 11.50276 ## 146 Nelson Agholor 44.03776 ## 147 Nick Westbrook-Ikhine 41.92132 ## 148 Nico Collins 144.81428 ## 149 Noah Brown 65.33498 ## 150 Olamide Zaccheaus 30.22371 ## 151 Parker Washington 29.45832 ## 152 Parris Campbell 25.03521 ## 153 Phillip Dorsett 25.38451 ## 154 Puka Nacua 177.88186 ## 155 Quentin Johnston 59.37848 ## 156 Quez Watkins 33.41007 ## 157 Rakim Jarrett 19.98260 ## 158 Randall Cobb 24.41586 ## 159 Rashee Rice 100.68511 ## 160 Rashid Shaheed 87.59368 ## 161 Rashod Bateman 58.97460 ## 162 Ray-Ray McCloud 23.30915 ## 163 Richie James 23.15227 ## 164 River Cracraft 15.36569 ## 165 Robbie Chosen 19.97650 ## 166 Robert Woods 51.73724 ## 167 Romeo Doubs 80.02505 ## 168 Rondale Moore 56.61910 ## 169 Ronnie Bell 31.29796 ## 170 Samori Toure 21.76577 ## 171 Scott Miller 30.93037 ## 172 Shedrick Jackson 18.44799 ## 173 Simi Fehoko 18.43777 ## 174 Skyy Moore 34.11112 ## 175 Stefon Diggs 136.06008 ## 176 Sterling Shepard 25.60539 ## 177 Steven Sims 17.01397 ## 178 Tank Dell 131.24927 ## 179 Tee Higgins 87.99714 ## 180 Terrace Marshall NA ## 181 Terry McLaurin 108.34191 ## 182 Tim Jones 31.20877 ## 183 Tre Tucker 59.63487 ## 184 Trent Sherfield 20.57037 ## 185 Trent Taylor 19.45240 ## 186 Trenton Irwin 34.80355 ## 187 Trey Palmer 62.84441 ## 188 Treylon Burks 52.32640 ## 189 Trishton Jackson 15.53072 ## 190 Tutu Atwell 56.67214 ## 191 Ty Montgomery 15.31157 ## 192 Tylan Wallace 14.89528 ## 193 Tyler Boyd 58.06943 ## 194 Tyler Johnson 19.57512 ## 195 Tyler Lockett 92.65408 ## 196 Tyler Scott 35.43359 ## 197 Tyquan Thornton 25.36256 ## 198 Tyreek Hill 194.43879 ## 199 Van Jefferson 41.56438 ## 200 Velus Jones NA ## 201 Wan&#39;Dale Robinson 67.81444 ## 202 Willie Snead 12.65379 ## 203 Xavier Gipson 44.94178 ## 204 Xavier Hutchinson 35.47533 ## 205 Zach Pascal 24.39360 ## 206 Zay Flowers 113.21412 ## 207 Zay Jones 60.16770 mse of specific seasons? we need to factor in the qbs and teamates of the players 16.2 RBs # Access the RB data rb_data &lt;- data_split$RB # Identify zero-variance columns zero_var_cols &lt;- nearZeroVar(rb_data) # Check and remove zero-variance columns, excluding &#39;position&#39; zero_var_colnames &lt;- colnames(rb_data)[zero_var_cols] cols_to_keep &lt;- names(rb_data)[!(names(rb_data) %in% zero_var_colnames) | names(rb_data) == &quot;position&quot;] rb_data &lt;- rb_data[, cols_to_keep] rb_2023 &lt;- subset(rb_data, season == 2023) # Create new columns for the next season&#39;s fantasy points rb_data$next_fantasy_points &lt;- ave(rb_data$fantasy_points, rb_data$name, FUN = function(x) c(x[-1], NA)) rb_data$next_fantasy_points_ppr &lt;- ave(rb_data$fantasy_points_ppr, rb_data$name, FUN = function(x) c(x[-1], NA)) # View the first few rows of the updated data head(rb_data) ## # A tibble: 6 × 55 ## id name height_in position team season carries rushing_yards rushing_tds ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 A.J. … 72 RB GB 2020 46 242 2 ## 2 3 A.J. … 72 RB GB 2021 187 803 5 ## 3 3 A.J. … 72 RB GB 2022 186 770 7 ## 4 10 Aaron… 69 RB GB 2017 81 448 4 ## 5 10 Aaron… 69 RB GB 2018 133 728 8 ## 6 10 Aaron… 69 RB GB 2019 236 1084 16 ## # ℹ 46 more variables: rushing_fumbles &lt;dbl&gt;, rushing_fumbles_lost &lt;dbl&gt;, ## # rushing_first_downs &lt;dbl&gt;, rushing_2pt_conversions &lt;dbl&gt;, receptions &lt;dbl&gt;, ## # targets &lt;dbl&gt;, receiving_yards &lt;dbl&gt;, receiving_tds &lt;dbl&gt;, ## # receiving_fumbles &lt;dbl&gt;, receiving_fumbles_lost &lt;dbl&gt;, ## # receiving_air_yards &lt;dbl&gt;, receiving_yards_after_catch &lt;dbl&gt;, ## # receiving_first_downs &lt;dbl&gt;, target_share &lt;dbl&gt;, air_yards_share &lt;dbl&gt;, ## # fantasy_points &lt;dbl&gt;, fantasy_points_ppr &lt;dbl&gt;, total_yards &lt;dbl&gt;, … rb_data &lt;- rb_data[rb_data$rookie_season != 0, ] # Count the number of NA values in each column na_table &lt;- colSums(is.na(rb_data)) print(na_table) ## id name ## 0 0 ## height_in position ## 0 0 ## team season ## 0 0 ## carries rushing_yards ## 0 0 ## rushing_tds rushing_fumbles ## 0 0 ## rushing_fumbles_lost rushing_first_downs ## 0 0 ## rushing_2pt_conversions receptions ## 0 0 ## targets receiving_yards ## 0 0 ## receiving_tds receiving_fumbles ## 0 0 ## receiving_fumbles_lost receiving_air_yards ## 0 0 ## receiving_yards_after_catch receiving_first_downs ## 0 0 ## target_share air_yards_share ## 0 0 ## fantasy_points fantasy_points_ppr ## 0 0 ## total_yards games ## 0 0 ## offense_snaps teams_offense_snaps ## 0 0 ## ypc ypr ## 0 0 ## touches rush_td_percentage ## 0 0 ## rec_td_percentage total_tds ## 0 0 ## td_percentage offense_pct ## 0 50 ## rush_ypg rec_ypg ## 0 0 ## ppg ppr_ppg ## 0 0 ## yps ypg ## 0 0 ## rookie_season round ## 0 0 ## overall forty ## 0 0 ## bench vertical ## 0 0 ## years_played fp_ps ## 0 0 ## ppr_fp_ps next_fantasy_points ## 0 429 ## next_fantasy_points_ppr ## 429 # For now, remove all rows with NAs rb_data &lt;- na.omit(rb_data) # Drop the next_fantasy_points_ppr column rb_data &lt;- rb_data[, !names(rb_data) %in% &quot;next_fantasy_points_ppr&quot;] # Build Random Forest model rbrf_model &lt;- randomForest(next_fantasy_points ~ ., data = rb_data, ntree = 500, importance = TRUE) # Create a simplified dataset for linear regression lm_data &lt;- rb_data[, !names(rb_data) %in% c(&quot;name&quot;, &quot;id&quot;, &quot;position&quot;)] # Initialize a vector to store RMSPE values for each run rmspe &lt;- c() for(i in 1:10) { # Resample the dataset with replacement ind &lt;- unique(sample(nrow(lm_data), nrow(lm_data), replace = TRUE)) train &lt;- lm_data[ind, ] test &lt;- lm_data[-ind, ] # Build Linear Regression model lm_model &lt;- lm(next_fantasy_points ~ ., data = train) # Predict on the test data yhat &lt;- predict(lm_model, newdata = test) # Calculate RMSPE rmspe[i] &lt;- sqrt(mean((test$next_fantasy_points - yhat)^2)) } ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases ## Warning in predict.lm(lm_model, newdata = test): prediction from rank-deficient ## fit; attr(*, &quot;non-estim&quot;) has doubtful cases # Calculate the mean RMSPE over the 10 runs mean_rmspe &lt;- mean(rmspe) print(paste(&quot;Mean RMSPE over 10 runs:&quot;, mean_rmspe)) ## [1] &quot;Mean RMSPE over 10 runs: 60.7854844772501&quot; # Optionally, print the RMSPE for each run to see variability print(rmspe) ## [1] 63.79008 60.63969 62.23519 60.39532 61.30245 62.32508 57.03457 56.46881 ## [9] 60.45813 63.20552 # Evaluate feature importance importance_values &lt;- importance(rbrf_model) # Sort and extract the top 10 features by %IncMSE and IncNodePurity top10_mse &lt;- importance_values[order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE), ][1:10, ] top10_purity &lt;- importance_values[order(importance_values[, &quot;IncNodePurity&quot;], decreasing = TRUE), ][1:10, ] # Visualize top 10 features by %IncMSE par(mar = c(5, 12, 4, 2)) # Increase margins for labels barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Mean Decrease in Accuracy)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Visualize top 10 features by IncNodePurity barplot(top10_purity[, &quot;IncNodePurity&quot;], names.arg = rownames(top10_purity), main = &quot;Top 10 Features by IncNodePurity (Mean Decrease in Gini)&quot;, las = 2, col = &quot;red&quot;, horiz = TRUE, cex.names = 0.8) # Calculate OOB MAE # Extract OOB predictions oob_predictions &lt;- rbrf_model$predicted # Calculate the absolute errors absolute_errors &lt;- abs(oob_predictions - rbrf_model$y) # Calculate the Mean Absolute Error (MAE) oob_mae &lt;- mean(absolute_errors) print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 42.8327659290246&quot; # Predict 2024 fantasy points for 2023 RBs predicted_2024_fantasy_points &lt;- predict(rbrf_model, newdata = rb_2023) # Create a data frame with the names and the predicted 2024 fantasy points results_rb &lt;- data.frame(name = rb_2023$name, predicted_2024_fantasy_points = predicted_2024_fantasy_points) print(results_rb) ## name predicted_2024_fantasy_points ## 1 Aaron Jones 118.04262 ## 2 Alex Armah 25.09065 ## 3 Alexander Mattison 84.97637 ## 4 Alvin Kamara 166.35528 ## 5 Ameer Abdullah 48.34743 ## 6 Anthony McFarland NA ## 7 Antonio Gibson 68.02155 ## 8 Austin Ekeler 152.20587 ## 9 Bijan Robinson 136.79453 ## 10 Boston Scott 17.78374 ## 11 Brandon Bolden 50.07336 ## 12 Breece Hall 155.01342 ## 13 Brian Robinson NA ## 14 Cam Akers 39.38520 ## 15 Chase Brown 80.88865 ## 16 Chase Edmonds 53.12924 ## 17 Chris Evans 50.61122 ## 18 Chris Rodriguez 59.85606 ## 19 Christian McCaffrey 212.77041 ## 20 Chuba Hubbard 110.80196 ## 21 Clyde Edwards-Helaire 52.44498 ## 22 Cordarrelle Patterson 36.70601 ## 23 Craig Reynolds 36.41821 ## 24 D&#39;Andre Swift 118.47322 ## 25 D&#39;Ernest Johnson 32.30104 ## 26 D&#39;Onta Foreman 58.50286 ## 27 Dameon Pierce 82.89876 ## 28 Damien Harris 23.31411 ## 29 Damien Williams 21.62917 ## 30 Dare Ogunbowale 42.97832 ## 31 Darrell Henderson 54.02835 ## 32 Darrynton Evans 39.19007 ## 33 David Montgomery 129.91948 ## 34 DeeJay Dallas 33.84922 ## 35 Deon Jackson 40.77150 ## 36 Derrick Gore 44.56223 ## 37 Derrick Henry 129.12637 ## 38 Deuce Vaughn 27.40912 ## 39 Devin Singletary 111.36104 ## 40 Devine Ozigbo 12.63288 ## 41 Elijah Dotson 26.75654 ## 42 Elijah Mitchell 50.35636 ## 43 Emanuel Wilson 22.55430 ## 44 Emari Demercado 70.48332 ## 45 Eric Gray 27.81213 ## 46 Evan Hull 37.88231 ## 47 Ezekiel Elliott 88.84338 ## 48 Gary Brightwell 22.48163 ## 49 Gus Edwards 102.98812 ## 50 Hunter Luepke 23.78956 ## 51 Isaiah Spiller 32.32182 ## 52 Isiah Pacheco 155.74796 ## 53 Israel Abanikanda 23.44183 ## 54 J.K. Dobbins 47.20577 ## 55 Jahmyr Gibbs 169.47437 ## 56 Jaleel McLaughlin 113.92307 ## 57 Jamaal Williams 43.21933 ## 58 James Conner 141.83972 ## 59 James Cook 123.57604 ## 60 Jashaun Corbin 37.93056 ## 61 Javonte Williams 93.81538 ## 62 Jaylen Warren 120.62452 ## 63 Jerick McKinnon 63.07916 ## 64 Jerome Ford 114.49215 ## 65 Joe Mixon 152.96433 ## 66 Jonathan Taylor 139.61376 ## 67 Jonathan Ward 22.94492 ## 68 Jonathan Williams 54.93015 ## 69 Jordan Mason 51.34162 ## 70 Jordan Mims 49.01447 ## 71 Josh Jacobs 159.46634 ## 72 Joshua Kelley 65.14497 ## 73 Justice Hill 65.55675 ## 74 Kareem Hunt 64.13019 ## 75 Keaton Mitchell 87.22186 ## 76 Kendre Miller 57.41223 ## 77 Kene Nwangwu 15.80442 ## 78 Kenneth Gainwell 67.31551 ## 79 Kenneth Walker NA ## 80 Kenyan Drake 42.61243 ## 81 Kevin Harris 31.02931 ## 82 Khalil Herbert 77.41246 ## 83 Kyren Williams 173.44194 ## 84 La&#39;Mical Perine 41.98330 ## 85 Latavius Murray 58.62617 ## 86 Leonard Fournette 44.06344 ## 87 Matt Breida 39.62983 ## 88 Melvin Gordon 29.20249 ## 89 Michael Carter 45.15985 ## 90 Miles Sanders 67.67319 ## 91 Najee Harris 103.52405 ## 92 Nick Chubb 65.56131 ## 93 Patrick Taylor 43.48695 ## 94 Pierre Strong 69.88942 ## 95 Rachaad White 153.88568 ## 96 Raheem Blackshear 15.10576 ## 97 Raheem Mostert 144.29180 ## 98 Rashaad Penny 26.38452 ## 99 Rhamondre Stevenson 133.91477 ## 100 Rico Dowdle 81.54722 ## 101 Ronnie Rivers 47.11891 ## 102 Roschon Johnson 101.38723 ## 103 Royce Freeman 48.34645 ## 104 Salvon Ahmed 26.08709 ## 105 Samaje Perine 72.79172 ## 106 Saquon Barkley 137.55416 ## 107 Sean Tucker 21.35669 ## 108 Tank Bigsby 74.38335 ## 109 Tony Jones 29.54301 ## 110 Tony Pollard 143.83668 ## 111 Travis Etienne 159.74291 ## 112 Travis Homer 44.79461 ## 113 Trayveon Williams 47.70385 ## 114 Trey Sermon 42.27016 ## 115 Ty Chandler 87.44841 ## 116 Ty Johnson 31.38996 ## 117 Tyjae Spears 114.03370 ## 118 Tyler Allgeier 111.67694 ## 119 Tyler Goodson 30.34336 ## 120 Tyrion Davis-Price 49.65526 ## 121 Zach Charbonnet 113.75264 ## 122 Zach Evans 39.63292 ## 123 Zack Moss 91.01460 ## 124 Zamir White 88.61307 16.3 QBs # Access the QB data qb_data &lt;- data_split$QB # Identify zero-variance columns zero_var_cols &lt;- nearZeroVar(qb_data) # Check and remove zero-variance columns zero_var_colnames &lt;- colnames(qb_data)[zero_var_cols] qb_data &lt;- qb_data[, !names(qb_data) %in% zero_var_colnames] # Filter the data for the 2023 season qb_2023 &lt;- subset(qb_data, season == 2023) # Create new columns for the next season&#39;s fantasy points qb_data$next_fantasy_points &lt;- ave(qb_data$fantasy_points, qb_data$name, FUN = function(x) c(x[-1], NA)) # Remove rows where rookie_season is 0 qb_data &lt;- qb_data[qb_data$rookie_season != 0, ] # Remove rows with NAs qb_data &lt;- na.omit(qb_data) # Build Random Forest model qbrf_model &lt;- randomForest(next_fantasy_points ~ ., data = qb_data, ntree = 500, importance = TRUE) # Evaluate feature importance importance_values &lt;- importance(qbrf_model) # Sort and extract the top 10 features by %IncMSE and IncNodePurity top10_mse &lt;- importance_values[order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE), ][1:10, ] top10_purity &lt;- importance_values[order(importance_values[, &quot;IncNodePurity&quot;], decreasing = TRUE), ][1:10, ] # Visualize top 10 features by %IncMSE par(mar = c(5, 12, 4, 2)) # Increase margins for labels barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Mean Decrease in Accuracy)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Visualize top 10 features by IncNodePurity barplot(top10_purity[, &quot;IncNodePurity&quot;], names.arg = rownames(top10_purity), main = &quot;Top 10 Features by IncNodePurity (Mean Decrease in Gini)&quot;, las = 2, col = &quot;red&quot;, horiz = TRUE, cex.names = 0.8) # Calculate OOB rmse oob_rmse &lt;- sqrt(qbrf_model$mse[qbrf_model$ntree]) # Extract OOB predictions oob_predictions &lt;- qbrf_model$predicted # Calculate the absolute errors absolute_errors &lt;- abs(oob_predictions - qbrf_model$y) # Calculate the Mean Absolute Error (MAE) oob_mae &lt;- mean(absolute_errors) print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 61.9047808581408&quot; oob_rmse ## [1] 82.62143 # Predict 2024 fantasy points for 2023 QBs predicted_2024_fantasy_points &lt;- predict(qbrf_model, newdata = qb_2023) # Create a data frame with the names and the predicted 2024 fantasy points results_qb &lt;- data.frame(name = qb_2023$name, predicted_2024_fantasy_points = predicted_2024_fantasy_points) print(head(results_qb)) ## name predicted_2024_fantasy_points ## 1 Aaron Rodgers 82.75772 ## 2 Aidan O&#39;Connell 131.55928 ## 3 AJ McCarron NA ## 4 Andy Dalton 89.99066 ## 5 Anthony Richardson 148.35138 ## 6 Bailey Zappe 129.82826 16.4 Plots and Analysis # For QB model (qbrf_model) pdp_qb_games &lt;- partial(qbrf_model, pred.var = &quot;games&quot;, plot = TRUE, main = &quot;PDP for Games (QB Random Forest Model)&quot;, xlab = &quot;Games&quot;, ylab = &quot;Partial Dependence&quot;) # For WR model (wrrf_model) pdp_wr_games &lt;- partial(wrrf_model, pred.var = &quot;games&quot;, plot = TRUE, main = &quot;PDP for Games (WR Random Forest Model)&quot;, xlab = &quot;Games&quot;, ylab = &quot;Partial Dependence&quot;) # For RB model (rbrf_model) pdp_rb_games &lt;- partial(rbrf_model, pred.var = &quot;games&quot;, plot = TRUE, main = &quot;PDP for Games (RB Random Forest Model)&quot;, xlab = &quot;Games&quot;, ylab = &quot;Partial Dependence&quot;) pdp_qb_games pdp_wr_games pdp_rb_games Games played is being tracked by other statistics… i dont give a crap about games played i only care about your stats at the end of the season… 16.5 Lists to do need to look at if multiple years back is good idea(i dont think so because then can no use 2nd yr players), and to be able to add teamates. also look if we are good for the seasons that there are 17 games. break the seasons down into quarters. 16.6 team mates library(nflfastR) ## Warning: package &#39;nflfastR&#39; was built under R version 4.3.3 library(dplyr) # Fetch rosters for 2024 rosters &lt;- fast_scraper_roster(season = 2024) # Filter WR and RB positions wr_rosters &lt;- rosters %&gt;% filter(position == &quot;WR&quot;) rb_rosters &lt;- rosters %&gt;% filter(position == &quot;RB&quot;) # Rename columns to match merging requirements colnames(wr_2023)[colnames(wr_2023) == &quot;team.x&quot;] &lt;- &quot;team&quot; colnames(rb_2023)[colnames(rb_2023) == &quot;team.x&quot;] &lt;- &quot;team&quot; # Merge WR data with rosters wr_teams &lt;- merge(wr_2023, wr_rosters, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) colnames(wr_teams)[colnames(wr_teams) == &quot;team.x&quot;] &lt;- &quot;team&quot; # Use team.x for original team colnames(wr_teams)[colnames(wr_teams) == &quot;team.y&quot;] &lt;- &quot;new_team&quot; # Use team.y for updated team wr_teams &lt;- wr_teams[, c(&quot;name&quot;, &quot;team&quot;)] # Merge RB data with rosters rb_teams &lt;- merge(rb_2023, rb_rosters, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) colnames(rb_teams)[colnames(rb_teams) == &quot;team.x&quot;] &lt;- &quot;team&quot; # Use team.x for original team colnames(rb_teams)[colnames(rb_teams) == &quot;team.y&quot;] &lt;- &quot;new_team&quot; # Use team.y for updated team rb_teams &lt;- rb_teams[, c(&quot;name&quot;, &quot;team&quot;)] create wr_2023-2024 that has the 2023 stats that have the players on there correct teams. we will do the same for the rb data # Load the necessary libraries library(readr) library(randomForest) library(caret) library(tidyr) library(pdp) library(dplyr) library(nflfastR) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Sort the dataset by player name and season data &lt;- data[order(data$name, data$season), ] # Adjust columns names(data)[names(data) == &quot;height_cm&quot;] &lt;- &quot;height_in&quot; data &lt;- data[, !names(data) %in% c(&quot;height_ft&quot;)] # Split the dataset by position data_split &lt;- split(data, data$position) # Access the WR and RB data wr_data &lt;- data_split$WR rb_data &lt;- data_split$RB # Identify zero-variance columns and remove them, excluding &#39;position&#39; # For WR data zero_var_cols_wr &lt;- nearZeroVar(wr_data) zero_var_colnames_wr &lt;- colnames(wr_data)[zero_var_cols_wr] cols_to_remove_wr &lt;- zero_var_colnames_wr[zero_var_colnames_wr != &quot;position&quot;] wr_data &lt;- wr_data[, !names(wr_data) %in% cols_to_remove_wr] # For RB data zero_var_cols_rb &lt;- nearZeroVar(rb_data) zero_var_colnames_rb &lt;- colnames(rb_data)[zero_var_cols_rb] cols_to_remove_rb &lt;- zero_var_colnames_rb[zero_var_colnames_rb != &quot;position&quot;] rb_data &lt;- rb_data[, !names(rb_data) %in% cols_to_remove_rb] # Assuming the previous steps up to filtering the 2023 data are done # Filter the data for the 2023 season wr_2023 &lt;- subset(wr_data, season == 2023) rb_2023 &lt;- subset(rb_data, season == 2023) # Fetch current rosters for 2024 rosters &lt;- fast_scraper_roster(season = 2024) wr_rosters &lt;- rosters %&gt;% filter(position == &quot;WR&quot;) rb_rosters &lt;- rosters %&gt;% filter(position == &quot;RB&quot;) # For WR data wr_rosters_temp &lt;- wr_rosters[, c(&quot;full_name&quot;, &quot;team&quot;)] names(wr_rosters_temp)[names(wr_rosters_temp) == &quot;team&quot;] &lt;- &quot;team_roster&quot; wr_2023_2024 &lt;- merge(wr_2023, wr_rosters_temp, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) # Replace NA in team with team_roster if available wr_2023_2024$team &lt;- ifelse(is.na(wr_2023_2024$team_roster), wr_2023_2024$team, wr_2023_2024$team_roster) wr_2023_2024 &lt;- wr_2023_2024[, !names(wr_2023_2024) %in% &quot;team_roster&quot;] # For RB data rb_rosters_temp &lt;- rb_rosters[, c(&quot;full_name&quot;, &quot;team&quot;)] names(rb_rosters_temp)[names(rb_rosters_temp) == &quot;team&quot;] &lt;- &quot;team_roster&quot; rb_2023_2024 &lt;- merge(rb_2023, rb_rosters_temp, by.x = &quot;name&quot;, by.y = &quot;full_name&quot;, all.x = TRUE) # Replace NA in team with team_roster if available rb_2023_2024$team &lt;- ifelse(is.na(rb_2023_2024$team_roster), rb_2023_2024$team, rb_2023_2024$team_roster) rb_2023_2024 &lt;- rb_2023_2024[, !names(rb_2023_2024) %in% &quot;team_roster&quot;] # Now, wr_2023_2024 and rb_2023_2024 should have the correct teams for the players in 2023, with fallback to original team if NA. now in wr_data and rb we need to look to include teamate stats. (we will group them for the 2023/2024 one later) we will create collums that have a the teamates states. to determine a teamate use the team and the season… now we obviously cant use every teamate. so for now we wil use the best qb on that team the 2 best wrs(that isnt themselves) and the best rb… now we have to determine what best is. best for qb will be pass yards. best for rbs will be rush yards. and best for wrs will be receving yards we may need to restructure it from the begining a bit because some colloms where dropped but u figure that out # Load the necessary libraries library(readr) library(dplyr) library(tidyr) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Sort the dataset by player name and season data &lt;- data[order(data$name, data$season), ] # Adjust columns names(data)[names(data) == &quot;height_cm&quot;] &lt;- &quot;height_in&quot; # Rename column data &lt;- data[, !names(data) %in% &quot;height_ft&quot;] # Remove height_ft column # Split the dataset by position data_split &lt;- split(data, data$position) # Access the WR, RB, and QB data wr_data &lt;- data_split$WR rb_data &lt;- data_split$RB qb_data &lt;- data_split$QB # Load the necessary libraries library(readr) library(dplyr) # Load the offensive yearly data data &lt;- read_csv(&quot;offense_yearly_data.csv&quot;) ## Rows: 5453 Columns: 76 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): name, height_ft, position, team, season_type ## dbl (71): id, height_cm, season, completions, attempts, passing_yards, passi... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Group by season and team to get a column with the names of all team members get_team_members &lt;- function(data) { data %&gt;% group_by(season, team) %&gt;% summarise(team_members = paste(name, collapse = &quot;, &quot;)) %&gt;% ungroup() } # Apply the function to WR, RB, and QB data wr_team_members &lt;- get_team_members(data_split$WR) ## `summarise()` has grouped output by &#39;season&#39;. You can override using the ## `.groups` argument. rb_team_members &lt;- get_team_members(data_split$RB) ## `summarise()` has grouped output by &#39;season&#39;. You can override using the ## `.groups` argument. qb_team_members &lt;- get_team_members(data_split$QB) ## `summarise()` has grouped output by &#39;season&#39;. You can override using the ## `.groups` argument. # Function to get team members get_team_members &lt;- function(data) { data %&gt;% group_by(season, team) %&gt;% summarise(team_members = paste(name, collapse = &quot;, &quot;)) %&gt;% ungroup() } # Apply the function to existing data wr_team_members &lt;- get_team_members(data_split$WR) ## `summarise()` has grouped output by &#39;season&#39;. You can override using the ## `.groups` argument. rb_team_members &lt;- get_team_members(data_split$RB) ## `summarise()` has grouped output by &#39;season&#39;. You can override using the ## `.groups` argument. qb_team_members &lt;- get_team_members(data_split$QB) ## `summarise()` has grouped output by &#39;season&#39;. You can override using the ## `.groups` argument. # Function to get top WRs by team get_top_wr_team_members &lt;- function(ranked_wr_data) { # Sort data ranked_wr_data &lt;- ranked_wr_data[order(ranked_wr_data$season, ranked_wr_data$team, ranked_wr_data$rank), ] # Function to get nth value safely get_nth &lt;- function(x, n) { if(length(x) &gt;= n) return(x[n]) return(NA) } # Split data by season and team split_data &lt;- split(ranked_wr_data, list(ranked_wr_data$season, ranked_wr_data$team)) # Create result data frame result_list &lt;- lapply(split_data, function(group) { # If the group is empty, return NULL if (nrow(group) == 0) { return(NULL) } data.frame( season = unique(group$season), team = unique(group$team), WR1_name = get_nth(group$name, 1), WR1_yards = get_nth(group$receiving_yards, 1), WR2_name = get_nth(group$name, 2), WR2_yards = get_nth(group$receiving_yards, 2), WR3_name = get_nth(group$name, 3), WR3_yards = get_nth(group$receiving_yards, 3), WR4_name = get_nth(group$name, 4), WR4_yards = get_nth(group$receiving_yards, 4), WR5_name = get_nth(group$name, 5), WR5_yards = get_nth(group$receiving_yards, 5), stringsAsFactors = FALSE ) }) # Filter out NULL entries result_list &lt;- Filter(Negate(is.null), result_list) # Combine all data frames into one result &lt;- do.call(rbind, result_list) # Create team_key result$team_key &lt;- paste(result$team, result$season, sep = &quot;_&quot;) # Reorder columns col_order &lt;- c(&quot;team_key&quot;, &quot;season&quot;, &quot;team&quot;, names(result)[!names(result) %in% c(&quot;team_key&quot;, &quot;season&quot;, &quot;team&quot;)]) result &lt;- result[, col_order] rownames(result) &lt;- NULL return(result) } # Create a function to calculate dense rank dense_rank_desc &lt;- function(x) { # Sort unique values in descending order sorted_unique &lt;- sort(unique(x), decreasing = TRUE) # Create rank mapping rank_map &lt;- seq_along(sorted_unique) names(rank_map) &lt;- sorted_unique # Return ranks rank_map[as.character(x)] } # Split data by season and team splits &lt;- split(wr_data, list(wr_data$season, wr_data$team)) # Function to process each group process_group &lt;- function(group) { if(nrow(group) == 0) return(NULL) # Calculate rank within group group$rank &lt;- dense_rank_desc(group$receiving_yards) # Select only needed columns cols_to_keep &lt;- c(&quot;season&quot;, &quot;team&quot;, &quot;name&quot;, &quot;receiving_yards&quot;, &quot;rank&quot;) group &lt;- group[, cols_to_keep[cols_to_keep %in% names(group)]] return(group) } # Apply processing to each group and combine results ranked_wr_data &lt;- do.call(rbind, lapply(splits, process_group)) # Reset row names rownames(ranked_wr_data) &lt;- NULL # Create WR summary wr_summary &lt;- get_top_wr_team_members(ranked_wr_data) # Check structure str(wr_summary) ## &#39;data.frame&#39;: 326 obs. of 13 variables: ## $ team_key : chr &quot;ARI_2016&quot; &quot;ARI_2017&quot; &quot;ARI_2018&quot; &quot;ARI_2019&quot; ... ## $ season : num 2016 2017 2018 2019 2020 ... ## $ team : chr &quot;ARI&quot; &quot;ARI&quot; &quot;ARI&quot; &quot;ARI&quot; ... ## $ WR1_name : chr &quot;Larry Fitzgerald&quot; &quot;Larry Fitzgerald&quot; &quot;Larry Fitzgerald&quot; &quot;Larry Fitzgerald&quot; ... ## $ WR1_yards: num 1023 1156 734 804 1407 ... ## $ WR2_name : chr &quot;J.J. Nelson&quot; &quot;J.J. Nelson&quot; &quot;Christian Kirk&quot; &quot;Christian Kirk&quot; ... ## $ WR2_yards: num 568 508 590 709 621 848 709 565 711 921 ... ## $ WR3_name : chr &quot;John Brown&quot; &quot;Jaron Brown&quot; &quot;Trent Sherfield&quot; &quot;Damiere Byrd&quot; ... ## $ WR3_yards: num 517 477 210 359 409 572 467 352 580 556 ... ## $ WR4_name : chr &quot;Jaron Brown&quot; &quot;John Brown&quot; &quot;Chad Williams&quot; &quot;Pharoh Cooper&quot; ... ## $ WR4_yards: num 187 299 171 243 224 435 414 280 210 504 ... ## $ WR5_name : chr &quot;Brittan Golden&quot; &quot;Brittan Golden&quot; &quot;J.J. Nelson&quot; &quot;Andy Isabella&quot; ... ## $ WR5_yards: num 82 70 64 189 173 208 236 19 12 102 ... 16.7 Testing # Create team_key and merge with wr_summary wr_data_final &lt;- wr_data wr_data_final$team_key &lt;- paste(wr_data_final$team, wr_data_final$season, sep = &quot;_&quot;) wr_data_final &lt;- merge(wr_data_final, wr_summary, by = &quot;team_key&quot;, all.x = TRUE) # Rename columns and create initial wr_dataf wr_dataf &lt;- wr_data_final names(wr_dataf)[names(wr_dataf) == &quot;season.x&quot;] &lt;- &quot;season&quot; names(wr_dataf)[names(wr_dataf) == &quot;team.x&quot;] &lt;- &quot;team&quot; names(wr_dataf)[names(wr_dataf) == &quot;name&quot;] &lt;- &quot;player_name&quot; # Remove ID column if it exists if(&quot;id&quot; %in% names(wr_dataf)) { wr_dataf &lt;- wr_dataf[, !names(wr_dataf) == &quot;id&quot;] } # Subset 2023 data wr_2023 &lt;- wr_dataf[wr_dataf$season == 2023, ] # Create next season&#39;s fantasy points create_next_points &lt;- function(points, player) { ave(points, player, FUN = function(x) c(x[-1], NA)) } wr_dataf$next_fantasy_points &lt;- create_next_points(wr_dataf$fantasy_points, wr_dataf$player_name) wr_dataf$next_fantasy_points_ppr &lt;- create_next_points(wr_dataf$fantasy_points_ppr, wr_dataf$player_name) # Filter rookie seasons wr_dataf &lt;- wr_dataf[wr_dataf$rookie_season != 0, ] wr_dataf &lt;- wr_dataf[wr_dataf$rookie_season != wr_dataf$season, ] # Remove next_fantasy_points_ppr wr_dataf$next_fantasy_points_ppr &lt;- NULL # Remove zero-variance columns n_distinct &lt;- function(x) length(unique(x)) zero_var_cols &lt;- names(wr_dataf)[sapply(wr_dataf, n_distinct) == 1] wr_dataf &lt;- wr_dataf[, !names(wr_dataf) %in% zero_var_cols] # Remove non-predictive columns columns_to_remove &lt;- c(&quot;player_name&quot;, &quot;team_key&quot;, &quot;WR2_name&quot;, &quot;WR1_name&quot;, &quot;RB3_name&quot;, &quot;RB2_name&quot;, &quot;RB1_name&quot;, &quot;QB_name&quot;, &quot;season&quot;, &quot;team&quot;, &quot;season.y&quot;, &quot;team.y&quot;) wr_dataf &lt;- wr_dataf[, !names(wr_dataf) %in% columns_to_remove] # Train Random Forest model rf_model &lt;- randomForest(next_fantasy_points ~ ., ntree = 1200, data = wr_dataf, importance = TRUE, na.action = na.omit) # Calculate performance metrics oob_predictions &lt;- rf_model$predicted absolute_errors &lt;- abs(oob_predictions - rf_model$y) oob_mae &lt;- mean(absolute_errors) oob_mse &lt;- rf_model$mse[1200] oobrmse &lt;- sqrt(oob_mse) # Print metrics print(paste(&quot;OOB MAE:&quot;, oob_mae)) ## [1] &quot;OOB MAE: 33.185773344464&quot; print(paste(&quot;OOB rMSE:&quot;, oobrmse)) ## [1] &quot;OOB rMSE: 43.5526590552396&quot; # Feature importance importance_values &lt;- importance(rf_model) ordered_importance &lt;- order(importance_values[, &quot;%IncMSE&quot;], decreasing = TRUE) top10_mse &lt;- importance_values[ordered_importance[1:10], ] # Plot importance par(mar = c(5, 12, 4, 2)) barplot(top10_mse[, &quot;%IncMSE&quot;], names.arg = rownames(top10_mse), main = &quot;Top 10 Features by %IncMSE (Enhanced Model)&quot;, las = 2, col = &quot;blue&quot;, horiz = TRUE, cex.names = 0.8) # Save player names and clean 2023 data nms &lt;- wr_2023$player_name wr_2023 &lt;- wr_2023[, !names(wr_2023) %in% c(zero_var_cols, columns_to_remove)] # Generate predictions predicted_2024_fantasy_points &lt;- predict(rf_model, newdata = wr_2023) # Create and sort results results_wr &lt;- data.frame( name = nms, predicted_2024_fantasy_points = predicted_2024_fantasy_points ) results_wr &lt;- results_wr[order(results_wr$predicted_2024_fantasy_points, decreasing = TRUE), ] # Print predictions print(&quot;2024 WR Predictions (Enhanced Model):&quot;) ## [1] &quot;2024 WR Predictions (Enhanced Model):&quot; print(results_wr) ## name predicted_2024_fantasy_points ## 1243 Tyreek Hill 185.162621 ## 709 Amon-Ra St. Brown 181.902197 ## 571 CeeDee Lamb 176.375749 ## 1104 Puka Nacua 173.810705 ## 1667 A.J. Brown 156.575436 ## 836 Nico Collins 147.701385 ## 1149 Keenan Allen 144.626829 ## 1298 Justin Jefferson 138.842454 ## 381 D.J. Moore 138.780915 ## 1869 Brandon Aiyuk 138.453522 ## 1437 Chris Olave 137.055756 ## 449 Ja&#39;Marr Chase 135.917803 ## 835 Tank Dell 135.092963 ## 1950 Mike Evans 129.856823 ## 1173 Davante Adams 127.095602 ## 1874 Deebo Samuel 125.512753 ## 569 Brandin Cooks 125.328287 ## 1244 Jaylen Waddle 124.441766 ## 1732 George Pickens 120.920435 ## 503 Amari Cooper 118.602809 ## 244 Stefon Diggs 117.551138 ## 1664 DeVonta Smith 112.078249 ## 2023 DeAndre Hopkins 111.497775 ## 984 Calvin Ridley 108.000884 ## 1602 Garrett Wilson 104.717264 ## 1949 Chris Godwin 104.210092 ## 780 Jayden Reed 103.529231 ## 1296 Jordan Addison 100.667143 ## 2101 Terry McLaurin 100.389592 ## 1734 Diontae Johnson 99.751770 ## 49 Marquise Brown 96.965655 ## 986 Christian Kirk 95.732603 ## 1370 JuJu Smith-Schuster 95.419767 ## 1109 Cooper Kupp 93.957530 ## 170 Nelson Agholor 92.892029 ## 1801 Tyler Lockett 92.884345 ## 640 Courtland Sutton 92.713896 ## 505 Elijah Moore 90.776869 ## 169 Zay Flowers 90.768064 ## 711 Marvin Jones 89.829859 ## 1441 Rashid Shaheed 89.650017 ## 456 Tee Higgins 89.281682 ## 119 Drake London 88.013422 ## 784 Romeo Doubs 87.764446 ## 786 Christian Watson 85.741719 ## 639 Jerry Jeudy 80.608937 ## 1105 Tutu Atwell 78.751648 ## 1055 Rashee Rice 78.091953 ## 907 Josh Downs 77.574511 ## 1150 Josh Palmer 76.983056 ## 834 Robert Woods 76.829277 ## 1371 Demario Douglas 76.507717 ## 1247 Chase Claypool 75.226794 ## 2096 Curtis Samuel 74.858425 ## 300 Jonathan Mingo 73.380302 ## 1153 Quentin Johnston 73.238412 ## 453 Tyler Boyd 72.865363 ## 1733 Allen Robinson 71.645617 ## 2100 Jahan Dotson 71.436261 ## 1525 Darius Slayton 71.088471 ## 785 Bo Melton 70.135551 ## 1524 Wan&#39;Dale Robinson 69.948916 ## 1800 Jaxon Smith-Njigba 69.461586 ## 47 Michael Wilson 68.431257 ## 306 Adam Thielen 67.960838 ## 782 Dontayvion Wicks 67.672484 ## 1107 Demarcus Robinson 67.517942 ## 905 Alec Pierce 66.141972 ## 120 Mack Hollins 65.823145 ## 1603 Randall Cobb 65.553190 ## 1175 Jakobi Meyers 64.292969 ## 168 Rashod Bateman 63.942489 ## 455 Trenton Irwin 63.889527 ## 1665 Julio Jones 63.091664 ## 1365 DeVante Parker 61.914881 ## 242 Khalil Shakir 61.345004 ## 1440 Michael Thomas 60.914386 ## 1605 Allen Lazard 58.735050 ## 1522 Jalin Hyatt 58.300095 ## 507 Marquise Goodwin 57.427049 ## 1151 Mike Williams 57.196543 ## 506 Cedric Tillman 56.528484 ## 51 Rondale Moore 56.160105 ## 1871 Willie Snead 52.976833 ## 643 Marvin Mims 52.964329 ## 1052 Richie James 52.804939 ## 832 Noah Brown 52.679326 ## 1154 Alex Erickson 52.509506 ## 378 Darnell Mooney 52.086392 ## 1369 Kendrick Bourne 51.253158 ## 2024 Chris Moore 50.045149 ## 988 Zay Jones 49.764489 ## 2028 Treylon Burks 49.640461 ## 1057 Marquez Valdes-Scantling 49.377940 ## 708 Jameson Williams 49.324772 ## 1951 Trey Palmer 48.739577 ## 1172 Tre Tucker 48.528006 ## 173 Laquon Treadwell 48.210274 ## 1054 Kadarius Toney 48.115705 ## 1299 K.J. Osborn 47.591019 ## 1438 A.T. Perry 46.901041 ## 1176 DeAndre Carter 46.119276 ## 1174 Hunter Renfrow 45.005643 ## 1056 Skyy Moore 44.714032 ## 568 Jalen Tolbert 44.321699 ## 1248 Erik Ezukanma 43.473493 ## 1870 Chris Conley 43.341574 ## 1152 Derius Davis 43.210672 ## 1368 Jalen Reagor 42.651032 ## 1526 Isaiah Hodgins 42.549517 ## 1301 Brandon Powell 41.731107 ## 379 Tyler Scott 41.636601 ## 1872 Ronnie Bell 41.324308 ## 1873 Jauan Jennings 41.262680 ## 50 Greg Dortch 40.800184 ## 1604 Xavier Gipson 40.793818 ## 2097 Jamison Crowder 40.759849 ## 638 Brandon Johnson 39.362354 ## 171 Devin Duvernay 39.212830 ## 454 Andrei Iosivas 39.185428 ## 2026 Nick Westbrook-Ikhine 38.663963 ## 572 Jalen Brooks 37.934622 ## 710 Josh Reynolds 37.847174 ## 1663 Quez Watkins 37.468539 ## 1523 Parris Campbell 37.392809 ## 1730 Calvin Austin 37.079670 ## 1108 Tyler Johnson 36.189691 ## 48 Zach Pascal 35.976621 ## 383 Trent Taylor 34.774925 ## 502 David Bell 34.412056 ## 837 Xavier Hutchinson 34.147361 ## 1607 Jason Brownlee 33.931763 ## 1527 Sterling Shepard 33.816765 ## 1155 Keelan Doss 33.138372 ## 989 Jamal Agnew 33.015933 ## 985 Parker Washington 32.752353 ## 570 Michael Gallup 32.693754 ## 1364 Tyquan Thornton 32.280751 ## 906 Isaiah McKenzie 31.911440 ## 504 James Proche 31.888204 ## 1051 Mecole Hardman 30.967637 ## 1246 Braxton Berrios 30.411571 ## 1053 Justin Watson 30.253522 ## 1948 Deven Thompkins 29.802537 ## 240 Trent Sherfield 29.589862 ## 1802 Jake Bobo 29.321234 ## 305 Ihmir Smith-Marsette 29.141881 ## 1147 Simi Fehoko 28.591626 ## 783 Malik Heath 27.692556 ## 117 Van Jefferson 27.144753 ## 2099 Dyami Brown 27.010045 ## 2025 Kyle Philips 26.721271 ## 641 Phillip Dorsett 26.244963 ## 1297 Jalen Nailor 26.176881 ## 1666 Olamide Zaccheaus 26.140126 ## 382 Collin Johnson 26.052158 ## 1367 Ty Montgomery 25.724863 ## 1300 Trishton Jackson 25.583921 ## 1731 Miles Boykin 25.108936 ## 1366 Kayshon Boutte 24.964324 ## 172 Tylan Wallace 24.411415 ## 380 Equanimeous St. Brown 24.297427 ## 1245 Robbie Chosen 23.823578 ## 712 Antoine Green 23.785908 ## 909 Amari Rodgers 23.777108 ## 983 Tim Jones 23.584516 ## 707 Kalif Raymond 23.134101 ## 1875 Ray-Ray McCloud 22.882116 ## 706 Donovan Peoples-Jones 22.714729 ## 1436 Keith Kirkwood 22.444647 ## 116 KhaDarel Hodge 21.997801 ## 1103 Austin Trammell 20.917124 ## 642 Lil&#39;Jordan Humphrey 20.895207 ## 118 Scott Miller 20.411157 ## 1148 Jalen Guyton 20.293935 ## 1803 D&#39;Wayne Eskridge 20.213843 ## 1799 Cody Thompson 20.101604 ## 451 Charlie Jones 19.442446 ## 1606 Irvin Charles 19.035434 ## 904 D.J. Montgomery 18.224262 ## 1106 Ben Skowronek 17.767927 ## 1528 Gunner Olszewski 17.594065 ## 903 Juwann Winfree 17.539985 ## 450 Shedrick Jackson 17.439769 ## 1242 River Cracraft 16.962426 ## 2098 Byron Pringle 16.638514 ## 781 Samori Toure 16.189997 ## 1668 Britain Covey 15.996191 ## 1952 Rakim Jarrett 15.667073 ## 1953 David Moore 15.505266 ## 2022 Colton Dowell 14.706743 ## 1608 Malik Taylor 14.628111 ## 241 Deonte Harty 13.234701 ## 303 Mike Strachan 12.597809 ## 987 Elijah Cooks 12.225265 ## 1058 Justyn Ross 11.635965 ## 833 Steven Sims 11.447536 ## 2027 Mason Kinsey 9.686098 ## 243 Gabe Davis NA ## 301 Laviska Shenault NA ## 302 D.J. Chark NA ## 304 Terrace Marshall NA ## 377 Velus Jones NA ## 452 Kwamie Lassiter NA ## 908 Michael Pittman NA ## 1439 Lynn Bowden NA ## 1804 DK Metcalf NA "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
